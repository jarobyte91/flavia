{"sentences": "{\"filename\":{\"0\":\"two level boolean rules.pdf\",\"1\":\"two level boolean rules.pdf\",\"2\":\"two level boolean rules.pdf\",\"3\":\"two level boolean rules.pdf\",\"4\":\"two level boolean rules.pdf\",\"5\":\"two level boolean rules.pdf\",\"6\":\"two level boolean rules.pdf\",\"7\":\"two level boolean rules.pdf\",\"8\":\"two level boolean rules.pdf\",\"9\":\"two level boolean rules.pdf\",\"10\":\"two level boolean rules.pdf\",\"11\":\"two level boolean rules.pdf\",\"12\":\"two level boolean rules.pdf\",\"13\":\"two level boolean rules.pdf\",\"14\":\"two level boolean rules.pdf\",\"15\":\"two level boolean rules.pdf\",\"16\":\"two level boolean rules.pdf\",\"17\":\"two level boolean rules.pdf\",\"18\":\"two level boolean rules.pdf\",\"19\":\"two level boolean rules.pdf\",\"20\":\"two level boolean rules.pdf\",\"21\":\"two level boolean rules.pdf\",\"22\":\"two level boolean rules.pdf\",\"23\":\"two level boolean rules.pdf\",\"24\":\"two level boolean rules.pdf\",\"25\":\"two level boolean rules.pdf\",\"26\":\"two level boolean rules.pdf\",\"27\":\"two level boolean rules.pdf\",\"28\":\"two level boolean rules.pdf\",\"29\":\"two level boolean rules.pdf\",\"30\":\"two level boolean rules.pdf\",\"31\":\"two level boolean rules.pdf\",\"32\":\"two level boolean rules.pdf\",\"33\":\"two level boolean rules.pdf\",\"34\":\"two level boolean rules.pdf\",\"35\":\"two level boolean rules.pdf\",\"36\":\"two level boolean rules.pdf\",\"37\":\"two level boolean rules.pdf\",\"38\":\"two level boolean rules.pdf\",\"39\":\"two level boolean rules.pdf\",\"40\":\"two level boolean rules.pdf\",\"41\":\"two level boolean rules.pdf\",\"42\":\"two level boolean rules.pdf\",\"43\":\"two level boolean rules.pdf\",\"44\":\"two level boolean rules.pdf\",\"45\":\"two level boolean rules.pdf\",\"46\":\"two level boolean rules.pdf\",\"47\":\"two level boolean rules.pdf\",\"48\":\"two level boolean rules.pdf\",\"49\":\"two level boolean rules.pdf\",\"50\":\"two level boolean rules.pdf\",\"51\":\"two level boolean rules.pdf\",\"52\":\"two level boolean rules.pdf\",\"53\":\"two level boolean rules.pdf\",\"54\":\"two level boolean rules.pdf\",\"55\":\"two level boolean rules.pdf\",\"56\":\"two level boolean rules.pdf\",\"57\":\"two level boolean rules.pdf\",\"58\":\"two level boolean rules.pdf\",\"59\":\"two level boolean rules.pdf\",\"60\":\"two level boolean rules.pdf\",\"61\":\"two level boolean rules.pdf\",\"62\":\"two level boolean rules.pdf\",\"63\":\"two level boolean rules.pdf\",\"64\":\"two level boolean rules.pdf\",\"65\":\"two level boolean rules.pdf\",\"66\":\"two level boolean rules.pdf\",\"67\":\"two level boolean rules.pdf\",\"68\":\"two level boolean rules.pdf\",\"69\":\"two level boolean rules.pdf\",\"70\":\"two level boolean rules.pdf\",\"71\":\"two level boolean rules.pdf\",\"72\":\"two level boolean rules.pdf\",\"73\":\"two level boolean rules.pdf\",\"74\":\"two level boolean rules.pdf\",\"75\":\"two level boolean rules.pdf\",\"76\":\"two level boolean rules.pdf\",\"77\":\"two level boolean rules.pdf\",\"78\":\"two level boolean rules.pdf\",\"79\":\"two level boolean rules.pdf\",\"80\":\"two level boolean rules.pdf\",\"81\":\"two level boolean rules.pdf\",\"82\":\"two level boolean rules.pdf\",\"83\":\"two level boolean rules.pdf\",\"84\":\"two level boolean rules.pdf\",\"85\":\"two level boolean rules.pdf\",\"86\":\"two level boolean rules.pdf\",\"87\":\"two level boolean rules.pdf\",\"88\":\"two level boolean rules.pdf\",\"89\":\"two level boolean rules.pdf\",\"90\":\"two level boolean rules.pdf\",\"91\":\"two level boolean rules.pdf\",\"92\":\"two level boolean rules.pdf\",\"93\":\"two level boolean rules.pdf\",\"94\":\"two level boolean rules.pdf\",\"95\":\"two level boolean rules.pdf\",\"96\":\"two level boolean rules.pdf\",\"97\":\"two level boolean rules.pdf\",\"98\":\"two level boolean rules.pdf\",\"99\":\"two level boolean rules.pdf\",\"100\":\"two level boolean rules.pdf\",\"101\":\"two level boolean rules.pdf\",\"102\":\"two level boolean rules.pdf\",\"103\":\"two level boolean rules.pdf\",\"104\":\"two level boolean rules.pdf\",\"105\":\"two level boolean rules.pdf\",\"106\":\"two level boolean rules.pdf\",\"107\":\"two level boolean rules.pdf\",\"108\":\"two level boolean rules.pdf\",\"109\":\"two level boolean rules.pdf\",\"110\":\"two level boolean rules.pdf\",\"111\":\"two level boolean rules.pdf\",\"112\":\"two level boolean rules.pdf\",\"113\":\"two level boolean rules.pdf\",\"114\":\"two level boolean rules.pdf\",\"115\":\"two level boolean rules.pdf\",\"116\":\"two level boolean rules.pdf\",\"117\":\"two level boolean rules.pdf\",\"118\":\"two level boolean rules.pdf\",\"119\":\"two level boolean rules.pdf\",\"120\":\"two level boolean rules.pdf\",\"121\":\"two level boolean rules.pdf\",\"122\":\"two level boolean rules.pdf\",\"123\":\"two level boolean rules.pdf\",\"124\":\"two level boolean rules.pdf\",\"125\":\"two level boolean rules.pdf\",\"126\":\"two level boolean rules.pdf\",\"127\":\"two level boolean rules.pdf\",\"128\":\"two level boolean rules.pdf\",\"129\":\"two level boolean rules.pdf\",\"130\":\"two level boolean rules.pdf\",\"131\":\"two level boolean rules.pdf\",\"132\":\"two level boolean rules.pdf\",\"133\":\"two level boolean rules.pdf\",\"134\":\"two level boolean rules.pdf\",\"135\":\"two level boolean rules.pdf\",\"136\":\"two level boolean rules.pdf\",\"137\":\"two level boolean rules.pdf\",\"138\":\"two level boolean rules.pdf\",\"139\":\"two level boolean rules.pdf\",\"140\":\"two level boolean rules.pdf\",\"141\":\"two level boolean rules.pdf\",\"142\":\"two level boolean rules.pdf\",\"143\":\"two level boolean rules.pdf\",\"144\":\"two level boolean rules.pdf\",\"145\":\"two level boolean rules.pdf\",\"146\":\"two level boolean rules.pdf\",\"147\":\"two level boolean rules.pdf\",\"148\":\"two level boolean rules.pdf\",\"149\":\"two level boolean rules.pdf\",\"150\":\"two level boolean rules.pdf\",\"151\":\"two level boolean rules.pdf\",\"152\":\"two level boolean rules.pdf\",\"153\":\"two level boolean rules.pdf\",\"154\":\"two level boolean rules.pdf\",\"155\":\"two level boolean rules.pdf\",\"156\":\"two level boolean rules.pdf\",\"157\":\"two level boolean rules.pdf\",\"158\":\"two level boolean rules.pdf\",\"159\":\"two level boolean rules.pdf\",\"160\":\"two level boolean rules.pdf\",\"161\":\"two level boolean rules.pdf\",\"162\":\"two level boolean rules.pdf\",\"163\":\"two level boolean rules.pdf\",\"164\":\"two level boolean rules.pdf\",\"165\":\"two level boolean rules.pdf\",\"166\":\"two level boolean rules.pdf\",\"167\":\"two level boolean rules.pdf\",\"168\":\"two level boolean rules.pdf\",\"169\":\"two level boolean rules.pdf\",\"170\":\"two level boolean rules.pdf\",\"171\":\"two level boolean rules.pdf\",\"172\":\"two level boolean rules.pdf\",\"173\":\"two level boolean rules.pdf\",\"174\":\"two level boolean rules.pdf\",\"175\":\"two level boolean rules.pdf\",\"176\":\"two level boolean rules.pdf\",\"177\":\"two level boolean rules.pdf\",\"178\":\"two level boolean rules.pdf\",\"179\":\"two level boolean rules.pdf\",\"180\":\"two level boolean rules.pdf\",\"181\":\"two level boolean rules.pdf\",\"182\":\"two level boolean rules.pdf\",\"183\":\"two level boolean rules.pdf\",\"184\":\"two level boolean rules.pdf\",\"185\":\"two level boolean rules.pdf\",\"186\":\"two level boolean rules.pdf\",\"187\":\"two level boolean rules.pdf\",\"188\":\"two level boolean rules.pdf\",\"189\":\"two level boolean rules.pdf\",\"190\":\"two level boolean rules.pdf\",\"191\":\"two level boolean rules.pdf\",\"192\":\"two level boolean rules.pdf\",\"193\":\"two level boolean rules.pdf\",\"194\":\"two level boolean rules.pdf\",\"195\":\"two level boolean rules.pdf\",\"196\":\"two level boolean rules.pdf\",\"197\":\"two level boolean rules.pdf\",\"198\":\"two level boolean rules.pdf\",\"199\":\"two level boolean rules.pdf\",\"200\":\"two level boolean rules.pdf\",\"201\":\"two level boolean rules.pdf\",\"202\":\"two level boolean rules.pdf\",\"203\":\"two level boolean rules.pdf\",\"204\":\"two level boolean rules.pdf\",\"205\":\"two level boolean rules.pdf\",\"206\":\"two level boolean rules.pdf\",\"207\":\"two level boolean rules.pdf\",\"208\":\"two level boolean rules.pdf\",\"209\":\"two level boolean rules.pdf\",\"210\":\"two level boolean rules.pdf\",\"211\":\"two level boolean rules.pdf\",\"212\":\"two level boolean rules.pdf\",\"213\":\"two level boolean rules.pdf\",\"214\":\"two level boolean rules.pdf\",\"215\":\"two level boolean rules.pdf\",\"216\":\"two level boolean rules.pdf\",\"217\":\"two level boolean rules.pdf\",\"218\":\"two level boolean rules.pdf\",\"219\":\"two level boolean rules.pdf\",\"220\":\"two level boolean rules.pdf\",\"221\":\"two level boolean rules.pdf\",\"222\":\"two level boolean rules.pdf\",\"223\":\"two level boolean rules.pdf\",\"224\":\"two level boolean rules.pdf\",\"225\":\"two level boolean rules.pdf\",\"226\":\"two level boolean rules.pdf\",\"227\":\"two level boolean rules.pdf\",\"228\":\"two level boolean rules.pdf\",\"229\":\"two level boolean rules.pdf\",\"230\":\"two level boolean rules.pdf\",\"231\":\"two level boolean rules.pdf\",\"232\":\"two level boolean rules.pdf\",\"233\":\"two level boolean rules.pdf\",\"234\":\"two level boolean rules.pdf\",\"235\":\"two level boolean rules.pdf\",\"236\":\"two level boolean rules.pdf\",\"237\":\"two level boolean rules.pdf\",\"238\":\"two level boolean rules.pdf\",\"239\":\"two level boolean rules.pdf\",\"240\":\"two level boolean rules.pdf\",\"241\":\"two level boolean rules.pdf\",\"242\":\"two level boolean rules.pdf\",\"243\":\"two level boolean rules.pdf\",\"244\":\"two level boolean rules.pdf\",\"245\":\"two level boolean rules.pdf\",\"246\":\"two level boolean rules.pdf\",\"247\":\"two level boolean rules.pdf\",\"248\":\"two level boolean rules.pdf\",\"249\":\"two level boolean rules.pdf\",\"250\":\"two level boolean rules.pdf\",\"251\":\"two level boolean rules.pdf\",\"252\":\"two level boolean rules.pdf\",\"253\":\"two level boolean rules.pdf\",\"254\":\"two level boolean rules.pdf\",\"255\":\"two level boolean rules.pdf\",\"256\":\"two level boolean rules.pdf\",\"257\":\"two level boolean rules.pdf\",\"258\":\"two level boolean rules.pdf\",\"259\":\"two level boolean rules.pdf\",\"260\":\"two level boolean rules.pdf\",\"261\":\"two level boolean rules.pdf\",\"262\":\"two level boolean rules.pdf\",\"263\":\"two level boolean rules.pdf\",\"264\":\"two level boolean rules.pdf\",\"265\":\"two level boolean rules.pdf\",\"266\":\"two level boolean rules.pdf\",\"267\":\"two level boolean rules.pdf\",\"268\":\"two level boolean rules.pdf\",\"269\":\"two level boolean rules.pdf\",\"270\":\"two level boolean rules.pdf\",\"271\":\"two level boolean rules.pdf\",\"272\":\"two level boolean rules.pdf\",\"273\":\"two level boolean rules.pdf\",\"274\":\"two level boolean rules.pdf\",\"275\":\"two level boolean rules.pdf\",\"276\":\"two level boolean rules.pdf\",\"277\":\"two level boolean rules.pdf\",\"278\":\"two level boolean rules.pdf\",\"279\":\"two level boolean rules.pdf\",\"280\":\"two level boolean rules.pdf\",\"281\":\"BETA2.pdf\",\"282\":\"BETA2.pdf\",\"283\":\"BETA2.pdf\",\"284\":\"BETA2.pdf\",\"285\":\"BETA2.pdf\",\"286\":\"BETA2.pdf\",\"287\":\"BETA2.pdf\",\"288\":\"BETA2.pdf\",\"289\":\"BETA2.pdf\",\"290\":\"BETA2.pdf\",\"291\":\"BETA2.pdf\",\"292\":\"BETA2.pdf\",\"293\":\"BETA2.pdf\",\"294\":\"BETA2.pdf\",\"295\":\"BETA2.pdf\",\"296\":\"BETA2.pdf\",\"297\":\"BETA2.pdf\",\"298\":\"BETA2.pdf\",\"299\":\"BETA2.pdf\",\"300\":\"BETA2.pdf\",\"301\":\"BETA2.pdf\",\"302\":\"BETA2.pdf\",\"303\":\"BETA2.pdf\",\"304\":\"BETA2.pdf\",\"305\":\"BETA2.pdf\",\"306\":\"BETA2.pdf\",\"307\":\"BETA2.pdf\",\"308\":\"BETA2.pdf\",\"309\":\"BETA2.pdf\",\"310\":\"BETA2.pdf\",\"311\":\"BETA2.pdf\",\"312\":\"BETA2.pdf\",\"313\":\"BETA2.pdf\",\"314\":\"BETA2.pdf\",\"315\":\"BETA2.pdf\",\"316\":\"BETA2.pdf\",\"317\":\"BETA2.pdf\",\"318\":\"BETA2.pdf\",\"319\":\"BETA2.pdf\",\"320\":\"BETA2.pdf\",\"321\":\"BETA2.pdf\",\"322\":\"BETA2.pdf\",\"323\":\"BETA2.pdf\",\"324\":\"BETA2.pdf\",\"325\":\"BETA2.pdf\",\"326\":\"BETA2.pdf\",\"327\":\"BETA2.pdf\",\"328\":\"BETA2.pdf\",\"329\":\"BETA2.pdf\",\"330\":\"BETA2.pdf\",\"331\":\"BETA2.pdf\",\"332\":\"BETA2.pdf\",\"333\":\"BETA2.pdf\",\"334\":\"BETA2.pdf\",\"335\":\"BETA2.pdf\",\"336\":\"BETA2.pdf\",\"337\":\"BETA2.pdf\",\"338\":\"BETA2.pdf\",\"339\":\"BETA2.pdf\",\"340\":\"BETA2.pdf\",\"341\":\"BETA2.pdf\",\"342\":\"BETA2.pdf\",\"343\":\"BETA2.pdf\",\"344\":\"BETA2.pdf\",\"345\":\"BETA2.pdf\",\"346\":\"BETA2.pdf\",\"347\":\"BETA2.pdf\",\"348\":\"BETA2.pdf\",\"349\":\"BETA2.pdf\",\"350\":\"BETA2.pdf\",\"351\":\"BETA2.pdf\",\"352\":\"BETA2.pdf\",\"353\":\"BETA2.pdf\",\"354\":\"BETA2.pdf\",\"355\":\"BETA2.pdf\",\"356\":\"BETA2.pdf\",\"357\":\"BETA2.pdf\",\"358\":\"BETA2.pdf\",\"359\":\"BETA2.pdf\",\"360\":\"BETA2.pdf\",\"361\":\"BETA2.pdf\",\"362\":\"BETA2.pdf\",\"363\":\"BETA2.pdf\",\"364\":\"BETA2.pdf\",\"365\":\"BETA2.pdf\",\"366\":\"BETA2.pdf\",\"367\":\"BETA2.pdf\",\"368\":\"BETA2.pdf\",\"369\":\"BETA2.pdf\",\"370\":\"BETA2.pdf\",\"371\":\"BETA2.pdf\",\"372\":\"BETA2.pdf\",\"373\":\"BETA2.pdf\",\"374\":\"BETA2.pdf\",\"375\":\"BETA2.pdf\",\"376\":\"BETA2.pdf\",\"377\":\"BETA2.pdf\",\"378\":\"BETA2.pdf\",\"379\":\"BETA2.pdf\",\"380\":\"BETA2.pdf\",\"381\":\"BETA2.pdf\",\"382\":\"BETA2.pdf\",\"383\":\"BETA2.pdf\",\"384\":\"BETA2.pdf\",\"385\":\"BETA2.pdf\",\"386\":\"BETA2.pdf\",\"387\":\"BETA2.pdf\",\"388\":\"BETA2.pdf\",\"389\":\"BETA2.pdf\",\"390\":\"BETA2.pdf\",\"391\":\"BETA2.pdf\",\"392\":\"BETA2.pdf\",\"393\":\"BETA2.pdf\",\"394\":\"BETA2.pdf\",\"395\":\"BETA2.pdf\",\"396\":\"BETA2.pdf\",\"397\":\"BETA2.pdf\",\"398\":\"BETA2.pdf\",\"399\":\"BETA2.pdf\",\"400\":\"BETA2.pdf\",\"401\":\"BETA2.pdf\",\"402\":\"BETA2.pdf\",\"403\":\"BETA2.pdf\",\"404\":\"BETA2.pdf\",\"405\":\"BETA2.pdf\",\"406\":\"BETA2.pdf\",\"407\":\"BETA2.pdf\",\"408\":\"BETA2.pdf\",\"409\":\"BETA2.pdf\",\"410\":\"BETA2.pdf\",\"411\":\"BETA2.pdf\",\"412\":\"BETA2.pdf\",\"413\":\"BETA2.pdf\",\"414\":\"BETA2.pdf\",\"415\":\"BETA2.pdf\",\"416\":\"BETA2.pdf\",\"417\":\"BETA2.pdf\",\"418\":\"BETA2.pdf\",\"419\":\"BETA2.pdf\",\"420\":\"BETA2.pdf\",\"421\":\"BETA2.pdf\",\"422\":\"BETA2.pdf\",\"423\":\"BETA2.pdf\",\"424\":\"BETA2.pdf\",\"425\":\"BETA2.pdf\",\"426\":\"BETA2.pdf\",\"427\":\"BETA2.pdf\",\"428\":\"BETA2.pdf\",\"429\":\"BETA2.pdf\",\"430\":\"BETA2.pdf\",\"431\":\"BETA2.pdf\",\"432\":\"BETA2.pdf\",\"433\":\"BETA2.pdf\",\"434\":\"BETA2.pdf\",\"435\":\"BETA2.pdf\",\"436\":\"BETA2.pdf\",\"437\":\"BETA2.pdf\",\"438\":\"BETA2.pdf\",\"439\":\"BETA2.pdf\",\"440\":\"BETA2.pdf\",\"441\":\"BETA2.pdf\",\"442\":\"BETA2.pdf\",\"443\":\"BETA2.pdf\",\"444\":\"BETA2.pdf\",\"445\":\"BETA2.pdf\",\"446\":\"BETA2.pdf\",\"447\":\"BETA2.pdf\",\"448\":\"BETA2.pdf\",\"449\":\"BETA2.pdf\",\"450\":\"BETA2.pdf\",\"451\":\"BETA2.pdf\",\"452\":\"distill-and-compare.pdf\",\"453\":\"distill-and-compare.pdf\",\"454\":\"distill-and-compare.pdf\",\"455\":\"distill-and-compare.pdf\",\"456\":\"distill-and-compare.pdf\",\"457\":\"distill-and-compare.pdf\",\"458\":\"distill-and-compare.pdf\",\"459\":\"distill-and-compare.pdf\",\"460\":\"distill-and-compare.pdf\",\"461\":\"distill-and-compare.pdf\",\"462\":\"distill-and-compare.pdf\",\"463\":\"distill-and-compare.pdf\",\"464\":\"distill-and-compare.pdf\",\"465\":\"distill-and-compare.pdf\",\"466\":\"distill-and-compare.pdf\",\"467\":\"distill-and-compare.pdf\",\"468\":\"distill-and-compare.pdf\",\"469\":\"distill-and-compare.pdf\",\"470\":\"distill-and-compare.pdf\",\"471\":\"distill-and-compare.pdf\",\"472\":\"distill-and-compare.pdf\",\"473\":\"distill-and-compare.pdf\",\"474\":\"distill-and-compare.pdf\",\"475\":\"distill-and-compare.pdf\",\"476\":\"distill-and-compare.pdf\",\"477\":\"distill-and-compare.pdf\",\"478\":\"distill-and-compare.pdf\",\"479\":\"distill-and-compare.pdf\",\"480\":\"distill-and-compare.pdf\",\"481\":\"distill-and-compare.pdf\",\"482\":\"distill-and-compare.pdf\",\"483\":\"distill-and-compare.pdf\",\"484\":\"distill-and-compare.pdf\",\"485\":\"distill-and-compare.pdf\",\"486\":\"distill-and-compare.pdf\",\"487\":\"distill-and-compare.pdf\",\"488\":\"distill-and-compare.pdf\",\"489\":\"distill-and-compare.pdf\",\"490\":\"distill-and-compare.pdf\",\"491\":\"distill-and-compare.pdf\",\"492\":\"distill-and-compare.pdf\",\"493\":\"distill-and-compare.pdf\",\"494\":\"distill-and-compare.pdf\",\"495\":\"distill-and-compare.pdf\",\"496\":\"distill-and-compare.pdf\",\"497\":\"distill-and-compare.pdf\",\"498\":\"distill-and-compare.pdf\",\"499\":\"distill-and-compare.pdf\",\"500\":\"distill-and-compare.pdf\",\"501\":\"distill-and-compare.pdf\",\"502\":\"distill-and-compare.pdf\",\"503\":\"distill-and-compare.pdf\",\"504\":\"distill-and-compare.pdf\",\"505\":\"distill-and-compare.pdf\",\"506\":\"distill-and-compare.pdf\",\"507\":\"distill-and-compare.pdf\",\"508\":\"distill-and-compare.pdf\",\"509\":\"distill-and-compare.pdf\",\"510\":\"distill-and-compare.pdf\",\"511\":\"distill-and-compare.pdf\",\"512\":\"distill-and-compare.pdf\",\"513\":\"distill-and-compare.pdf\",\"514\":\"distill-and-compare.pdf\",\"515\":\"distill-and-compare.pdf\",\"516\":\"distill-and-compare.pdf\",\"517\":\"distill-and-compare.pdf\",\"518\":\"distill-and-compare.pdf\",\"519\":\"distill-and-compare.pdf\",\"520\":\"distill-and-compare.pdf\",\"521\":\"distill-and-compare.pdf\",\"522\":\"distill-and-compare.pdf\",\"523\":\"distill-and-compare.pdf\",\"524\":\"distill-and-compare.pdf\",\"525\":\"distill-and-compare.pdf\",\"526\":\"distill-and-compare.pdf\",\"527\":\"distill-and-compare.pdf\",\"528\":\"distill-and-compare.pdf\",\"529\":\"distill-and-compare.pdf\",\"530\":\"distill-and-compare.pdf\",\"531\":\"distill-and-compare.pdf\",\"532\":\"distill-and-compare.pdf\",\"533\":\"distill-and-compare.pdf\",\"534\":\"distill-and-compare.pdf\",\"535\":\"distill-and-compare.pdf\",\"536\":\"distill-and-compare.pdf\",\"537\":\"distill-and-compare.pdf\",\"538\":\"distill-and-compare.pdf\",\"539\":\"distill-and-compare.pdf\",\"540\":\"distill-and-compare.pdf\",\"541\":\"distill-and-compare.pdf\",\"542\":\"distill-and-compare.pdf\",\"543\":\"distill-and-compare.pdf\",\"544\":\"distill-and-compare.pdf\",\"545\":\"distill-and-compare.pdf\",\"546\":\"distill-and-compare.pdf\",\"547\":\"distill-and-compare.pdf\",\"548\":\"distill-and-compare.pdf\",\"549\":\"distill-and-compare.pdf\",\"550\":\"distill-and-compare.pdf\",\"551\":\"distill-and-compare.pdf\",\"552\":\"distill-and-compare.pdf\",\"553\":\"distill-and-compare.pdf\",\"554\":\"distill-and-compare.pdf\",\"555\":\"distill-and-compare.pdf\",\"556\":\"distill-and-compare.pdf\",\"557\":\"distill-and-compare.pdf\",\"558\":\"distill-and-compare.pdf\",\"559\":\"distill-and-compare.pdf\",\"560\":\"distill-and-compare.pdf\",\"561\":\"distill-and-compare.pdf\",\"562\":\"distill-and-compare.pdf\",\"563\":\"distill-and-compare.pdf\",\"564\":\"distill-and-compare.pdf\",\"565\":\"distill-and-compare.pdf\",\"566\":\"distill-and-compare.pdf\",\"567\":\"distill-and-compare.pdf\",\"568\":\"distill-and-compare.pdf\",\"569\":\"distill-and-compare.pdf\",\"570\":\"distill-and-compare.pdf\",\"571\":\"distill-and-compare.pdf\",\"572\":\"distill-and-compare.pdf\",\"573\":\"distill-and-compare.pdf\",\"574\":\"distill-and-compare.pdf\",\"575\":\"distill-and-compare.pdf\",\"576\":\"distill-and-compare.pdf\",\"577\":\"distill-and-compare.pdf\",\"578\":\"distill-and-compare.pdf\",\"579\":\"distill-and-compare.pdf\",\"580\":\"distill-and-compare.pdf\",\"581\":\"distill-and-compare.pdf\",\"582\":\"distill-and-compare.pdf\",\"583\":\"distill-and-compare.pdf\",\"584\":\"distill-and-compare.pdf\",\"585\":\"distill-and-compare.pdf\",\"586\":\"distill-and-compare.pdf\",\"587\":\"distill-and-compare.pdf\",\"588\":\"distill-and-compare.pdf\",\"589\":\"distill-and-compare.pdf\",\"590\":\"distill-and-compare.pdf\",\"591\":\"distill-and-compare.pdf\",\"592\":\"distill-and-compare.pdf\",\"593\":\"distill-and-compare.pdf\",\"594\":\"distill-and-compare.pdf\",\"595\":\"distill-and-compare.pdf\",\"596\":\"distill-and-compare.pdf\",\"597\":\"distill-and-compare.pdf\",\"598\":\"distill-and-compare.pdf\",\"599\":\"distill-and-compare.pdf\",\"600\":\"distill-and-compare.pdf\",\"601\":\"distill-and-compare.pdf\",\"602\":\"distill-and-compare.pdf\",\"603\":\"distill-and-compare.pdf\",\"604\":\"distill-and-compare.pdf\",\"605\":\"distill-and-compare.pdf\",\"606\":\"distill-and-compare.pdf\",\"607\":\"distill-and-compare.pdf\",\"608\":\"distill-and-compare.pdf\",\"609\":\"distill-and-compare.pdf\",\"610\":\"distill-and-compare.pdf\",\"611\":\"distill-and-compare.pdf\",\"612\":\"distill-and-compare.pdf\",\"613\":\"distill-and-compare.pdf\",\"614\":\"distill-and-compare.pdf\",\"615\":\"distill-and-compare.pdf\",\"616\":\"distill-and-compare.pdf\",\"617\":\"distill-and-compare.pdf\",\"618\":\"distill-and-compare.pdf\",\"619\":\"distill-and-compare.pdf\",\"620\":\"distill-and-compare.pdf\",\"621\":\"distill-and-compare.pdf\",\"622\":\"distill-and-compare.pdf\",\"623\":\"distill-and-compare.pdf\",\"624\":\"distill-and-compare.pdf\",\"625\":\"distill-and-compare.pdf\",\"626\":\"distill-and-compare.pdf\",\"627\":\"distill-and-compare.pdf\",\"628\":\"distill-and-compare.pdf\",\"629\":\"distill-and-compare.pdf\",\"630\":\"distill-and-compare.pdf\",\"631\":\"distill-and-compare.pdf\",\"632\":\"distill-and-compare.pdf\",\"633\":\"distill-and-compare.pdf\",\"634\":\"distill-and-compare.pdf\",\"635\":\"distill-and-compare.pdf\",\"636\":\"distill-and-compare.pdf\",\"637\":\"distill-and-compare.pdf\",\"638\":\"distill-and-compare.pdf\",\"639\":\"distill-and-compare.pdf\",\"640\":\"distill-and-compare.pdf\",\"641\":\"distill-and-compare.pdf\",\"642\":\"distill-and-compare.pdf\",\"643\":\"distill-and-compare.pdf\",\"644\":\"distill-and-compare.pdf\",\"645\":\"distill-and-compare.pdf\",\"646\":\"distill-and-compare.pdf\",\"647\":\"distill-and-compare.pdf\",\"648\":\"distill-and-compare.pdf\",\"649\":\"distill-and-compare.pdf\",\"650\":\"distill-and-compare.pdf\",\"651\":\"distill-and-compare.pdf\",\"652\":\"distill-and-compare.pdf\",\"653\":\"distill-and-compare.pdf\",\"654\":\"distill-and-compare.pdf\",\"655\":\"distill-and-compare.pdf\",\"656\":\"distill-and-compare.pdf\",\"657\":\"distill-and-compare.pdf\",\"658\":\"distill-and-compare.pdf\",\"659\":\"distill-and-compare.pdf\",\"660\":\"distill-and-compare.pdf\",\"661\":\"distill-and-compare.pdf\",\"662\":\"distill-and-compare.pdf\",\"663\":\"distill-and-compare.pdf\",\"664\":\"distill-and-compare.pdf\",\"665\":\"distill-and-compare.pdf\",\"666\":\"distill-and-compare.pdf\",\"667\":\"distill-and-compare.pdf\",\"668\":\"distill-and-compare.pdf\",\"669\":\"distill-and-compare.pdf\",\"670\":\"distill-and-compare.pdf\",\"671\":\"distill-and-compare.pdf\",\"672\":\"distill-and-compare.pdf\",\"673\":\"distill-and-compare.pdf\",\"674\":\"distill-and-compare.pdf\",\"675\":\"distill-and-compare.pdf\",\"676\":\"distill-and-compare.pdf\",\"677\":\"distill-and-compare.pdf\",\"678\":\"distill-and-compare.pdf\",\"679\":\"distill-and-compare.pdf\",\"680\":\"distill-and-compare.pdf\",\"681\":\"distill-and-compare.pdf\",\"682\":\"distill-and-compare.pdf\",\"683\":\"distill-and-compare.pdf\",\"684\":\"distill-and-compare.pdf\",\"685\":\"distill-and-compare.pdf\",\"686\":\"distill-and-compare.pdf\",\"687\":\"distill-and-compare.pdf\",\"688\":\"distill-and-compare.pdf\",\"689\":\"distill-and-compare.pdf\",\"690\":\"distill-and-compare.pdf\",\"691\":\"distill-and-compare.pdf\",\"692\":\"distill-and-compare.pdf\",\"693\":\"distill-and-compare.pdf\",\"694\":\"distill-and-compare.pdf\",\"695\":\"distill-and-compare.pdf\",\"696\":\"distill-and-compare.pdf\",\"697\":\"distill-and-compare.pdf\",\"698\":\"distill-and-compare.pdf\",\"699\":\"distill-and-compare.pdf\",\"700\":\"distill-and-compare.pdf\",\"701\":\"distill-and-compare.pdf\",\"702\":\"distill-and-compare.pdf\",\"703\":\"distill-and-compare.pdf\",\"704\":\"distill-and-compare.pdf\",\"705\":\"distill-and-compare.pdf\",\"706\":\"distill-and-compare.pdf\",\"707\":\"distill-and-compare.pdf\",\"708\":\"distill-and-compare.pdf\",\"709\":\"distill-and-compare.pdf\",\"710\":\"distill-and-compare.pdf\",\"711\":\"distill-and-compare.pdf\",\"712\":\"distill-and-compare.pdf\",\"713\":\"distill-and-compare.pdf\",\"714\":\"distill-and-compare.pdf\",\"715\":\"distill-and-compare.pdf\",\"716\":\"distill-and-compare.pdf\",\"717\":\"distill-and-compare.pdf\",\"718\":\"distill-and-compare.pdf\",\"719\":\"distill-and-compare.pdf\",\"720\":\"distill-and-compare.pdf\",\"721\":\"distill-and-compare.pdf\",\"722\":\"distill-and-compare.pdf\",\"723\":\"distill-and-compare.pdf\",\"724\":\"distill-and-compare.pdf\",\"725\":\"distill-and-compare.pdf\",\"726\":\"distill-and-compare.pdf\",\"727\":\"distill-and-compare.pdf\",\"728\":\"distill-and-compare.pdf\",\"729\":\"distill-and-compare.pdf\",\"730\":\"distill-and-compare.pdf\",\"731\":\"distill-and-compare.pdf\",\"732\":\"distill-and-compare.pdf\",\"733\":\"distill-and-compare.pdf\",\"734\":\"distill-and-compare.pdf\",\"735\":\"distill-and-compare.pdf\",\"736\":\"distill-and-compare.pdf\",\"737\":\"distill-and-compare.pdf\",\"738\":\"distill-and-compare.pdf\",\"739\":\"distill-and-compare.pdf\",\"740\":\"distill-and-compare.pdf\",\"741\":\"distill-and-compare.pdf\",\"742\":\"distill-and-compare.pdf\",\"743\":\"distill-and-compare.pdf\",\"744\":\"distill-and-compare.pdf\",\"745\":\"distill-and-compare.pdf\",\"746\":\"distill-and-compare.pdf\",\"747\":\"distill-and-compare.pdf\",\"748\":\"distill-and-compare.pdf\",\"749\":\"distill-and-compare.pdf\",\"750\":\"distill-and-compare.pdf\",\"751\":\"distill-and-compare.pdf\",\"752\":\"distill-and-compare.pdf\",\"753\":\"distill-and-compare.pdf\",\"754\":\"distill-and-compare.pdf\",\"755\":\"distill-and-compare.pdf\",\"756\":\"distill-and-compare.pdf\",\"757\":\"distill-and-compare.pdf\",\"758\":\"distill-and-compare.pdf\",\"759\":\"distill-and-compare.pdf\",\"760\":\"distill-and-compare.pdf\",\"761\":\"distill-and-compare.pdf\",\"762\":\"distill-and-compare.pdf\",\"763\":\"distill-and-compare.pdf\",\"764\":\"distill-and-compare.pdf\",\"765\":\"distill-and-compare.pdf\",\"766\":\"distill-and-compare.pdf\",\"767\":\"distill-and-compare.pdf\",\"768\":\"distill-and-compare.pdf\",\"769\":\"distill-and-compare.pdf\",\"770\":\"distill-and-compare.pdf\",\"771\":\"distill-and-compare.pdf\",\"772\":\"distill-and-compare.pdf\",\"773\":\"distill-and-compare.pdf\",\"774\":\"distill-and-compare.pdf\",\"775\":\"distill-and-compare.pdf\",\"776\":\"distill-and-compare.pdf\",\"777\":\"distill-and-compare.pdf\",\"778\":\"distill-and-compare.pdf\",\"779\":\"distill-and-compare.pdf\",\"780\":\"distill-and-compare.pdf\",\"781\":\"distill-and-compare.pdf\",\"782\":\"distill-and-compare.pdf\",\"783\":\"distill-and-compare.pdf\",\"784\":\"distill-and-compare.pdf\",\"785\":\"distill-and-compare.pdf\",\"786\":\"distill-and-compare.pdf\",\"787\":\"distill-and-compare.pdf\",\"788\":\"distill-and-compare.pdf\",\"789\":\"distill-and-compare.pdf\",\"790\":\"distill-and-compare.pdf\",\"791\":\"distill-and-compare.pdf\",\"792\":\"distill-and-compare.pdf\",\"793\":\"distill-and-compare.pdf\",\"794\":\"distill-and-compare.pdf\",\"795\":\"distill-and-compare.pdf\",\"796\":\"distill-and-compare.pdf\",\"797\":\"distill-and-compare.pdf\",\"798\":\"distill-and-compare.pdf\",\"799\":\"distill-and-compare.pdf\",\"800\":\"distill-and-compare.pdf\",\"801\":\"distill-and-compare.pdf\",\"802\":\"distill-and-compare.pdf\",\"803\":\"distill-and-compare.pdf\",\"804\":\"distill-and-compare.pdf\",\"805\":\"distill-and-compare.pdf\",\"806\":\"distill-and-compare.pdf\",\"807\":\"distill-and-compare.pdf\",\"808\":\"distill-and-compare.pdf\",\"809\":\"distill-and-compare.pdf\",\"810\":\"distill-and-compare.pdf\",\"811\":\"distill-and-compare.pdf\",\"812\":\"distill-and-compare.pdf\",\"813\":\"distill-and-compare.pdf\",\"814\":\"distill-and-compare.pdf\",\"815\":\"distill-and-compare.pdf\",\"816\":\"distill-and-compare.pdf\",\"817\":\"distill-and-compare.pdf\",\"818\":\"distill-and-compare.pdf\",\"819\":\"distill-and-compare.pdf\",\"820\":\"distill-and-compare.pdf\",\"821\":\"distill-and-compare.pdf\",\"822\":\"distill-and-compare.pdf\",\"823\":\"distill-and-compare.pdf\",\"824\":\"distill-and-compare.pdf\",\"825\":\"distill-and-compare.pdf\",\"826\":\"distill-and-compare.pdf\",\"827\":\"distill-and-compare.pdf\",\"828\":\"distill-and-compare.pdf\",\"829\":\"distill-and-compare.pdf\",\"830\":\"distill-and-compare.pdf\",\"831\":\"distill-and-compare.pdf\",\"832\":\"distill-and-compare.pdf\",\"833\":\"distill-and-compare.pdf\",\"834\":\"distill-and-compare.pdf\",\"835\":\"distill-and-compare.pdf\",\"836\":\"distill-and-compare.pdf\",\"837\":\"distill-and-compare.pdf\",\"838\":\"distill-and-compare.pdf\",\"839\":\"ROPE (robust MUSE).pdf\",\"840\":\"ROPE (robust MUSE).pdf\",\"841\":\"ROPE (robust MUSE).pdf\",\"842\":\"ROPE (robust MUSE).pdf\",\"843\":\"ROPE (robust MUSE).pdf\",\"844\":\"ROPE (robust MUSE).pdf\",\"845\":\"ROPE (robust MUSE).pdf\",\"846\":\"ROPE (robust MUSE).pdf\",\"847\":\"ROPE (robust MUSE).pdf\",\"848\":\"ROPE (robust MUSE).pdf\",\"849\":\"ROPE (robust MUSE).pdf\",\"850\":\"ROPE (robust MUSE).pdf\",\"851\":\"ROPE (robust MUSE).pdf\",\"852\":\"ROPE (robust MUSE).pdf\",\"853\":\"ROPE (robust MUSE).pdf\",\"854\":\"ROPE (robust MUSE).pdf\",\"855\":\"ROPE (robust MUSE).pdf\",\"856\":\"ROPE (robust MUSE).pdf\",\"857\":\"ROPE (robust MUSE).pdf\",\"858\":\"ROPE (robust MUSE).pdf\",\"859\":\"ROPE (robust MUSE).pdf\",\"860\":\"ROPE (robust MUSE).pdf\",\"861\":\"ROPE (robust MUSE).pdf\",\"862\":\"ROPE (robust MUSE).pdf\",\"863\":\"ROPE (robust MUSE).pdf\",\"864\":\"ROPE (robust MUSE).pdf\",\"865\":\"ROPE (robust MUSE).pdf\",\"866\":\"ROPE (robust MUSE).pdf\",\"867\":\"ROPE (robust MUSE).pdf\",\"868\":\"ROPE (robust MUSE).pdf\",\"869\":\"ROPE (robust MUSE).pdf\",\"870\":\"ROPE (robust MUSE).pdf\",\"871\":\"ROPE (robust MUSE).pdf\",\"872\":\"ROPE (robust MUSE).pdf\",\"873\":\"ROPE (robust MUSE).pdf\",\"874\":\"ROPE (robust MUSE).pdf\",\"875\":\"ROPE (robust MUSE).pdf\",\"876\":\"ROPE (robust MUSE).pdf\",\"877\":\"ROPE (robust MUSE).pdf\",\"878\":\"ROPE (robust MUSE).pdf\",\"879\":\"ROPE (robust MUSE).pdf\",\"880\":\"ROPE (robust MUSE).pdf\",\"881\":\"ROPE (robust MUSE).pdf\",\"882\":\"ROPE (robust MUSE).pdf\",\"883\":\"ROPE (robust MUSE).pdf\",\"884\":\"ROPE (robust MUSE).pdf\",\"885\":\"ROPE (robust MUSE).pdf\",\"886\":\"ROPE (robust MUSE).pdf\",\"887\":\"ROPE (robust MUSE).pdf\",\"888\":\"ROPE (robust MUSE).pdf\",\"889\":\"ROPE (robust MUSE).pdf\",\"890\":\"ROPE (robust MUSE).pdf\",\"891\":\"ROPE (robust MUSE).pdf\",\"892\":\"ROPE (robust MUSE).pdf\",\"893\":\"ROPE (robust MUSE).pdf\",\"894\":\"ROPE (robust MUSE).pdf\",\"895\":\"ROPE (robust MUSE).pdf\",\"896\":\"ROPE (robust MUSE).pdf\",\"897\":\"ROPE (robust MUSE).pdf\",\"898\":\"ROPE (robust MUSE).pdf\",\"899\":\"ROPE (robust MUSE).pdf\",\"900\":\"ROPE (robust MUSE).pdf\",\"901\":\"ROPE (robust MUSE).pdf\",\"902\":\"ROPE (robust MUSE).pdf\",\"903\":\"ROPE (robust MUSE).pdf\",\"904\":\"ROPE (robust MUSE).pdf\",\"905\":\"ROPE (robust MUSE).pdf\",\"906\":\"ROPE (robust MUSE).pdf\",\"907\":\"ROPE (robust MUSE).pdf\",\"908\":\"ROPE (robust MUSE).pdf\",\"909\":\"ROPE (robust MUSE).pdf\",\"910\":\"ROPE (robust MUSE).pdf\",\"911\":\"ROPE (robust MUSE).pdf\",\"912\":\"ROPE (robust MUSE).pdf\",\"913\":\"ROPE (robust MUSE).pdf\",\"914\":\"ROPE (robust MUSE).pdf\",\"915\":\"ROPE (robust MUSE).pdf\",\"916\":\"ROPE (robust MUSE).pdf\",\"917\":\"ROPE (robust MUSE).pdf\",\"918\":\"ROPE (robust MUSE).pdf\",\"919\":\"ROPE (robust MUSE).pdf\",\"920\":\"ROPE (robust MUSE).pdf\",\"921\":\"ROPE (robust MUSE).pdf\",\"922\":\"ROPE (robust MUSE).pdf\",\"923\":\"ROPE (robust MUSE).pdf\",\"924\":\"ROPE (robust MUSE).pdf\",\"925\":\"ROPE (robust MUSE).pdf\",\"926\":\"ROPE (robust MUSE).pdf\",\"927\":\"ROPE (robust MUSE).pdf\",\"928\":\"ROPE (robust MUSE).pdf\",\"929\":\"ROPE (robust MUSE).pdf\",\"930\":\"ROPE (robust MUSE).pdf\",\"931\":\"ROPE (robust MUSE).pdf\",\"932\":\"ROPE (robust MUSE).pdf\",\"933\":\"ROPE (robust MUSE).pdf\",\"934\":\"ROPE (robust MUSE).pdf\",\"935\":\"ROPE (robust MUSE).pdf\",\"936\":\"ROPE (robust MUSE).pdf\",\"937\":\"ROPE (robust MUSE).pdf\",\"938\":\"ROPE (robust MUSE).pdf\",\"939\":\"ROPE (robust MUSE).pdf\",\"940\":\"ROPE (robust MUSE).pdf\",\"941\":\"ROPE (robust MUSE).pdf\",\"942\":\"ROPE (robust MUSE).pdf\",\"943\":\"ROPE (robust MUSE).pdf\",\"944\":\"ROPE (robust MUSE).pdf\",\"945\":\"ROPE (robust MUSE).pdf\",\"946\":\"ROPE (robust MUSE).pdf\",\"947\":\"ROPE (robust MUSE).pdf\",\"948\":\"ROPE (robust MUSE).pdf\",\"949\":\"ROPE (robust MUSE).pdf\",\"950\":\"ROPE (robust MUSE).pdf\",\"951\":\"ROPE (robust MUSE).pdf\",\"952\":\"ROPE (robust MUSE).pdf\",\"953\":\"ROPE (robust MUSE).pdf\",\"954\":\"ROPE (robust MUSE).pdf\",\"955\":\"ROPE (robust MUSE).pdf\",\"956\":\"ROPE (robust MUSE).pdf\",\"957\":\"ROPE (robust MUSE).pdf\",\"958\":\"ROPE (robust MUSE).pdf\",\"959\":\"ROPE (robust MUSE).pdf\",\"960\":\"ROPE (robust MUSE).pdf\",\"961\":\"ROPE (robust MUSE).pdf\",\"962\":\"ROPE (robust MUSE).pdf\",\"963\":\"ROPE (robust MUSE).pdf\",\"964\":\"ROPE (robust MUSE).pdf\",\"965\":\"ROPE (robust MUSE).pdf\",\"966\":\"ROPE (robust MUSE).pdf\",\"967\":\"ROPE (robust MUSE).pdf\",\"968\":\"ROPE (robust MUSE).pdf\",\"969\":\"ROPE (robust MUSE).pdf\",\"970\":\"ROPE (robust MUSE).pdf\",\"971\":\"ROPE (robust MUSE).pdf\",\"972\":\"ROPE (robust MUSE).pdf\",\"973\":\"ROPE (robust MUSE).pdf\",\"974\":\"ROPE (robust MUSE).pdf\",\"975\":\"ROPE (robust MUSE).pdf\",\"976\":\"ROPE (robust MUSE).pdf\",\"977\":\"ROPE (robust MUSE).pdf\",\"978\":\"ROPE (robust MUSE).pdf\",\"979\":\"ROPE (robust MUSE).pdf\",\"980\":\"ROPE (robust MUSE).pdf\",\"981\":\"ROPE (robust MUSE).pdf\",\"982\":\"ROPE (robust MUSE).pdf\",\"983\":\"ROPE (robust MUSE).pdf\",\"984\":\"ROPE (robust MUSE).pdf\",\"985\":\"ROPE (robust MUSE).pdf\",\"986\":\"ROPE (robust MUSE).pdf\",\"987\":\"ROPE (robust MUSE).pdf\",\"988\":\"ROPE (robust MUSE).pdf\",\"989\":\"ROPE (robust MUSE).pdf\",\"990\":\"ROPE (robust MUSE).pdf\",\"991\":\"ROPE (robust MUSE).pdf\",\"992\":\"ROPE (robust MUSE).pdf\",\"993\":\"ROPE (robust MUSE).pdf\",\"994\":\"ROPE (robust MUSE).pdf\",\"995\":\"ROPE (robust MUSE).pdf\",\"996\":\"ROPE (robust MUSE).pdf\",\"997\":\"ROPE (robust MUSE).pdf\",\"998\":\"ROPE (robust MUSE).pdf\",\"999\":\"ROPE (robust MUSE).pdf\",\"1000\":\"ROPE (robust MUSE).pdf\",\"1001\":\"ROPE (robust MUSE).pdf\",\"1002\":\"ROPE (robust MUSE).pdf\",\"1003\":\"ROPE (robust MUSE).pdf\",\"1004\":\"ROPE (robust MUSE).pdf\",\"1005\":\"ROPE (robust MUSE).pdf\",\"1006\":\"ROPE (robust MUSE).pdf\",\"1007\":\"ROPE (robust MUSE).pdf\",\"1008\":\"ROPE (robust MUSE).pdf\",\"1009\":\"ROPE (robust MUSE).pdf\",\"1010\":\"ROPE (robust MUSE).pdf\",\"1011\":\"ROPE (robust MUSE).pdf\",\"1012\":\"ROPE (robust MUSE).pdf\",\"1013\":\"ROPE (robust MUSE).pdf\",\"1014\":\"ROPE (robust MUSE).pdf\",\"1015\":\"ROPE (robust MUSE).pdf\",\"1016\":\"ROPE (robust MUSE).pdf\",\"1017\":\"ROPE (robust MUSE).pdf\",\"1018\":\"ROPE (robust MUSE).pdf\",\"1019\":\"ROPE (robust MUSE).pdf\",\"1020\":\"ROPE (robust MUSE).pdf\",\"1021\":\"ROPE (robust MUSE).pdf\",\"1022\":\"ROPE (robust MUSE).pdf\",\"1023\":\"ROPE (robust MUSE).pdf\",\"1024\":\"ROPE (robust MUSE).pdf\",\"1025\":\"ROPE (robust MUSE).pdf\",\"1026\":\"ROPE (robust MUSE).pdf\",\"1027\":\"ROPE (robust MUSE).pdf\",\"1028\":\"ROPE (robust MUSE).pdf\",\"1029\":\"ROPE (robust MUSE).pdf\",\"1030\":\"ROPE (robust MUSE).pdf\",\"1031\":\"ROPE (robust MUSE).pdf\",\"1032\":\"ROPE (robust MUSE).pdf\",\"1033\":\"ROPE (robust MUSE).pdf\",\"1034\":\"ROPE (robust MUSE).pdf\",\"1035\":\"ROPE (robust MUSE).pdf\",\"1036\":\"ROPE (robust MUSE).pdf\",\"1037\":\"ROPE (robust MUSE).pdf\",\"1038\":\"ROPE (robust MUSE).pdf\",\"1039\":\"ROPE (robust MUSE).pdf\",\"1040\":\"ROPE (robust MUSE).pdf\",\"1041\":\"ROPE (robust MUSE).pdf\",\"1042\":\"ROPE (robust MUSE).pdf\",\"1043\":\"ROPE (robust MUSE).pdf\",\"1044\":\"ROPE (robust MUSE).pdf\",\"1045\":\"ROPE (robust MUSE).pdf\",\"1046\":\"ROPE (robust MUSE).pdf\",\"1047\":\"ROPE (robust MUSE).pdf\",\"1048\":\"ROPE (robust MUSE).pdf\",\"1049\":\"ROPE (robust MUSE).pdf\",\"1050\":\"ROPE (robust MUSE).pdf\",\"1051\":\"ROPE (robust MUSE).pdf\",\"1052\":\"ROPE (robust MUSE).pdf\",\"1053\":\"ROPE (robust MUSE).pdf\",\"1054\":\"ROPE (robust MUSE).pdf\",\"1055\":\"ROPE (robust MUSE).pdf\",\"1056\":\"ROPE (robust MUSE).pdf\",\"1057\":\"ROPE (robust MUSE).pdf\",\"1058\":\"ROPE (robust MUSE).pdf\",\"1059\":\"ROPE (robust MUSE).pdf\",\"1060\":\"ROPE (robust MUSE).pdf\",\"1061\":\"ROPE (robust MUSE).pdf\",\"1062\":\"ROPE (robust MUSE).pdf\",\"1063\":\"ROPE (robust MUSE).pdf\",\"1064\":\"ROPE (robust MUSE).pdf\",\"1065\":\"ROPE (robust MUSE).pdf\",\"1066\":\"ROPE (robust MUSE).pdf\",\"1067\":\"ROPE (robust MUSE).pdf\",\"1068\":\"ROPE (robust MUSE).pdf\",\"1069\":\"ROPE (robust MUSE).pdf\",\"1070\":\"ROPE (robust MUSE).pdf\",\"1071\":\"ROPE (robust MUSE).pdf\",\"1072\":\"ROPE (robust MUSE).pdf\",\"1073\":\"ROPE (robust MUSE).pdf\",\"1074\":\"ROPE (robust MUSE).pdf\",\"1075\":\"ROPE (robust MUSE).pdf\",\"1076\":\"ROPE (robust MUSE).pdf\",\"1077\":\"ROPE (robust MUSE).pdf\",\"1078\":\"ROPE (robust MUSE).pdf\",\"1079\":\"ROPE (robust MUSE).pdf\",\"1080\":\"ROPE (robust MUSE).pdf\",\"1081\":\"ROPE (robust MUSE).pdf\",\"1082\":\"ROPE (robust MUSE).pdf\",\"1083\":\"ROPE (robust MUSE).pdf\",\"1084\":\"ROPE (robust MUSE).pdf\",\"1085\":\"ROPE (robust MUSE).pdf\",\"1086\":\"ROPE (robust MUSE).pdf\",\"1087\":\"ROPE (robust MUSE).pdf\",\"1088\":\"ROPE (robust MUSE).pdf\",\"1089\":\"ROPE (robust MUSE).pdf\",\"1090\":\"ROPE (robust MUSE).pdf\",\"1091\":\"ROPE (robust MUSE).pdf\",\"1092\":\"ROPE (robust MUSE).pdf\",\"1093\":\"ROPE (robust MUSE).pdf\",\"1094\":\"ROPE (robust MUSE).pdf\",\"1095\":\"ROPE (robust MUSE).pdf\",\"1096\":\"ROPE (robust MUSE).pdf\",\"1097\":\"ROPE (robust MUSE).pdf\",\"1098\":\"ROPE (robust MUSE).pdf\",\"1099\":\"ROPE (robust MUSE).pdf\",\"1100\":\"ROPE (robust MUSE).pdf\",\"1101\":\"ROPE (robust MUSE).pdf\",\"1102\":\"ROPE (robust MUSE).pdf\",\"1103\":\"ROPE (robust MUSE).pdf\",\"1104\":\"ROPE (robust MUSE).pdf\",\"1105\":\"ROPE (robust MUSE).pdf\",\"1106\":\"ROPE (robust MUSE).pdf\",\"1107\":\"ROPE (robust MUSE).pdf\",\"1108\":\"ROPE (robust MUSE).pdf\",\"1109\":\"ROPE (robust MUSE).pdf\",\"1110\":\"ROPE (robust MUSE).pdf\",\"1111\":\"ROPE (robust MUSE).pdf\",\"1112\":\"ROPE (robust MUSE).pdf\",\"1113\":\"ROPE (robust MUSE).pdf\",\"1114\":\"ROPE (robust MUSE).pdf\",\"1115\":\"ROPE (robust MUSE).pdf\",\"1116\":\"ROPE (robust MUSE).pdf\",\"1117\":\"ROPE (robust MUSE).pdf\",\"1118\":\"ROPE (robust MUSE).pdf\",\"1119\":\"ROPE (robust MUSE).pdf\",\"1120\":\"ROPE (robust MUSE).pdf\",\"1121\":\"ROPE (robust MUSE).pdf\",\"1122\":\"ROPE (robust MUSE).pdf\",\"1123\":\"ROPE (robust MUSE).pdf\",\"1124\":\"ROPE (robust MUSE).pdf\",\"1125\":\"ROPE (robust MUSE).pdf\",\"1126\":\"ROPE (robust MUSE).pdf\",\"1127\":\"ROPE (robust MUSE).pdf\",\"1128\":\"ROPE (robust MUSE).pdf\",\"1129\":\"ROPE (robust MUSE).pdf\",\"1130\":\"ROPE (robust MUSE).pdf\",\"1131\":\"ROPE (robust MUSE).pdf\",\"1132\":\"ROPE (robust MUSE).pdf\",\"1133\":\"ROPE (robust MUSE).pdf\",\"1134\":\"ROPE (robust MUSE).pdf\",\"1135\":\"ROPE (robust MUSE).pdf\",\"1136\":\"ROPE (robust MUSE).pdf\",\"1137\":\"ROPE (robust MUSE).pdf\",\"1138\":\"ROPE (robust MUSE).pdf\",\"1139\":\"ROPE (robust MUSE).pdf\",\"1140\":\"ROPE (robust MUSE).pdf\",\"1141\":\"ROPE (robust MUSE).pdf\",\"1142\":\"ROPE (robust MUSE).pdf\",\"1143\":\"ROPE (robust MUSE).pdf\",\"1144\":\"ROPE (robust MUSE).pdf\",\"1145\":\"ROPE (robust MUSE).pdf\",\"1146\":\"ROPE (robust MUSE).pdf\",\"1147\":\"ROPE (robust MUSE).pdf\",\"1148\":\"ROPE (robust MUSE).pdf\",\"1149\":\"ROPE (robust MUSE).pdf\",\"1150\":\"ROPE (robust MUSE).pdf\",\"1151\":\"ROPE (robust MUSE).pdf\",\"1152\":\"ROPE (robust MUSE).pdf\",\"1153\":\"ROPE (robust MUSE).pdf\",\"1154\":\"ROPE (robust MUSE).pdf\",\"1155\":\"ROPE (robust MUSE).pdf\",\"1156\":\"ROPE (robust MUSE).pdf\",\"1157\":\"ROPE (robust MUSE).pdf\",\"1158\":\"ROPE (robust MUSE).pdf\",\"1159\":\"ROPE (robust MUSE).pdf\",\"1160\":\"ROPE (robust MUSE).pdf\",\"1161\":\"ROPE (robust MUSE).pdf\",\"1162\":\"ROPE (robust MUSE).pdf\",\"1163\":\"ROPE (robust MUSE).pdf\",\"1164\":\"ROPE (robust MUSE).pdf\",\"1165\":\"ROPE (robust MUSE).pdf\",\"1166\":\"ROPE (robust MUSE).pdf\",\"1167\":\"ROPE (robust MUSE).pdf\",\"1168\":\"ROPE (robust MUSE).pdf\",\"1169\":\"ROPE (robust MUSE).pdf\",\"1170\":\"ROPE (robust MUSE).pdf\",\"1171\":\"ROPE (robust MUSE).pdf\",\"1172\":\"ROPE (robust MUSE).pdf\",\"1173\":\"ROPE (robust MUSE).pdf\",\"1174\":\"ROPE (robust MUSE).pdf\",\"1175\":\"ROPE (robust MUSE).pdf\",\"1176\":\"ROPE (robust MUSE).pdf\",\"1177\":\"ROPE (robust MUSE).pdf\",\"1178\":\"ROPE (robust MUSE).pdf\",\"1179\":\"ROPE (robust MUSE).pdf\",\"1180\":\"ROPE (robust MUSE).pdf\",\"1181\":\"ROPE (robust MUSE).pdf\",\"1182\":\"ROPE (robust MUSE).pdf\",\"1183\":\"ROPE (robust MUSE).pdf\",\"1184\":\"ROPE (robust MUSE).pdf\",\"1185\":\"ROPE (robust MUSE).pdf\",\"1186\":\"ROPE (robust MUSE).pdf\",\"1187\":\"ROPE (robust MUSE).pdf\",\"1188\":\"ROPE (robust MUSE).pdf\",\"1189\":\"ROPE (robust MUSE).pdf\",\"1190\":\"ROPE (robust MUSE).pdf\",\"1191\":\"ROPE (robust MUSE).pdf\",\"1192\":\"ROPE (robust MUSE).pdf\",\"1193\":\"ROPE (robust MUSE).pdf\",\"1194\":\"ROPE (robust MUSE).pdf\",\"1195\":\"ROPE (robust MUSE).pdf\",\"1196\":\"ROPE (robust MUSE).pdf\",\"1197\":\"ROPE (robust MUSE).pdf\",\"1198\":\"ROPE (robust MUSE).pdf\",\"1199\":\"ROPE (robust MUSE).pdf\",\"1200\":\"ROPE (robust MUSE).pdf\",\"1201\":\"ROPE (robust MUSE).pdf\",\"1202\":\"ROPE (robust MUSE).pdf\",\"1203\":\"ROPE (robust MUSE).pdf\",\"1204\":\"ROPE (robust MUSE).pdf\",\"1205\":\"ROPE (robust MUSE).pdf\",\"1206\":\"ROPE (robust MUSE).pdf\",\"1207\":\"ROPE (robust MUSE).pdf\",\"1208\":\"ROPE (robust MUSE).pdf\",\"1209\":\"ROPE (robust MUSE).pdf\",\"1210\":\"ROPE (robust MUSE).pdf\",\"1211\":\"ROPE (robust MUSE).pdf\",\"1212\":\"ROPE (robust MUSE).pdf\",\"1213\":\"ROPE (robust MUSE).pdf\",\"1214\":\"ROPE (robust MUSE).pdf\",\"1215\":\"ROPE (robust MUSE).pdf\",\"1216\":\"ROPE (robust MUSE).pdf\",\"1217\":\"ROPE (robust MUSE).pdf\",\"1218\":\"ROPE (robust MUSE).pdf\",\"1219\":\"ROPE (robust MUSE).pdf\",\"1220\":\"ROPE (robust MUSE).pdf\",\"1221\":\"ROPE (robust MUSE).pdf\",\"1222\":\"ROPE (robust MUSE).pdf\",\"1223\":\"ROPE (robust MUSE).pdf\",\"1224\":\"ROPE (robust MUSE).pdf\",\"1225\":\"ROPE (robust MUSE).pdf\",\"1226\":\"ROPE (robust MUSE).pdf\",\"1227\":\"ROPE (robust MUSE).pdf\",\"1228\":\"ROPE (robust MUSE).pdf\",\"1229\":\"ROPE (robust MUSE).pdf\",\"1230\":\"ROPE (robust MUSE).pdf\",\"1231\":\"ROPE (robust MUSE).pdf\",\"1232\":\"ROPE (robust MUSE).pdf\",\"1233\":\"ROPE (robust MUSE).pdf\",\"1234\":\"ROPE (robust MUSE).pdf\",\"1235\":\"ROPE (robust MUSE).pdf\",\"1236\":\"ROPE (robust MUSE).pdf\",\"1237\":\"ROPE (robust MUSE).pdf\",\"1238\":\"ROPE (robust MUSE).pdf\",\"1239\":\"ROPE (robust MUSE).pdf\",\"1240\":\"ROPE (robust MUSE).pdf\",\"1241\":\"ROPE (robust MUSE).pdf\",\"1242\":\"ROPE (robust MUSE).pdf\",\"1243\":\"ROPE (robust MUSE).pdf\",\"1244\":\"ROPE (robust MUSE).pdf\",\"1245\":\"ROPE (robust MUSE).pdf\",\"1246\":\"ROPE (robust MUSE).pdf\",\"1247\":\"ROPE (robust MUSE).pdf\",\"1248\":\"ROPE (robust MUSE).pdf\",\"1249\":\"ROPE (robust MUSE).pdf\",\"1250\":\"ROPE (robust MUSE).pdf\",\"1251\":\"ROPE (robust MUSE).pdf\",\"1252\":\"ROPE (robust MUSE).pdf\",\"1253\":\"ROPE (robust MUSE).pdf\",\"1254\":\"ROPE (robust MUSE).pdf\",\"1255\":\"ROPE (robust MUSE).pdf\",\"1256\":\"ROPE (robust MUSE).pdf\",\"1257\":\"ROPE (robust MUSE).pdf\",\"1258\":\"ROPE (robust MUSE).pdf\",\"1259\":\"ROPE (robust MUSE).pdf\",\"1260\":\"ROPE (robust MUSE).pdf\",\"1261\":\"ROPE (robust MUSE).pdf\",\"1262\":\"ROPE (robust MUSE).pdf\",\"1263\":\"ROPE (robust MUSE).pdf\",\"1264\":\"ROPE (robust MUSE).pdf\",\"1265\":\"ROPE (robust MUSE).pdf\",\"1266\":\"ROPE (robust MUSE).pdf\",\"1267\":\"ROPE (robust MUSE).pdf\",\"1268\":\"ROPE (robust MUSE).pdf\",\"1269\":\"ROPE (robust MUSE).pdf\",\"1270\":\"ROPE (robust MUSE).pdf\",\"1271\":\"ROPE (robust MUSE).pdf\",\"1272\":\"ROPE (robust MUSE).pdf\",\"1273\":\"ROPE (robust MUSE).pdf\",\"1274\":\"ROPE (robust MUSE).pdf\",\"1275\":\"ROPE (robust MUSE).pdf\",\"1276\":\"ROPE (robust MUSE).pdf\",\"1277\":\"ROPE (robust MUSE).pdf\",\"1278\":\"ROPE (robust MUSE).pdf\",\"1279\":\"TreeView2.pdf\",\"1280\":\"TreeView2.pdf\",\"1281\":\"TreeView2.pdf\",\"1282\":\"TreeView2.pdf\",\"1283\":\"TreeView2.pdf\",\"1284\":\"TreeView2.pdf\",\"1285\":\"TreeView2.pdf\",\"1286\":\"TreeView2.pdf\",\"1287\":\"TreeView2.pdf\",\"1288\":\"TreeView2.pdf\",\"1289\":\"TreeView2.pdf\",\"1290\":\"TreeView2.pdf\",\"1291\":\"TreeView2.pdf\",\"1292\":\"TreeView2.pdf\",\"1293\":\"TreeView2.pdf\",\"1294\":\"TreeView2.pdf\",\"1295\":\"TreeView2.pdf\",\"1296\":\"TreeView2.pdf\",\"1297\":\"TreeView2.pdf\",\"1298\":\"TreeView2.pdf\",\"1299\":\"TreeView2.pdf\",\"1300\":\"TreeView2.pdf\",\"1301\":\"TreeView2.pdf\",\"1302\":\"TreeView2.pdf\",\"1303\":\"TreeView2.pdf\",\"1304\":\"TreeView2.pdf\",\"1305\":\"TreeView2.pdf\",\"1306\":\"TreeView2.pdf\",\"1307\":\"TreeView2.pdf\",\"1308\":\"TreeView2.pdf\",\"1309\":\"TreeView2.pdf\",\"1310\":\"TreeView2.pdf\",\"1311\":\"TreeView2.pdf\",\"1312\":\"TreeView2.pdf\",\"1313\":\"TreeView2.pdf\",\"1314\":\"TreeView2.pdf\",\"1315\":\"TreeView2.pdf\",\"1316\":\"TreeView2.pdf\",\"1317\":\"TreeView2.pdf\",\"1318\":\"TreeView2.pdf\",\"1319\":\"TreeView2.pdf\",\"1320\":\"TreeView2.pdf\",\"1321\":\"TreeView2.pdf\",\"1322\":\"TreeView2.pdf\",\"1323\":\"TreeView2.pdf\",\"1324\":\"TreeView2.pdf\",\"1325\":\"TreeView2.pdf\",\"1326\":\"TreeView2.pdf\",\"1327\":\"TreeView2.pdf\",\"1328\":\"TreeView2.pdf\",\"1329\":\"TreeView2.pdf\",\"1330\":\"TreeView2.pdf\",\"1331\":\"TreeView2.pdf\",\"1332\":\"TreeView2.pdf\",\"1333\":\"TreeView2.pdf\",\"1334\":\"TreeView2.pdf\",\"1335\":\"TreeView2.pdf\",\"1336\":\"TreeView2.pdf\",\"1337\":\"TreeView2.pdf\",\"1338\":\"TreeView2.pdf\",\"1339\":\"TreeView2.pdf\",\"1340\":\"TreeView2.pdf\",\"1341\":\"TreeView2.pdf\",\"1342\":\"TreeView2.pdf\",\"1343\":\"TreeView2.pdf\",\"1344\":\"TreeView2.pdf\",\"1345\":\"TreeView2.pdf\",\"1346\":\"TreeView2.pdf\",\"1347\":\"TreeView2.pdf\",\"1348\":\"TreeView2.pdf\",\"1349\":\"TreeView2.pdf\",\"1350\":\"TreeView2.pdf\",\"1351\":\"TreeView2.pdf\",\"1352\":\"TreeView2.pdf\",\"1353\":\"TreeView2.pdf\",\"1354\":\"TreeView2.pdf\",\"1355\":\"TreeView2.pdf\",\"1356\":\"TreeView2.pdf\",\"1357\":\"TreeView2.pdf\",\"1358\":\"TreeView2.pdf\",\"1359\":\"TreeView2.pdf\",\"1360\":\"TreeView2.pdf\",\"1361\":\"TreeView2.pdf\",\"1362\":\"TreeView2.pdf\",\"1363\":\"TreeView2.pdf\",\"1364\":\"TreeView2.pdf\",\"1365\":\"TreeView2.pdf\",\"1366\":\"TreeView2.pdf\",\"1367\":\"TreeView2.pdf\",\"1368\":\"TreeView2.pdf\",\"1369\":\"TreeView2.pdf\",\"1370\":\"TreeView2.pdf\",\"1371\":\"TreeView2.pdf\",\"1372\":\"TreeView2.pdf\",\"1373\":\"TreeView2.pdf\",\"1374\":\"TreeView2.pdf\",\"1375\":\"TreeView2.pdf\",\"1376\":\"TreeView2.pdf\",\"1377\":\"TreeView2.pdf\",\"1378\":\"TreeView2.pdf\",\"1379\":\"TreeView2.pdf\",\"1380\":\"TreeView2.pdf\",\"1381\":\"TreeView2.pdf\",\"1382\":\"TreeView2.pdf\",\"1383\":\"TreeView2.pdf\",\"1384\":\"TreeView2.pdf\",\"1385\":\"TreeView2.pdf\",\"1386\":\"TreeView2.pdf\",\"1387\":\"TreeView2.pdf\",\"1388\":\"TreeView2.pdf\",\"1389\":\"TreeView2.pdf\",\"1390\":\"TreeView2.pdf\",\"1391\":\"TreeView2.pdf\",\"1392\":\"TreeView2.pdf\",\"1393\":\"TreeView2.pdf\",\"1394\":\"TreeView2.pdf\",\"1395\":\"TreeView2.pdf\",\"1396\":\"TreeView2.pdf\",\"1397\":\"TreeView2.pdf\",\"1398\":\"TreeView2.pdf\",\"1399\":\"TreeView2.pdf\",\"1400\":\"TreeView2.pdf\",\"1401\":\"TreeView2.pdf\",\"1402\":\"TreeView2.pdf\",\"1403\":\"TreeView2.pdf\",\"1404\":\"TreeView2.pdf\",\"1405\":\"TreeView2.pdf\",\"1406\":\"TreeView2.pdf\",\"1407\":\"TreeView2.pdf\",\"1408\":\"TreeView2.pdf\",\"1409\":\"TreeView2.pdf\",\"1410\":\"TreeView2.pdf\",\"1411\":\"TreeView2.pdf\",\"1412\":\"TreeView2.pdf\",\"1413\":\"TreeView2.pdf\",\"1414\":\"TreeView2.pdf\",\"1415\":\"TreeView2.pdf\",\"1416\":\"TreeView2.pdf\",\"1417\":\"TreeView2.pdf\",\"1418\":\"TreeView2.pdf\",\"1419\":\"TreeView2.pdf\",\"1420\":\"TreeView2.pdf\",\"1421\":\"TreeView2.pdf\",\"1422\":\"TreeView2.pdf\",\"1423\":\"TreeView2.pdf\",\"1424\":\"TreeView2.pdf\",\"1425\":\"TreeView2.pdf\",\"1426\":\"TreeView2.pdf\",\"1427\":\"TreeView2.pdf\",\"1428\":\"TreeView2.pdf\",\"1429\":\"TreeView2.pdf\",\"1430\":\"TreeView2.pdf\",\"1431\":\"TreeView2.pdf\",\"1432\":\"TreeView2.pdf\",\"1433\":\"TreeView2.pdf\",\"1434\":\"TreeView2.pdf\",\"1435\":\"TreeView2.pdf\",\"1436\":\"TreeView2.pdf\",\"1437\":\"TreeView2.pdf\",\"1438\":\"TreeView2.pdf\",\"1439\":\"TreeView2.pdf\",\"1440\":\"TreeView2.pdf\",\"1441\":\"TreeView2.pdf\",\"1442\":\"TreeView2.pdf\",\"1443\":\"TreeView2.pdf\",\"1444\":\"TreeView2.pdf\",\"1445\":\"TreeView2.pdf\",\"1446\":\"TreeView2.pdf\",\"1447\":\"TreeView2.pdf\",\"1448\":\"TreeView2.pdf\",\"1449\":\"TreeView2.pdf\",\"1450\":\"TreeView2.pdf\",\"1451\":\"TreeView2.pdf\",\"1452\":\"TreeView2.pdf\",\"1453\":\"TreeView2.pdf\",\"1454\":\"TreeView2.pdf\",\"1455\":\"TreeView2.pdf\",\"1456\":\"TreeView2.pdf\",\"1457\":\"TreeView2.pdf\",\"1458\":\"TreeView2.pdf\",\"1459\":\"TreeView2.pdf\",\"1460\":\"TreeView2.pdf\",\"1461\":\"TreeView2.pdf\",\"1462\":\"TreeView2.pdf\"},\"sentence\":{\"0\":0,\"1\":1,\"2\":2,\"3\":3,\"4\":4,\"5\":5,\"6\":6,\"7\":7,\"8\":8,\"9\":9,\"10\":10,\"11\":11,\"12\":12,\"13\":13,\"14\":14,\"15\":15,\"16\":16,\"17\":17,\"18\":18,\"19\":19,\"20\":20,\"21\":21,\"22\":22,\"23\":23,\"24\":24,\"25\":25,\"26\":26,\"27\":27,\"28\":28,\"29\":29,\"30\":30,\"31\":31,\"32\":32,\"33\":33,\"34\":34,\"35\":35,\"36\":36,\"37\":37,\"38\":38,\"39\":39,\"40\":40,\"41\":41,\"42\":42,\"43\":43,\"44\":44,\"45\":45,\"46\":46,\"47\":47,\"48\":48,\"49\":49,\"50\":50,\"51\":51,\"52\":52,\"53\":53,\"54\":54,\"55\":55,\"56\":56,\"57\":57,\"58\":58,\"59\":59,\"60\":60,\"61\":61,\"62\":62,\"63\":63,\"64\":64,\"65\":65,\"66\":66,\"67\":67,\"68\":68,\"69\":69,\"70\":70,\"71\":71,\"72\":72,\"73\":73,\"74\":74,\"75\":75,\"76\":76,\"77\":77,\"78\":78,\"79\":79,\"80\":80,\"81\":81,\"82\":82,\"83\":83,\"84\":84,\"85\":85,\"86\":86,\"87\":87,\"88\":88,\"89\":89,\"90\":90,\"91\":91,\"92\":92,\"93\":93,\"94\":94,\"95\":95,\"96\":96,\"97\":97,\"98\":98,\"99\":99,\"100\":100,\"101\":101,\"102\":102,\"103\":103,\"104\":104,\"105\":105,\"106\":106,\"107\":107,\"108\":108,\"109\":109,\"110\":110,\"111\":111,\"112\":112,\"113\":113,\"114\":114,\"115\":115,\"116\":116,\"117\":117,\"118\":118,\"119\":119,\"120\":120,\"121\":121,\"122\":122,\"123\":123,\"124\":124,\"125\":125,\"126\":126,\"127\":127,\"128\":128,\"129\":129,\"130\":130,\"131\":131,\"132\":132,\"133\":133,\"134\":134,\"135\":135,\"136\":136,\"137\":137,\"138\":138,\"139\":139,\"140\":140,\"141\":141,\"142\":142,\"143\":143,\"144\":144,\"145\":145,\"146\":146,\"147\":147,\"148\":148,\"149\":149,\"150\":150,\"151\":151,\"152\":152,\"153\":153,\"154\":154,\"155\":155,\"156\":156,\"157\":157,\"158\":158,\"159\":159,\"160\":160,\"161\":161,\"162\":162,\"163\":163,\"164\":164,\"165\":165,\"166\":166,\"167\":167,\"168\":168,\"169\":169,\"170\":170,\"171\":171,\"172\":172,\"173\":173,\"174\":174,\"175\":175,\"176\":176,\"177\":177,\"178\":178,\"179\":179,\"180\":180,\"181\":181,\"182\":182,\"183\":183,\"184\":184,\"185\":185,\"186\":186,\"187\":187,\"188\":188,\"189\":189,\"190\":190,\"191\":191,\"192\":192,\"193\":193,\"194\":194,\"195\":195,\"196\":196,\"197\":197,\"198\":198,\"199\":199,\"200\":200,\"201\":201,\"202\":202,\"203\":203,\"204\":204,\"205\":205,\"206\":206,\"207\":207,\"208\":208,\"209\":209,\"210\":210,\"211\":211,\"212\":212,\"213\":213,\"214\":214,\"215\":215,\"216\":216,\"217\":217,\"218\":218,\"219\":219,\"220\":220,\"221\":221,\"222\":222,\"223\":223,\"224\":224,\"225\":225,\"226\":226,\"227\":227,\"228\":228,\"229\":229,\"230\":230,\"231\":231,\"232\":232,\"233\":233,\"234\":234,\"235\":235,\"236\":236,\"237\":237,\"238\":238,\"239\":239,\"240\":240,\"241\":241,\"242\":242,\"243\":243,\"244\":244,\"245\":245,\"246\":246,\"247\":247,\"248\":248,\"249\":249,\"250\":250,\"251\":251,\"252\":252,\"253\":253,\"254\":254,\"255\":255,\"256\":256,\"257\":257,\"258\":258,\"259\":259,\"260\":260,\"261\":261,\"262\":262,\"263\":263,\"264\":264,\"265\":265,\"266\":266,\"267\":267,\"268\":268,\"269\":269,\"270\":270,\"271\":271,\"272\":272,\"273\":273,\"274\":274,\"275\":275,\"276\":276,\"277\":277,\"278\":278,\"279\":279,\"280\":280,\"281\":0,\"282\":1,\"283\":2,\"284\":3,\"285\":4,\"286\":5,\"287\":6,\"288\":7,\"289\":8,\"290\":9,\"291\":10,\"292\":11,\"293\":12,\"294\":13,\"295\":14,\"296\":15,\"297\":16,\"298\":17,\"299\":18,\"300\":19,\"301\":20,\"302\":21,\"303\":22,\"304\":23,\"305\":24,\"306\":25,\"307\":26,\"308\":27,\"309\":28,\"310\":29,\"311\":30,\"312\":31,\"313\":32,\"314\":33,\"315\":34,\"316\":35,\"317\":36,\"318\":37,\"319\":38,\"320\":39,\"321\":40,\"322\":41,\"323\":42,\"324\":43,\"325\":44,\"326\":45,\"327\":46,\"328\":47,\"329\":48,\"330\":49,\"331\":50,\"332\":51,\"333\":52,\"334\":53,\"335\":54,\"336\":55,\"337\":56,\"338\":57,\"339\":58,\"340\":59,\"341\":60,\"342\":61,\"343\":62,\"344\":63,\"345\":64,\"346\":65,\"347\":66,\"348\":67,\"349\":68,\"350\":69,\"351\":70,\"352\":71,\"353\":72,\"354\":73,\"355\":74,\"356\":75,\"357\":76,\"358\":77,\"359\":78,\"360\":79,\"361\":80,\"362\":81,\"363\":82,\"364\":83,\"365\":84,\"366\":85,\"367\":86,\"368\":87,\"369\":88,\"370\":89,\"371\":90,\"372\":91,\"373\":92,\"374\":93,\"375\":94,\"376\":95,\"377\":96,\"378\":97,\"379\":98,\"380\":99,\"381\":100,\"382\":101,\"383\":102,\"384\":103,\"385\":104,\"386\":105,\"387\":106,\"388\":107,\"389\":108,\"390\":109,\"391\":110,\"392\":111,\"393\":112,\"394\":113,\"395\":114,\"396\":115,\"397\":116,\"398\":117,\"399\":118,\"400\":119,\"401\":120,\"402\":121,\"403\":122,\"404\":123,\"405\":124,\"406\":125,\"407\":126,\"408\":127,\"409\":128,\"410\":129,\"411\":130,\"412\":131,\"413\":132,\"414\":133,\"415\":134,\"416\":135,\"417\":136,\"418\":137,\"419\":138,\"420\":139,\"421\":140,\"422\":141,\"423\":142,\"424\":143,\"425\":144,\"426\":145,\"427\":146,\"428\":147,\"429\":148,\"430\":149,\"431\":150,\"432\":151,\"433\":152,\"434\":153,\"435\":154,\"436\":155,\"437\":156,\"438\":157,\"439\":158,\"440\":159,\"441\":160,\"442\":161,\"443\":162,\"444\":163,\"445\":164,\"446\":165,\"447\":166,\"448\":167,\"449\":168,\"450\":169,\"451\":170,\"452\":0,\"453\":1,\"454\":2,\"455\":3,\"456\":4,\"457\":5,\"458\":6,\"459\":7,\"460\":8,\"461\":9,\"462\":10,\"463\":11,\"464\":12,\"465\":13,\"466\":14,\"467\":15,\"468\":16,\"469\":17,\"470\":18,\"471\":19,\"472\":20,\"473\":21,\"474\":22,\"475\":23,\"476\":24,\"477\":25,\"478\":26,\"479\":27,\"480\":28,\"481\":29,\"482\":30,\"483\":31,\"484\":32,\"485\":33,\"486\":34,\"487\":35,\"488\":36,\"489\":37,\"490\":38,\"491\":39,\"492\":40,\"493\":41,\"494\":42,\"495\":43,\"496\":44,\"497\":45,\"498\":46,\"499\":47,\"500\":48,\"501\":49,\"502\":50,\"503\":51,\"504\":52,\"505\":53,\"506\":54,\"507\":55,\"508\":56,\"509\":57,\"510\":58,\"511\":59,\"512\":60,\"513\":61,\"514\":62,\"515\":63,\"516\":64,\"517\":65,\"518\":66,\"519\":67,\"520\":68,\"521\":69,\"522\":70,\"523\":71,\"524\":72,\"525\":73,\"526\":74,\"527\":75,\"528\":76,\"529\":77,\"530\":78,\"531\":79,\"532\":80,\"533\":81,\"534\":82,\"535\":83,\"536\":84,\"537\":85,\"538\":86,\"539\":87,\"540\":88,\"541\":89,\"542\":90,\"543\":91,\"544\":92,\"545\":93,\"546\":94,\"547\":95,\"548\":96,\"549\":97,\"550\":98,\"551\":99,\"552\":100,\"553\":101,\"554\":102,\"555\":103,\"556\":104,\"557\":105,\"558\":106,\"559\":107,\"560\":108,\"561\":109,\"562\":110,\"563\":111,\"564\":112,\"565\":113,\"566\":114,\"567\":115,\"568\":116,\"569\":117,\"570\":118,\"571\":119,\"572\":120,\"573\":121,\"574\":122,\"575\":123,\"576\":124,\"577\":125,\"578\":126,\"579\":127,\"580\":128,\"581\":129,\"582\":130,\"583\":131,\"584\":132,\"585\":133,\"586\":134,\"587\":135,\"588\":136,\"589\":137,\"590\":138,\"591\":139,\"592\":140,\"593\":141,\"594\":142,\"595\":143,\"596\":144,\"597\":145,\"598\":146,\"599\":147,\"600\":148,\"601\":149,\"602\":150,\"603\":151,\"604\":152,\"605\":153,\"606\":154,\"607\":155,\"608\":156,\"609\":157,\"610\":158,\"611\":159,\"612\":160,\"613\":161,\"614\":162,\"615\":163,\"616\":164,\"617\":165,\"618\":166,\"619\":167,\"620\":168,\"621\":169,\"622\":170,\"623\":171,\"624\":172,\"625\":173,\"626\":174,\"627\":175,\"628\":176,\"629\":177,\"630\":178,\"631\":179,\"632\":180,\"633\":181,\"634\":182,\"635\":183,\"636\":184,\"637\":185,\"638\":186,\"639\":187,\"640\":188,\"641\":189,\"642\":190,\"643\":191,\"644\":192,\"645\":193,\"646\":194,\"647\":195,\"648\":196,\"649\":197,\"650\":198,\"651\":199,\"652\":200,\"653\":201,\"654\":202,\"655\":203,\"656\":204,\"657\":205,\"658\":206,\"659\":207,\"660\":208,\"661\":209,\"662\":210,\"663\":211,\"664\":212,\"665\":213,\"666\":214,\"667\":215,\"668\":216,\"669\":217,\"670\":218,\"671\":219,\"672\":220,\"673\":221,\"674\":222,\"675\":223,\"676\":224,\"677\":225,\"678\":226,\"679\":227,\"680\":228,\"681\":229,\"682\":230,\"683\":231,\"684\":232,\"685\":233,\"686\":234,\"687\":235,\"688\":236,\"689\":237,\"690\":238,\"691\":239,\"692\":240,\"693\":241,\"694\":242,\"695\":243,\"696\":244,\"697\":245,\"698\":246,\"699\":247,\"700\":248,\"701\":249,\"702\":250,\"703\":251,\"704\":252,\"705\":253,\"706\":254,\"707\":255,\"708\":256,\"709\":257,\"710\":258,\"711\":259,\"712\":260,\"713\":261,\"714\":262,\"715\":263,\"716\":264,\"717\":265,\"718\":266,\"719\":267,\"720\":268,\"721\":269,\"722\":270,\"723\":271,\"724\":272,\"725\":273,\"726\":274,\"727\":275,\"728\":276,\"729\":277,\"730\":278,\"731\":279,\"732\":280,\"733\":281,\"734\":282,\"735\":283,\"736\":284,\"737\":285,\"738\":286,\"739\":287,\"740\":288,\"741\":289,\"742\":290,\"743\":291,\"744\":292,\"745\":293,\"746\":294,\"747\":295,\"748\":296,\"749\":297,\"750\":298,\"751\":299,\"752\":300,\"753\":301,\"754\":302,\"755\":303,\"756\":304,\"757\":305,\"758\":306,\"759\":307,\"760\":308,\"761\":309,\"762\":310,\"763\":311,\"764\":312,\"765\":313,\"766\":314,\"767\":315,\"768\":316,\"769\":317,\"770\":318,\"771\":319,\"772\":320,\"773\":321,\"774\":322,\"775\":323,\"776\":324,\"777\":325,\"778\":326,\"779\":327,\"780\":328,\"781\":329,\"782\":330,\"783\":331,\"784\":332,\"785\":333,\"786\":334,\"787\":335,\"788\":336,\"789\":337,\"790\":338,\"791\":339,\"792\":340,\"793\":341,\"794\":342,\"795\":343,\"796\":344,\"797\":345,\"798\":346,\"799\":347,\"800\":348,\"801\":349,\"802\":350,\"803\":351,\"804\":352,\"805\":353,\"806\":354,\"807\":355,\"808\":356,\"809\":357,\"810\":358,\"811\":359,\"812\":360,\"813\":361,\"814\":362,\"815\":363,\"816\":364,\"817\":365,\"818\":366,\"819\":367,\"820\":368,\"821\":369,\"822\":370,\"823\":371,\"824\":372,\"825\":373,\"826\":374,\"827\":375,\"828\":376,\"829\":377,\"830\":378,\"831\":379,\"832\":380,\"833\":381,\"834\":382,\"835\":383,\"836\":384,\"837\":385,\"838\":386,\"839\":0,\"840\":1,\"841\":2,\"842\":3,\"843\":4,\"844\":5,\"845\":6,\"846\":7,\"847\":8,\"848\":9,\"849\":10,\"850\":11,\"851\":12,\"852\":13,\"853\":14,\"854\":15,\"855\":16,\"856\":17,\"857\":18,\"858\":19,\"859\":20,\"860\":21,\"861\":22,\"862\":23,\"863\":24,\"864\":25,\"865\":26,\"866\":27,\"867\":28,\"868\":29,\"869\":30,\"870\":31,\"871\":32,\"872\":33,\"873\":34,\"874\":35,\"875\":36,\"876\":37,\"877\":38,\"878\":39,\"879\":40,\"880\":41,\"881\":42,\"882\":43,\"883\":44,\"884\":45,\"885\":46,\"886\":47,\"887\":48,\"888\":49,\"889\":50,\"890\":51,\"891\":52,\"892\":53,\"893\":54,\"894\":55,\"895\":56,\"896\":57,\"897\":58,\"898\":59,\"899\":60,\"900\":61,\"901\":62,\"902\":63,\"903\":64,\"904\":65,\"905\":66,\"906\":67,\"907\":68,\"908\":69,\"909\":70,\"910\":71,\"911\":72,\"912\":73,\"913\":74,\"914\":75,\"915\":76,\"916\":77,\"917\":78,\"918\":79,\"919\":80,\"920\":81,\"921\":82,\"922\":83,\"923\":84,\"924\":85,\"925\":86,\"926\":87,\"927\":88,\"928\":89,\"929\":90,\"930\":91,\"931\":92,\"932\":93,\"933\":94,\"934\":95,\"935\":96,\"936\":97,\"937\":98,\"938\":99,\"939\":100,\"940\":101,\"941\":102,\"942\":103,\"943\":104,\"944\":105,\"945\":106,\"946\":107,\"947\":108,\"948\":109,\"949\":110,\"950\":111,\"951\":112,\"952\":113,\"953\":114,\"954\":115,\"955\":116,\"956\":117,\"957\":118,\"958\":119,\"959\":120,\"960\":121,\"961\":122,\"962\":123,\"963\":124,\"964\":125,\"965\":126,\"966\":127,\"967\":128,\"968\":129,\"969\":130,\"970\":131,\"971\":132,\"972\":133,\"973\":134,\"974\":135,\"975\":136,\"976\":137,\"977\":138,\"978\":139,\"979\":140,\"980\":141,\"981\":142,\"982\":143,\"983\":144,\"984\":145,\"985\":146,\"986\":147,\"987\":148,\"988\":149,\"989\":150,\"990\":151,\"991\":152,\"992\":153,\"993\":154,\"994\":155,\"995\":156,\"996\":157,\"997\":158,\"998\":159,\"999\":160,\"1000\":161,\"1001\":162,\"1002\":163,\"1003\":164,\"1004\":165,\"1005\":166,\"1006\":167,\"1007\":168,\"1008\":169,\"1009\":170,\"1010\":171,\"1011\":172,\"1012\":173,\"1013\":174,\"1014\":175,\"1015\":176,\"1016\":177,\"1017\":178,\"1018\":179,\"1019\":180,\"1020\":181,\"1021\":182,\"1022\":183,\"1023\":184,\"1024\":185,\"1025\":186,\"1026\":187,\"1027\":188,\"1028\":189,\"1029\":190,\"1030\":191,\"1031\":192,\"1032\":193,\"1033\":194,\"1034\":195,\"1035\":196,\"1036\":197,\"1037\":198,\"1038\":199,\"1039\":200,\"1040\":201,\"1041\":202,\"1042\":203,\"1043\":204,\"1044\":205,\"1045\":206,\"1046\":207,\"1047\":208,\"1048\":209,\"1049\":210,\"1050\":211,\"1051\":212,\"1052\":213,\"1053\":214,\"1054\":215,\"1055\":216,\"1056\":217,\"1057\":218,\"1058\":219,\"1059\":220,\"1060\":221,\"1061\":222,\"1062\":223,\"1063\":224,\"1064\":225,\"1065\":226,\"1066\":227,\"1067\":228,\"1068\":229,\"1069\":230,\"1070\":231,\"1071\":232,\"1072\":233,\"1073\":234,\"1074\":235,\"1075\":236,\"1076\":237,\"1077\":238,\"1078\":239,\"1079\":240,\"1080\":241,\"1081\":242,\"1082\":243,\"1083\":244,\"1084\":245,\"1085\":246,\"1086\":247,\"1087\":248,\"1088\":249,\"1089\":250,\"1090\":251,\"1091\":252,\"1092\":253,\"1093\":254,\"1094\":255,\"1095\":256,\"1096\":257,\"1097\":258,\"1098\":259,\"1099\":260,\"1100\":261,\"1101\":262,\"1102\":263,\"1103\":264,\"1104\":265,\"1105\":266,\"1106\":267,\"1107\":268,\"1108\":269,\"1109\":270,\"1110\":271,\"1111\":272,\"1112\":273,\"1113\":274,\"1114\":275,\"1115\":276,\"1116\":277,\"1117\":278,\"1118\":279,\"1119\":280,\"1120\":281,\"1121\":282,\"1122\":283,\"1123\":284,\"1124\":285,\"1125\":286,\"1126\":287,\"1127\":288,\"1128\":289,\"1129\":290,\"1130\":291,\"1131\":292,\"1132\":293,\"1133\":294,\"1134\":295,\"1135\":296,\"1136\":297,\"1137\":298,\"1138\":299,\"1139\":300,\"1140\":301,\"1141\":302,\"1142\":303,\"1143\":304,\"1144\":305,\"1145\":306,\"1146\":307,\"1147\":308,\"1148\":309,\"1149\":310,\"1150\":311,\"1151\":312,\"1152\":313,\"1153\":314,\"1154\":315,\"1155\":316,\"1156\":317,\"1157\":318,\"1158\":319,\"1159\":320,\"1160\":321,\"1161\":322,\"1162\":323,\"1163\":324,\"1164\":325,\"1165\":326,\"1166\":327,\"1167\":328,\"1168\":329,\"1169\":330,\"1170\":331,\"1171\":332,\"1172\":333,\"1173\":334,\"1174\":335,\"1175\":336,\"1176\":337,\"1177\":338,\"1178\":339,\"1179\":340,\"1180\":341,\"1181\":342,\"1182\":343,\"1183\":344,\"1184\":345,\"1185\":346,\"1186\":347,\"1187\":348,\"1188\":349,\"1189\":350,\"1190\":351,\"1191\":352,\"1192\":353,\"1193\":354,\"1194\":355,\"1195\":356,\"1196\":357,\"1197\":358,\"1198\":359,\"1199\":360,\"1200\":361,\"1201\":362,\"1202\":363,\"1203\":364,\"1204\":365,\"1205\":366,\"1206\":367,\"1207\":368,\"1208\":369,\"1209\":370,\"1210\":371,\"1211\":372,\"1212\":373,\"1213\":374,\"1214\":375,\"1215\":376,\"1216\":377,\"1217\":378,\"1218\":379,\"1219\":380,\"1220\":381,\"1221\":382,\"1222\":383,\"1223\":384,\"1224\":385,\"1225\":386,\"1226\":387,\"1227\":388,\"1228\":389,\"1229\":390,\"1230\":391,\"1231\":392,\"1232\":393,\"1233\":394,\"1234\":395,\"1235\":396,\"1236\":397,\"1237\":398,\"1238\":399,\"1239\":400,\"1240\":401,\"1241\":402,\"1242\":403,\"1243\":404,\"1244\":405,\"1245\":406,\"1246\":407,\"1247\":408,\"1248\":409,\"1249\":410,\"1250\":411,\"1251\":412,\"1252\":413,\"1253\":414,\"1254\":415,\"1255\":416,\"1256\":417,\"1257\":418,\"1258\":419,\"1259\":420,\"1260\":421,\"1261\":422,\"1262\":423,\"1263\":424,\"1264\":425,\"1265\":426,\"1266\":427,\"1267\":428,\"1268\":429,\"1269\":430,\"1270\":431,\"1271\":432,\"1272\":433,\"1273\":434,\"1274\":435,\"1275\":436,\"1276\":437,\"1277\":438,\"1278\":439,\"1279\":0,\"1280\":1,\"1281\":2,\"1282\":3,\"1283\":4,\"1284\":5,\"1285\":6,\"1286\":7,\"1287\":8,\"1288\":9,\"1289\":10,\"1290\":11,\"1291\":12,\"1292\":13,\"1293\":14,\"1294\":15,\"1295\":16,\"1296\":17,\"1297\":18,\"1298\":19,\"1299\":20,\"1300\":21,\"1301\":22,\"1302\":23,\"1303\":24,\"1304\":25,\"1305\":26,\"1306\":27,\"1307\":28,\"1308\":29,\"1309\":30,\"1310\":31,\"1311\":32,\"1312\":33,\"1313\":34,\"1314\":35,\"1315\":36,\"1316\":37,\"1317\":38,\"1318\":39,\"1319\":40,\"1320\":41,\"1321\":42,\"1322\":43,\"1323\":44,\"1324\":45,\"1325\":46,\"1326\":47,\"1327\":48,\"1328\":49,\"1329\":50,\"1330\":51,\"1331\":52,\"1332\":53,\"1333\":54,\"1334\":55,\"1335\":56,\"1336\":57,\"1337\":58,\"1338\":59,\"1339\":60,\"1340\":61,\"1341\":62,\"1342\":63,\"1343\":64,\"1344\":65,\"1345\":66,\"1346\":67,\"1347\":68,\"1348\":69,\"1349\":70,\"1350\":71,\"1351\":72,\"1352\":73,\"1353\":74,\"1354\":75,\"1355\":76,\"1356\":77,\"1357\":78,\"1358\":79,\"1359\":80,\"1360\":81,\"1361\":82,\"1362\":83,\"1363\":84,\"1364\":85,\"1365\":86,\"1366\":87,\"1367\":88,\"1368\":89,\"1369\":90,\"1370\":91,\"1371\":92,\"1372\":93,\"1373\":94,\"1374\":95,\"1375\":96,\"1376\":97,\"1377\":98,\"1378\":99,\"1379\":100,\"1380\":101,\"1381\":102,\"1382\":103,\"1383\":104,\"1384\":105,\"1385\":106,\"1386\":107,\"1387\":108,\"1388\":109,\"1389\":110,\"1390\":111,\"1391\":112,\"1392\":113,\"1393\":114,\"1394\":115,\"1395\":116,\"1396\":117,\"1397\":118,\"1398\":119,\"1399\":120,\"1400\":121,\"1401\":122,\"1402\":123,\"1403\":124,\"1404\":125,\"1405\":126,\"1406\":127,\"1407\":128,\"1408\":129,\"1409\":130,\"1410\":131,\"1411\":132,\"1412\":133,\"1413\":134,\"1414\":135,\"1415\":136,\"1416\":137,\"1417\":138,\"1418\":139,\"1419\":140,\"1420\":141,\"1421\":142,\"1422\":143,\"1423\":144,\"1424\":145,\"1425\":146,\"1426\":147,\"1427\":148,\"1428\":149,\"1429\":150,\"1430\":151,\"1431\":152,\"1432\":153,\"1433\":154,\"1434\":155,\"1435\":156,\"1436\":157,\"1437\":158,\"1438\":159,\"1439\":160,\"1440\":161,\"1441\":162,\"1442\":163,\"1443\":164,\"1444\":165,\"1445\":166,\"1446\":167,\"1447\":168,\"1448\":169,\"1449\":170,\"1450\":171,\"1451\":172,\"1452\":173,\"1453\":174,\"1454\":175,\"1455\":176,\"1456\":177,\"1457\":178,\"1458\":179,\"1459\":180,\"1460\":181,\"1461\":182,\"1462\":183},\"text\":{\"0\":\"arXiv:1511.07361v1 [cs.LG] 23 Nov 2015 Interpretable Two-level Boolean Rule Learning for Classification Guolong Su\\u2217 Dennis Wei\\u2020 Kush R. Varshney\\u2020 Dmitry M. Malioutov\\u2020 Abstract This paper proposes algorithms for learning two-level Boolean rules in Conjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF, i.e. OR-of-ANDs) as a type of human-interpretable classification model, aiming for a favorable trade-off between the classification accuracy and the simplicity of the rule.\",\"1\":\"Two formulations are proposed.\",\"2\":\"The first is an integer program whose objective function is a combination of the total number of errors and the total number of features used in the rule.\",\"3\":\"We generalize a previously proposed linear programming (LP) relaxation from onelevel to two-level rules.\",\"4\":\"The second formulation replaces the 0-1 classification error with the Hamming distance from the current two-level rule to the closest rule that correctly classifies a sample.\",\"5\":\"Based on this second formulation, block coordinate descent and alternating minimization algorithms are developed.\",\"6\":\"Experiments show that the two-level rules can yield noticeably better performance than one-level rules due to their dramatically larger modeling capacity, and the two algorithms based on the Hamming distance formulation are generally superior to the other two-level rule learning methods in our comparison.\",\"7\":\"A proposed approach to binarize any fractional values in the optimal solutions of LP relaxations is also shown to be effective.\",\"8\":\"Keywords Interpretable Classifier, Linear Programming Relaxation 1 Introduction Boolean rules are an important classification model for machine learning and data mining.\",\"9\":\"A typical Boolean rule connects a subset of binary input features with the logical operators conjunction (\\u201cAND\\u201d), disjunction (\\u201cOR\\u201d), and negation (\\u201cNOT\\u201d) to form the prediction.\",\"10\":\"As an example, a Boolean rule in [8] for the prediction of 10 year coronary heart disease (CHD) risk for a 45 \\u2217Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, MA, USA. (email: guolong@mit.edu) \\u2020Mobile, Solutions, and Mathematical Sciences Department, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA. (email: dwei, krvarshn, dmalioutov@us.ibm.com) year old male can be expressed as follows: IF 1.\",\"11\":\"NOT smoke; OR 2. total cholesterol < 160; AND systolic blood pressure < 140; THEN (10 year CHD risk < 5%)=TRUE.\",\"12\":\"This is a two-level rule in DNF (OR-of-ANDs), where in the lower level, conjunctions of binary features build clauses and in the upper level, the disjunction of the clauses forms the predictor.\",\"13\":\"An advantage of Boolean rules is high human interpretability [3, 4].\",\"14\":\"The features included in the learned rule provide key reasons behind the prediction results; in the example above, not smoking may be the reason for the prediction of low 10 year CHD risk.\",\"15\":\"These reasons can be easily understood by the users.\",\"16\":\"Human interpretability has high importance in a wide range of applications such as medicine and business [4, 8], where results from prediction models are generally presented to a human decision maker\\/agent who makes the final decision.\",\"17\":\"Such a decision maker often needs an understanding of the reasons for the prediction before accepting the result; thus, high prediction accuracy without providing the reasons is not sufficient for the model to be trusted.\",\"18\":\"As an example, medical diagnosis models [8] may predict a high risk of certain diseases for a patient; a doctor then needs to know the underlying factors to compare with his\\/her domain knowledge, take the correct action, and communicate with the patient.\",\"19\":\"Another application requiring interpretability is fraud detection [6], where convincing reasons are needed to justify further auditing.\",\"20\":\"This paper considers learning two-level Boolean rules from datasets, with the joint criteria of both classification accuracy and human interpretability measured by the total number of features used (i.e. sparsity) [3].\",\"21\":\"Two optimization-based formulations are introduced.\",\"22\":\"The objective function in the first formulation is a weighted combination of the total number of classification errors and the sparsity, based on which we extend a previously proposed LP relaxation approach from one-level to two-level rules.\",\"23\":\"The second formulation replaces the 0-1 classification error cost with the Hamming distance from the current rule to the closest rule that correctly classifies a sample; we propose \\fblock coordinate descent and alternating minimization approaches for optimizing the objective in the second formulation.\",\"24\":\"To tackle the issue of fractional optimal solutions to LP relaxations, we introduce a new binarization method to convert LP solutions into binary values.\",\"25\":\"Experiments show that compared with one-level rules, the two-level rules can have noticeably lower error rate as well as more flexible accuracy-simplicity tradeoffs.\",\"26\":\"The two algorithms based on the Hamming distance formulation generally have superior performance among the approaches for two-level rule learning that we compare, and the new binarization method is shown to be effective.\",\"27\":\"The remainder of this paper is organized as follows.\",\"28\":\"Section 2 reviews related work and fields.\",\"29\":\"After the problem formulations in Section 3, optimization approaches are introduced in Section 4 and evaluated in Section 5.\",\"30\":\"Section 6 concludes this work.\",\"31\":\"2 Review of Existing Work The two-level Boolean rules in this work are examples of sparse decision rule lists, one of the major classes of interpretable models [4].\",\"32\":\"Decision trees constitute another class that can represent the same Boolean functions and be converted to decision rule lists [14], although they may differ in the representation complexity depending on the dataset [4].\",\"33\":\"Section 2.1 and 2.2 focus on existing work in learning one-level and two-level Boolean rules, respectively.\",\"34\":\"The one-level rule learning method in [10] forms a building block in the current work.\",\"35\":\"2.1 One-level Rule Learning in [10] A standard binary supervised classification problem is considered in [10].\",\"36\":\"We have a training dataset with n labeled samples; the ith sample has a binary label yi \\u2208 {0, 1} and in total d binary features ai,j \\u2208 {0, 1} (1 \\u2264 j \\u2264 d).\",\"37\":\"The goal is to learn a classifier y\\u0302(\\u00b7) that can generalize well to unseen feature vectors sampled from the same distribution as the training dataset.\",\"38\":\"The class of classifiers considered in [10] consists of one-level Boolean rules, which take only a conjunction (or disjunction) of selected features.\",\"39\":\"De Morgan\\u2019s laws show an equivalence between corresponding conjunctive and disjunctive rules y = ^ j\\u2208C xj \\u21d4 y = _ j\\u2208C xj, where y and xj mean the negation of binary variables y and xj, respectively.\",\"40\":\"Due to this equivalence, algorithms in [10] focus on the disjunctive rule y\\u0302i = d _ j=1 ai,jwj, for 1 \\u2264 i \\u2264 n, where binary decision variable wj \\u2208 {0, 1} indicates whether the jth feature is selected in the rule, and y\\u0302i \\u2208 {0, 1} is the prediction of the ith sample.\",\"41\":\"Replacing binary operators with linear-algebraic expressions, a mixed integer program is formulated for the one-level rule learning problem [10]: min wj n X i=1 \\u03bei + \\u03b8 \\u00b7 d X j=1 wj (2.1) s.t. \\u03bei = max \\uf8f1 \\uf8f2 \\uf8f3 0, \\uf8eb \\uf8ed1 \\u2212 d X j=1 ai,jwj \\uf8f6 \\uf8f8 \\uf8fc \\uf8fd \\uf8fe , for yi = 1, (2.2) \\u03bei = d X j=1 ai,jwj, for yi = 0, (2.3) wj \\u2208 {0, 1}, for 1 \\u2264 j \\u2264 d.\",\"42\":\"(2.4) The objective function is a combination of accuracy and sparsity with the balance controlled by the parameter \\u03b8.\",\"43\":\"The accuracy related costs \\u03bei for false negatives and false positives are formulated in (2.2) and (2.3), respectively.\",\"44\":\"Relaxation of (2.4) into 0 \\u2264 wj \\u2264 1 yields a linear program that is efficiently solvable [10].\",\"45\":\"Sufficient conditions for the relaxation to be exact are discussed in [10].\",\"46\":\"2.2 Two-level Rule Learning Two-level Boolean rules have significantly larger modeling capacity than one-level rules.\",\"47\":\"In fact, if we include the negations of input features, then two-level rules can represent any Boolean function of the input features [12, 13], which does not hold for one-level rules.\",\"48\":\"Two algorithms are proposed in [10] for rule set learning, based on the one-level learning algorithm.\",\"49\":\"The first algorithm uses the set covering approach [11] and obtains a two-level rule.\",\"50\":\"Suppose we want to learn a rule in DNF (OR-of-ANDs).\",\"51\":\"After training the first clause with the entire training set, we remove the samples with output 1 from the first clause; the predictions on these samples have been determined regardless of the other clauses.\",\"52\":\"Then we train the second clause with the remaining samples, and repeat this removetrain procedure for the rest of clauses.\",\"53\":\"Since this set covering approach is a one-pass greedy-style algorithm, there should be space for improvement.\",\"54\":\"The second algorithm for rule sets in [10] applies boosting, in which the predictor is a weighted combination of rules rather than a two-level rule and thus hinders interpretability.\",\"55\":\"Another algorithm for DNF learning is the Hamming Clustering (HC) approach [13], which uses greedy methods to iteratively cluster samples in the same category and with features close to each other in Hamming \\fdistance.\",\"56\":\"HC may be seen as bottom-up, whereas our algorithms are top-down and treat the training dataset more globally.\",\"57\":\"Experiments in [13] seem to imply HC produces a high number of clauses, which hinders interpretability.\",\"58\":\"There are a number of other methods and fields related to two-level rule learning.\",\"59\":\"First, Bayesian approaches in [8, 16] typically utilize approximate inference algorithms to obtain the MAP solution or to produce posterior distribution over decision lists.\",\"60\":\"However, the assignment of prior and likelihood in the Bayesian framework may not always be clear, and certain approximate inference algorithms may have high computational cost.\",\"61\":\"Second, Logical Analysis of Data (LAD) [2] learns patterns for both positive and negative samples by techniques such as set covering [11], and typically builds a classifier by a weighted combination of the patterns, i.e. not a two-level rule.\",\"62\":\"Third, learnability of Boolean formulae is considered in [7] from the perspective of probably approximately correct (PAC) learning.\",\"63\":\"Different from our problem, the setup of [7] and related work typically assumes positive or negative samples can be generated on demand and without noise.\",\"64\":\"Fourth, two-level logic optimization in circuit design [12] considers simplifying two-level rules that exactly match a given truth table.\",\"65\":\"However, in rule learning, it is neither needed nor desirable to exactly match a noisy dataset.\",\"66\":\"3 Problem Formulation The goal is to learn a two-level Boolean rule in the Conjunctive Normal Form (AND-of-ORs) from a training dataset1 , with the same setup of binary supervised classification as in Section 2.1.\",\"67\":\"In the lower level of the rule, we form each clause by the disjunction of a selected subset of input features; in the upper level, the final predictor is formed by the conjunction of all clauses.\",\"68\":\"Suppose the total number of clauses is fixed and denoted by R.\",\"69\":\"If we let the binary decision variables wj,r represent whether to include the jth feature in the rth clause, then the output of the rth clause for the ith sample is (3.5) v\\u0302i,r = d _ j=1 (ai,jwj,r) , for 1 \\u2264 i \\u2264 n, 1 \\u2264 r \\u2264 R.\",\"70\":\"Then, the predictor y\\u0302i satisfies (3.6) y\\u0302i = R ^ r=1 v\\u0302i,r, for 1 \\u2264 i \\u2264 n.\",\"71\":\"Although this setup has a fixed R, an option to \\u201cdisable\\u201d a clause can be introduced to reduce the total number 1We assume the negation of each feature is included as another input feature; if not, we can pad the input features with negations.\",\"72\":\"of actual clauses if the assigned R is too large.\",\"73\":\"For a CNF rule, a clause can be regarded as disabled if its output is always 1.\",\"74\":\"Thus, we can pad the input feature matrix with a trivial \\u201calways true\\u201d feature ai,0 = 1 for all samples, and also include the corresponding decision variables w0,r for all clauses.\",\"75\":\"The sparsity cost for w0,r can be lower than other variables or even zero.\",\"76\":\"If w0,r = 1, then the rth clause has output 1 and is thus disabled in the CNF rule.\",\"77\":\"This option might reduce accuracy with the tradeoff of improved sparsity.\",\"78\":\"In certain cases, DNF rules could be more natural than CNF.\",\"79\":\"A CNF learning algorithm can apply to DNF learning by De Morgan\\u2019s laws: y = R _ r=1 \\uf8eb \\uf8ed ^ j\\u2208Cr xj \\uf8f6 \\uf8f8 \\u21d4 y = R ^ r=1 \\uf8eb \\uf8ed _ j\\u2208Cr xj \\uf8f6 \\uf8f8 where Cr is the index set of features selected in the rth clause.\",\"80\":\"To learn a DNF rule with a CNF learning algorithm, we can first negate both features and labels of all samples, then learn a CNF rule with the negated features and labels, and finally use the decision variables wj,r with the original features to construct a DNF rule.\",\"81\":\"Thus, Sections 3 and 4 focus on CNF only.\",\"82\":\"Two formulations are introduced with different accuracy costs in Section 3.1 and 3.2, respectively.\",\"83\":\"3.1 Formulation with 0-1 Error Cost A natural choice of the accuracy-related cost is the total number of misclassifications (i.e. 0-1 error for each sample).\",\"84\":\"With the sparsity cost as the sum of the number of features used in each clause, a formulation is as below min wj,r n X i=1 |y\\u0302i \\u2212 yi| + \\u03b8 \\u00b7 R X r=1 d X j=1 wj,r (3.7) s.t. y\\u0302i = R ^ r=1 \\uf8eb \\uf8ed d _ j=1 (ai,jwj,r) \\uf8f6 \\uf8f8 , for 1 \\u2264 i \\u2264 n, (3.8) wj,r \\u2208 {0, 1}, for 1 \\u2264 j \\u2264 d, 1 \\u2264 r \\u2264 R.\",\"85\":\"There are a few challenges in this formulation.\",\"86\":\"First, the same as one-level rule learning, the two-level rule learning problem is combinatorial.\",\"87\":\"Second, all the clauses are symmetric in (3.7) and (3.8), however, we generally would like the clauses to be distinct since duplication of clauses is inefficient.\",\"88\":\"3.2 Formulation with Minimal Hamming Distance This formulation has the two motivations below.\",\"89\":\"First, it is potentially desirable to have a finer-grained accuracy cost than the 0-1 error in (3.7).\",\"90\":\"As an example, consider two CNF rules, both with two clauses, predicting the same sample with ground truth label yi = 1.\",\"91\":\"Suppose both clauses in the first rule predict 0, while only one clause in the second rule predicts 0 and the other predicts 1.\",\"92\":\"Although both rules misclassify this sample after taking \\u201cAND\\u201d of their two clauses, the second rule is closer to correct than the first one.\",\"93\":\"If we use an iterative algorithm to refine the learned rule, it might be beneficial for the accuracy cost term to favor the second rule in this example, which could push the solution towards being correct.\",\"94\":\"The second motivation for this formulation is to avoid identical clauses by training each clause with a different subset of samples, as done in [10, 11, 15].\",\"95\":\"In the new formulation, the accuracy cost for a single sample is the minimal Hamming distance from a given CNF rule to an ideal CNF rule, where the latter means a rule that correctly classifies this sample.\",\"96\":\"The Hamming distance between two CNF rules is the total number of wj,r that are different in the two rules.\",\"97\":\"An intuitive explanation of this minimal Hamming distance is the smallest number of modifications (i.e. negations) of the current rule wj,r that are needed to correct a misclassification on a sample, i.e. how far is the rule from being correct.\",\"98\":\"For mathematical formulation, we introduce ideal clause outputs vi,r with 1 \\u2264 i \\u2264 n and 1 \\u2264 r \\u2264 R to represent a CNF rule that correctly classifies the ith sample.\",\"99\":\"The values of vi,r are always consistent with the ground truth labels, i.e. yi = VR r=1 vi,r for all 1 \\u2264 i \\u2264 n.\",\"100\":\"We let vi,r have a ternary alphabet {0, 1, DC}, where vi,r = DC means that we \\u201cdon\\u2019t care\\u201d about the value of vi,r.\",\"101\":\"With this setup, if yi = 1, then vi,r = 1 for all 1 \\u2264 r \\u2264 R; if yi = 0, then vi,r0 = 0 for at least one value of r0, and we can have vi,r = DC for all r 6= r0.\",\"102\":\"In implementation, vi,r = DC implies the removal of the ith sample in the training or updating for the rth clause, which generally leads to a different training subset for each clause.\",\"103\":\"Denote \\u03b7i as the minimal Hamming distance from the current CNF rule wj,r to an ideal CNF rule for the ith sample.\",\"104\":\"We derive \\u03b7i for positive and negative samples, respectively.\",\"105\":\"Since yi = 1 implies vi,r = 1 for all r, for each clause with output 0 in the current rule, at least one positive feature needs to be included to match vi,r = 1.\",\"106\":\"Thus, the minimal Hamming distance for a positive sample is the number of clauses with output 0: \\u03b7i = R X r=1 max \\uf8f1 \\uf8f2 \\uf8f3 0, \\uf8eb \\uf8ed1 \\u2212 d X j=1 ai,jwj,r \\uf8f6 \\uf8f8 \\uf8fc \\uf8fd \\uf8fe , for yi = 1.\",\"107\":\"For yi = 0, we first consider for fixed r the minimal Hamming distance between the rth clauses of the current rule and an ideal rule where vi,r = 0.\",\"108\":\"We need to negate wj,r in the current rule for j with wj,r = ai,j = 1 to match vi,r = 0, and thus the minimal Hamming distance of this clause is Pd j=1 ai,jwj,r.\",\"109\":\"Then, since vi,r = 0 needs to hold for at least one value of r while all other vi,r can be DC, the minimal Hamming distance of the CNF rule is given by the minimum over r, i.e. setting vi,r0 = 0 with (3.9) r0 = arg min 1\\u2264r\\u2264R \\uf8eb \\uf8ed d X j=1 ai,jwj,r \\uf8f6 \\uf8f8 .\",\"110\":\"Combining all analysis above, the new formulation with the minimal Hamming distance cost is as below min wj,r n X i=1 \\u03b7i + \\u03b8 \\u00b7 R X r=1 d X j=1 wj,r (3.10) s.t. \\u03b7i = R X r=1 max \\uf8f1 \\uf8f2 \\uf8f3 0, \\uf8eb \\uf8ed1 \\u2212 d X j=1 ai,jwj,r \\uf8f6 \\uf8f8 \\uf8fc \\uf8fd \\uf8fe , for yi = 1, \\u03b7i = min 1\\u2264r\\u2264R \\uf8eb \\uf8ed d X j=1 ai,jwj,r \\uf8f6 \\uf8f8 , for yi = 0, (3.11) wj,r \\u2208 {0, 1}, for 1 \\u2264 j \\u2264 d, 1 \\u2264 r \\u2264 R.\",\"111\":\"The binary variables wj,r can be further relaxed to 0 \\u2264 wj,r \\u2264 1.\",\"112\":\"The minimum over r in (3.11) implies the continuous relaxation of (3.10) is generally non-convex with R > 1, making the exact solution challenging.\",\"113\":\"Letting R = 1 in formulation (3.10), we can see it becomes identical to formulation (2.1) in one-level rule learning [10].\",\"114\":\"Thus, the accuracy cost in (2.1) could be interpreted as the minimal Hamming distance.\",\"115\":\"To simplify description of algorithms later, we show a formulation (3.12) below, which is equivalent to (3.10) but involves both vi,r and wj,r.\",\"116\":\"Taking the minimization over vi,r in (3.12) with fixed wj,r eliminates the variables vi,r, and (3.12) becomes identical to (3.10).\",\"117\":\"min wj,r, vi,r n X i=1 R X r=1 \\\" 1vi,r=1 \\u00b7 max ( 0, 1 \\u2212 d X j=1 ai,jwj,r !)\",\"118\":\"(3.12) + 1vi,r=0 \\u00b7 d X j=1 ai,jwj,r # + \\u03b8 \\u00b7 R X r=1 d X j=1 wj,r s.t. R ^ r=1 vi,r = yi, for 1 \\u2264 i \\u2264 n, (3.13) vi,r \\u2208 {0, 1, DC}, for 1 \\u2264 i \\u2264 n, 1 \\u2264 r \\u2264 R, wj,r \\u2208 {0, 1}, for 1 \\u2264 j \\u2264 d, 1 \\u2264 r \\u2264 R.\",\"119\":\"4 Optimization Approaches This section discusses various optimization approaches to the two-level rule learning problem.\",\"120\":\"Based on the for\\fmulation in Section 3.1, we generalize the LP approach from one-level rule learning to two-level rules by proper relaxation in Section 4.1.\",\"121\":\"Based on the formulation in Section 3.2, we propose the block coordinate descent algorithm in Section 4.2 and the alternating minimization algorithm in Section 4.3 for the objective (3.12).\",\"122\":\"Since all algorithms utilize LP relaxations, Section 4.4 considers the binarization problem if the result of the LP is not binary.\",\"123\":\"4.1 Two-level Linear Programming Relaxation This approach considers the 0-1 error formulation (3.7) and directly generalizes the idea of replacing binary operations \\u201cAND\\u201d and \\u201cOR\\u201d with linear-algebraic operations, as used in one-level rule learning [10].\",\"124\":\"Since \\u201cAND\\u201d and \\u201cOR\\u201d are defined only on binary points, there are various interpolations of these functions on fractional points, and thus both convex and concave interpolations exist for both operators.\",\"125\":\"The \\u201cOR\\u201d function has the following interpolations [5] d _ j=1 xj = max 1\\u2264j\\u2264d {xj} = min \\uf8f1 \\uf8f2 \\uf8f3 1, d X j=1 xj \\uf8fc \\uf8fd \\uf8fe , where the first is convex and the second is concave, both of which are the respective tightest interpolations.\",\"126\":\"The logical \\u201cAND\\u201d operator also has the tightest convex and concave interpolations as [5] d ^ j=1 xj = max \\uf8f1 \\uf8f2 \\uf8f3 0, \\uf8eb \\uf8ed d X j=1 xj \\uf8f6 \\uf8f8 \\u2212 (d \\u2212 1) \\uf8fc \\uf8fd \\uf8fe = min 1\\u2264j\\u2264d {xj}.\",\"127\":\"Since the predictor y\\u0302i of the two-level rule in (3.8) is a composition of \\u201cAND\\u201d and \\u201cOR\\u201d operators, it is possible to properly interpolate it using both a convex function and a concave function by composing the individual interpolations of the two operators.\",\"128\":\"From (3.5) and (3.6), a convex interpolation of y\\u0302i is y\\u0302i = max ( 0, R X r=1 max 1\\u2264j\\u2264d {ai,jwj,r} !\",\"129\":\"\\u2212 (R \\u2212 1) ) , and a concave interpolation can be obtained similarly.\",\"130\":\"Denote the 0-1 error cost for the ith sample as \\u03c8i.\",\"131\":\"Since the errors \\u03c8i in (3.7) should be minimized, if yi = 1, then \\u03c8i = 1 \\u2212 y\\u0302i and thus we need the concave interpolation for y\\u0302i; if yi = 0, then \\u03c8i = y\\u0302i and thus the convex interpolation is needed.\",\"132\":\"Finally, the formulation in (3.7) can be exactly converted into a mixed integer program as follows: min wj,r,\\u03c8i,\\u03b2i,r n X i=1 \\u03c8i + \\u03b8 \\u00b7 R X r=1 d X j=1 wj,r (4.14) s.t. \\u03c8i \\u2265 0, \\u2200i, \\u03c8i \\u2265 1 \\u2212 d X j=1 ai,jwj,r, for yi = 1, \\u2200r, \\u03c8i \\u2265 R X r=1 \\u03b2i,r !\",\"133\":\"\\u2212 (R \\u2212 1), for yi = 0, \\u03b2i,r \\u2265 ai,jwj,r, for yi = 0, \\u2200j, \\u2200r, wj,r \\u2208 {0, 1}, \\u2200j, \\u2200r.\",\"134\":\"If we relax the decision variables wj,r to the interval [0, 1], then we have a linear program.\",\"135\":\"Unfortunately, numerical results seem to suggest that this LP relaxation is likely to have fractional values in the optimal solution wj,r, and the optimal \\u03c8i may possibly be all close to 0, which may be undesirable since \\u03c8i aims to represent the 0-1 error cost term.\",\"136\":\"A possible reason is that the gap between the convex and concave interpolations may loosen the LP and enable fractional results with lower cost than binary solutions.\",\"137\":\"4.2 Block Coordinate Descent Algorithm This algorithm considers the decision variables in a single clause (wj,r with a fixed r) as a block of coordinates, and performs block coordinate descent to minimize the Hamming distance objective function in (3.12).\",\"138\":\"Each iteration updates a single clause with all the other (R \\u2212 1) clauses fixed, using the one-level rule learning algorithm [10].\",\"139\":\"We denote r0 as the clause to be updated.\",\"140\":\"The optimization of (3.12) even with (R\\u22121) clauses fixed still involves a joint minimization over wj,r0 and the ideal clause outputs vi,r for yi = 0 (vi,r = 1 for yi = 1 and thus fixed), so the exact solution could still be challenging.\",\"141\":\"To simplify, we fix the values of vi,r for yi = 0 and r 6= r0 to the actual clause outputs v\\u0302i,r in (3.5) with the current wj,r (r 6= r0).\",\"142\":\"Now we assign vi,r0 for yi = 0: if there exists vi,r = v\\u0302i,r = 0 with r 6= r0, then this sample is guaranteed to be correctly classified and we can assign vi,r0 = DC to minimize the objective in (3.12); in contrast, if v\\u0302i,r = 1 holds for all r 6= r0, then the constraint (3.13) requires vi,r0 = 0.\",\"143\":\"This derivation leads to the updating process as follows.\",\"144\":\"To update the rth 0 clause, we remove all samples that have label yi = 0 and are already predicted as 0 by at least one of the other (R \\u2212 1) clauses, and then update the rth 0 clause with the remaining samples using the one-level rule learning algorithm [10].\",\"145\":\"There are different choices of which clause to update in an iteration.\",\"146\":\"For example, we can update clauses cyclically or randomly, or we can try the update for each clause and then greedily choose the one with the minimum cost.\",\"147\":\"The greedy update is used in our experiments.\",\"148\":\"The initialization of wj,r in this algorithm also has different choices.\",\"149\":\"For example, one option is the set covering method [10], as is used in our experiments.\",\"150\":\"Random or all-zero initialization can also be used.\",\"151\":\"4.3 Alternating Minimization Algorithm This algorithm uses the Hamming distance formulation (3.12) and alternately minimizes with respect to the decision variables wj,r and the ideal clause outputs vi,r.\",\"152\":\"Each iteration has two steps: update vi,r with the current wj,r, and update wj,r with the new vi,r.\",\"153\":\"The latter step is simpler and will be first discussed.\",\"154\":\"With fixed values of vi,r, the minimization over wj,r is relatively straight-forward: the objective in (3.12) becomes separated into R terms, each of which depends only on a single clause wj,r with a fixed r.\",\"155\":\"Thus, all clauses are decoupled in the minimization over wj,r, and the problem becomes parallel learning of R onelevel clauses.\",\"156\":\"Explicitly, the update of the rth clause will first remove all the samples with vi,r = DC, and then utilize the one-level rule learning algorithm [10].\",\"157\":\"The update over vi,r with fixed wj,r follows the discussion in Section 3.2: for positive samples yi = 1, vi,r = 1, and for the negative samples yi = 0, vi,r0 = 0 for r0 defined in (3.9) and vi,r = DC for r 6= r0.\",\"158\":\"For negative samples with a \\u201ctie\\u201d, i.e. non-unique r0 in (3.9), tie breaking is achieved by a \\u201cclustering\\u201d approach similar to the spirit of [13].\",\"159\":\"First, for each clause 1 \\u2264 r0 \\u2264 R, we compute its cluster center in the feature space by taking the average of ai,j (for each j) over samples i for which r0 is minimal in (3.9) (including ties).\",\"160\":\"Then, each sample with a tie is assigned to the clause with the closest cluster center in \\u21131-norm among all minimal r0 in (3.9).\",\"161\":\"Similar to the block coordinate descent algorithm, various options exist for initializing wj,r in this algorithm.\",\"162\":\"The set covering approach [10] is used in our experiments.\",\"163\":\"4.4 Redundancy Aware Binarization This section discusses a solution to a potential issue with the LP relaxation that is widely used in the algorithms proposed in this paper.\",\"164\":\"Although there are conditions under which the optimal solution to the LP relaxation for one-level rule learning is guaranteed to be binary [10], we are not aware of similar guarantees in two-level rule learning; in addition, these conditions are unlikely to always hold with a real-world and noisy dataset.\",\"165\":\"Thus, the optimal solution to LP may have fractional values, in which case we need to convert them into binary.\",\"166\":\"If LP already yields a binary optimal solution, then the binarization methods here will not change it.\",\"167\":\"A straight-forward binarization method is to compare each wj,r from LP with a specified threshold, as done in [10].\",\"168\":\"However, empirical results seem to suggest that the resulting binarized rule may have redundancy, making the rule unnecessarily complex and possibly influencing the accuracy.\",\"169\":\"The following improved binarization method considers three types of redundancy sets of binary features in a single disjunctive clause.\",\"170\":\"Among the features in each redundancy sets, no more than one feature will appear in any single clause of the optimal CNF rule2 .\",\"171\":\"The first type of redundancy set corresponds to nested features.\",\"172\":\"If binary features ai,j1 and ai,j2 satisfy ai,j1 \\u2264 ai,j2 for all samples, then these two features cannot both appear in a single clause in the optimal CNF rule; otherwise, since ai,j1 W ai,j2 = ai,j2 , removing ai,j1 from the clause keeps the same output and improves the sparsity, leading to a better rule.\",\"173\":\"If there is a nested set ai,j1 \\u2264 ai,j2 \\u2264 .\",\"174\":\".\",\"175\":\".\",\"176\":\"\\u2264 ai,jP (\\u22001 \\u2264 i \\u2264 n), then at most one feature from this set can be selected in a single clause in the optimal CNF rule.\",\"177\":\"The second type consists of complementary binary features, when we have the option to \\u201cdisable\\u201d a clause as explained in Section 3.\",\"178\":\"Since complementary features ai,j1 and ai,j2 satisfy ai,j1 W ai,j2 = 1 (\\u2200i), the optimal CNF rule cannot have both of them in a single clause, otherwise disabling this clause by w0,r = 1 and wj,r = 0 (j > 0) keeps the output and improves sparsity.\",\"179\":\"The third type also applies only when we have the option to disable a clause.\",\"180\":\"This type can happen with two nested sets that are pairwise complementary, especially if some binary features are obtained by thresholding continuous valued features.\",\"181\":\"For example, suppose we have six binary features from thresholding the same continuous feature ci with thresholds \\u03c41 < \\u03c42 < \\u03c43: ai,1 = (ci \\u2264 \\u03c41) , ai,2 = (ci \\u2264 \\u03c42) , ai,3 = (ci \\u2264 \\u03c43) , ai,4 = (ci > \\u03c41) , ai,5 = (ci > \\u03c42) , ai,6 = (ci > \\u03c43) .\",\"182\":\"The \\u201czigzag\\u201d path (ai,4, ai,5, ai,2, ai,3) forms a redundancy set, since at most one out of the four features can be selected in a fixed clause of the optimal CNF rule, otherwise either the first or the second redundancies above will happen and thus the rule is not optimal.\",\"183\":\"There are typically multiple \\u201czigzag\\u201d paths, e.g. 2This statement holds for both formulations (3.7) and (3.12); for simplicity, we will focus on the 0-1 error cost formulation for illustration.\",\"184\":\"(ai,4, ai,1, ai,2, ai,3) and (ai,4, ai,5, ai,6, ai,3).\",\"185\":\"The new binarization approach takes the above types of redundancies into account.\",\"186\":\"For illustration, suppose all binary features are obtained by thresholding continuous valued features.\",\"187\":\"In a clause and for a fixed continuous valued feature, we sweep over all nonredundant combinations of the binary features induced by this continuous feature and obtain the one with minimal cost.\",\"188\":\"Since the total number of non-redundant combinations for nested and zigzag features is linear and quadratic with the number of thresholds, respectively, the sweeping is relatively efficient with a single continuous feature.\",\"189\":\"However, joint minimization across all continuous features seems combinatorial and challenging.\",\"190\":\"Thus, we first sort continuous features in the decreasing order by the sum of corresponding decision variables in the optimal solution to the LP relaxation, and then sequentially binarize the decision variables induced by each continuous feature.\",\"191\":\"5 Numerical Evaluation 5.1 Setup This section evaluates the algorithms with UCI repository datasets [9], including connectionist bench sonar (Sonar), BUPA liver disorders (Liver), Pima Indian diabetes (Pima), and Parkinsons (Parkin).\",\"192\":\"The continuous valued features in these datasets are converted to binary using quantile thresholds.\",\"193\":\"The goal is to learn a DNF rule (OR-of-ANDs) from each dataset.\",\"194\":\"We use stratified 10-fold cross validation, and then average the test and training error rates over these 10 folds.\",\"195\":\"All LPs are solved by CPLEX version 12 [1].\",\"196\":\"The sparsity parameter \\u03b8 = A\\u00d710B where we sweep A = 1, 2, 5 and B = \\u22124, \\u22123, \\u22122, \\u22121, 0, 1, for a total of 18 values.\",\"197\":\"We sweep the total number of clauses in the DNF rule between R = 1 and R = 5; the option to \\u201cdisable\\u201d a clause (which can reduce R) is not used in the evaluation, except in Section 5.5 where we compare the results with\\/without such an option.\",\"198\":\"Algorithms in comparison and their abbreviations are: two-level LP relaxation (TLP), block coordinate descent (BCD), alternating minimization (AM), set covering [10, 11] (SCS: simple binarization with threshold at 0.2, SCN: new redundancy-aware binarization), decision list [15] in IBM SPSS (DL), and decision trees [14] (C5.0: C5.0 with rule set option in IBM SPSS, CART: classification and regression trees algorithm in Matlab\\u2019s classregtree function).\",\"199\":\"TLP, BCD, and AM all use the redundancy-aware binarization.\",\"200\":\"The maximum number of iterations in BCD and AM is set as 100.\",\"201\":\"We show results on both the minimal average test error rate obtained from the 18 different values of \\u03b8 and the Pareto front for the tradeoff between accuracy and sparsity.\",\"202\":\"Table 1: Minimal Average Test Error Rate (unit: %) Dataset R SCS SCN TLP BCD AM DL C5.0 CART 1 30.3 25.5 25.5 25.5 25.5 2 29.8 23.6 28.4 25.5 21.2 Sonar 3 26.0 22.1 30.8 25.5 24.0 38.5 25.0 28.4 4 25.5 23.6 29.8 20.2 22.6 5 28.4 23.6 29.8 23.6 18.3 1 42.0 42.0 39.7 42.0 42.0 2 40.6 40.9 41.4 35.7 34.2 Liver 3 40.3 41.4 40.9 36.8 33.6 45.2 36.5 37.7 4 39.7 40.6 42.0 37.1 34.2 5 39.4 40.6 41.7 33.9 33.0 1 26.6 26.7 25.4 26.7 26.7 2 26.7 26.2 28.0 24.9 22.7 Pima 3 27.5 26.4 26.7 25.7 23.7 31.4 24.9 28.9 4 27.5 27.1 26.3 25.7 24.1 5 27.5 27.1 25.1 25.7 24.9 1 15.9 15.9 14.4 15.9 15.9 2 13.8 13.8 14.9 12.8 14.4 Parkin 3 14.4 14.4 14.4 12.3 13.8 25.1 16.4 12.8 4 14.4 14.4 14.9 13.3 14.9 5 14.4 14.4 14.9 13.3 15.4 5.2 Minimal Average Test Error Rate The minimal average test error rates achieved among the 18 values of \\u03b8 for all algorithms are listed in Table 1, where R denotes the total number of clauses.\",\"203\":\"The results for DL, C5.0, and CART are cited from [10].\",\"204\":\"We refer the reader to [10] for results from other classifiers that are generally not interpretable; the accuracy of our algorithms is generally quite competitive with them.\",\"205\":\"For each algorithm and each dataset, the number marked with bold font is the lowest error rate among 1 \\u2264 R \\u2264 5.\",\"206\":\"There are a few observations from these results.\",\"207\":\"First, most bold-font numbers appear in rows with R > 1.\",\"208\":\"Since R = 1 corresponds to the one-level rules while R > 1 corresponds to two-level rules, the two-level rules can reduce error rate on these datasets, especially significant for block coordinate descent and alternating minimization algorithms.\",\"209\":\"Second, the block coordinate descent and alternating minimization algorithms generally have superior performance to the other methods for two-level rule learning in our comparison; however, the two-level LP relaxation does not seem to have as good performance.\",\"210\":\"Thus, we focus on block coordinate descent and alternating minimization algorithms in the remainder of this section.\",\"211\":\"Third, for the Sonar dataset with the same R, the set covering approach with new binarization has noticeably lower error rates than with simple binarization, which shows the effectiveness of the redundancy-aware binarization.\",\"212\":\"Fourth, we can see that for a fixed dataset and a fixed algorithm, the error rate does not decrease monotonically with R, indicating overfitting with too many clauses.\",\"213\":\"As a preliminary comparison with the Hamming Clustering approach [13], we consider \\u201cPima\\u201d which is the only dataset shared by this work, [10], and [13].\",\"214\":\"HC has 25.0% test error rate with an average of 85 features used in the rule as reported in [13], while block coordinate descent and alternating minimization algorithms have lower minimal error rates of 24.9% (average of 6.3 features used) and 22.7% (average of 6 features used) when R = 2, respectively.\",\"215\":\"Thus, our algorithms on Pima dataset produce rules with higher accuracy and significantly fewer features used 3 .\",\"216\":\"5.3 Pareto Fronts with Different Numbers of Clauses The Pareto fronts with different numbers of clauses are shown in Fig. 1, where we vary R from 1 to 5.\",\"217\":\"Fig. 1 (a) and (b) show the average test and training error rates of the alternating minimization algorithm on the Pima dataset, while Fig. 1 (c) and (d) show the error rates of the block coordinate descent algorithm on the Liver dataset.\",\"218\":\"Each point in the figure corresponds to the pair of average error rate and the average number of features in the learned DNF rule that is obtained at one of the 18 values of \\u03b8, and the Pareto fronts are denoted by lines for ease of visualization.\",\"219\":\"The following observations are implied by Fig. 1.\",\"220\":\"First, a comparison of the Pareto fronts of R = 1 and R > 1 suggests that two-level rules may have more flexible tradeoff between accuracy and simplicity.\",\"221\":\"Second, as shown in Fig. 1 (b) and (d), with the increase of R, the learned rule typically uses more features and has lower training error rates.\",\"222\":\"However, the exact tendency of the Pareto front of the test error rate may depend on the complexity of datasets: in Fig. 1 (a), the Pareto front becomes worse with the increase of R when R > 2, implying overfitting on this relatively simple dataset; in contrast, for the relatively complex Liver dataset in Fig. 1 (c), the minimum test error rate has a decrease at R = 5 with more features used, which seems to suggest that R = 5 does not overfit yet. 5.4 Pareto Fronts of Different Algorithms The Pareto fronts of the average test error rates for different algorithms on the Sonar and Liver datasets with R = 5 are shown in Fig. 2 (a) and (b), respectively.\",\"223\":\"Comparing the block coordinate descent and alternating minimization algorithms, we can see that when the total number of features used is very small, the block 3There are two differences in setup: HC uses 12-fold cross validation [13], while we use 10-fold; the parameters to convert continuous features into binary may potentially be different.\",\"224\":\"Total Number of Features Used 0 5 10 15 20 25 Average Test Error Rate 0.2 0.22 0.24 0.26 0.28 0.3 R=1 R=2 R=3 R=4 R=5 (a) Total Number of Features Used 0 5 10 15 20 25 Average Training Error Rate 0.18 0.2 0.22 0.24 0.26 R=1 R=2 R=3 R=4 R=5 (b) Total Number of Features Used 0 5 10 15 Average Test Error Rate 0.3 0.35 0.4 0.45 0.5 R=1 R=2 R=3 R=4 R=5 (c) Total Number of Features Used 0 5 10 15 Average Training Error Rate 0.2 0.25 0.3 0.35 0.4 0.45 R=1 R=2 R=3 R=4 R=5 (d) Figure 1: Comparison of Pareto Fronts with Different Numbers of Clauses: (a) Pima AM Test Error Rate, (b) Pima AM Training Error Rate, (c) Liver BCD Test Error Rate, (d) Liver BCD Training Error Rate.\",\"225\":\"coordinate descent algorithm typically has lower error rate; however, when the total number of features used increases, the alternating minimization algorithm may start to outperform.\",\"226\":\"Comparing the set covering approach with the simple and new binarization, the new binarization generally obtains sparser rules with improved or similar accuracy.\",\"227\":\"5.5 Pareto Fronts with\\/without the Option to Disable a Clause Fig. 3 shows the comparison of Pareto fronts of the average test error rates with and without the option to \\u201cdisable\\u201d a clause by an \\u201calways true\\u201d feature.\",\"228\":\"This option generally improves sparsity, while the error rate may remain similar or increase.\",\"229\":\"6 Conclusion This paper has provided two optimization-based formulations for two-level Boolean rule learning, the first based on 0-1 classification error and the second on Hamming distance.\",\"230\":\"Three algorithms have been developed, namely the two-level LP relaxation, block coordinate descent, and alternating minimization.\",\"231\":\"A redundancyaware binarization method has been introduced.\",\"232\":\"Numerical results show that two-level Boolean \\fTotal Number of Features Used 0 10 20 30 40 Average Test Error Rate 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 SCS SCN BCD AM (a) Total Number of Features Used 0 5 10 15 20 25 Average Test Error Rate 0.3 0.35 0.4 0.45 0.5 SCS SCN BCD AM (b) Figure 2: Comparison of Pareto Fronts with Different Algorithms: (a) Sonar R = 5, (b) Liver R = 5.\",\"233\":\"Total Number of Features Used 0 5 10 15 20 25 Average Test Error Rate 0.2 0.22 0.24 0.26 0.28 0.3 Without \\\"Disable\\\" Option With \\\"Disable\\\" Option (a) Total Number of Features Used 0 5 10 15 Average Test Error Rate 0.3 0.35 0.4 0.45 Without \\\"Disable\\\" Option With \\\"Disable\\\" Option (b) Figure 3: Comparison of Pareto Fronts with and without the Option to Disable a Clause: (a) Pima AM R = 5, (b) Liver BCD R = 5. rules have noticeably lower error rate and more flexible accuracy-simplicity tradeoffs on certain complex datasets than one-level rules.\",\"234\":\"However, too many clauses may cause overfitting, and the optimal number of clauses may depend on the complexity of the dataset.\",\"235\":\"The block coordinate descent and alternating minimization algorithms can work with noisy datasets and generally outperform the other methods for two-level rule learning in our comparison.\",\"236\":\"For the tradeoff between accuracy and simplicity, block coordinate descent algorithm may dominate alternating minimization when we require the total number of features used to be very small; in contrast, alternating minimization algorithm may outperform with more features used.\",\"237\":\"The new redundancy-aware binarization has been shown more effective than simple thresholding binarization in certain situations.\",\"238\":\"Acknowledgment The authors thank for V. S. Iyengar, A. Mojsilovic\\u0301, K. N. Ramamurthy, and E. van den Berg for conversations and support.\",\"239\":\"The authors are thankful for the assistance in experiments by using datasets from [9].\",\"240\":\"References [1] IBM ILOG CPLEX optimization studio.\",\"241\":\"http:\\/\\/www03.ibm.com\\/software\\/products\\/en\\/ibmilogcpleoptistud.\",\"242\":\"[2] E. Boros, P. L. Hammer, T. Ibaraki, A. Kogan, E. Mayoraz, and I. Muchnik, An implementation of logical analysis of data, IEEE Trans.\",\"243\":\"Knowl.\",\"244\":\"Data Eng., 12 (2000), pp. 292\\u2013306.\",\"245\":\"[3] J. Feldman, Minimization of Boolean complexity in human concept learning, Nature, 407 (2000), pp. 630\\u2013 633.\",\"246\":\"[4] A.\",\"247\":\"A. Freitas, Comprehensible classification models \\u2013 a position paper, ACM SIGKDD Explor., 15 (2014), pp. 1\\u201310.\",\"248\":\"[5] R. Ha\\u0308hnle, Proof theory of many-valued logic\\u2013linear optimization\\u2013logic design: Connections and interactions, Soft Comput., 1 (1997), pp. 107\\u2013119.\",\"249\":\"[6] V. S. Iyengar, K. B. Hermiz, and R. Natarajan, Computer-aided auditing of prescription drug claims, Health Care Manag.\",\"250\":\"Sci., 17 (2014), pp. 203\\u2013214.\",\"251\":\"[7] M. Kearns, M. Li, L. Pitt, and L. Valiant, On the learnability of Boolean formulae, in Proc.\",\"252\":\"Annu.\",\"253\":\"ACM Symp.\",\"254\":\"on Theory of Comput., 1987, pp. 285\\u2013295.\",\"255\":\"[8] B. Letham, C. Rudin, T. H. McCormick, and D. Madigan, Building interpretable classifiers with rules using Bayesian analysis, Department of Stat.\",\"256\":\"Tech.\",\"257\":\"Report tr609, Univ.\",\"258\":\"of Washington, (2012).\",\"259\":\"[9] M. Lichman, UCI machine learning repository.\",\"260\":\"http:\\/\\/archive.ics.uci.edu\\/ml, Univ.\",\"261\":\"of Calif., Irvine, School of Information and Computer Sciences, 2013.\",\"262\":\"[10] D. M. Malioutov and K. R. Varshney, Exact rule learning via Boolean compressed sensing, in Proc.\",\"263\":\"Int. Conf.\",\"264\":\"Mach.\",\"265\":\"Learn., 2013, pp. 765\\u2013773.\",\"266\":\"[11] M. Marchand and J. Shawe-Taylor, The set covering machine, J. Mach.\",\"267\":\"Learn.\",\"268\":\"Res., 3 (2002), pp. 723\\u2013 746.\",\"269\":\"[12] P. C. McGeer, J. V. Sanghavi, R. K. Brayton, and A. L. Sangiovanni-Vincentelli, ESPRESSOSIGNATURE: A new exact minimizer for logic functions, IEEE Trans.\",\"270\":\"VLSI Syst., 1 (1993), pp. 432\\u2013440.\",\"271\":\"[13] M. Muselli and D. Liberati, Binary rule generation via Hamming Clustering, IEEE Trans.\",\"272\":\"Knowl.\",\"273\":\"Data Eng., 14 (2002), pp. 1258\\u20131268.\",\"274\":\"[14] J.\",\"275\":\"R. Quinlan, Simplifying decision trees, Int. J. ManMach.\",\"276\":\"Studies, 27 (1987), pp. 221\\u2013234.\",\"277\":\"[15] R. L. Rivest, Learning decision lists, Mach.\",\"278\":\"Learn., 2 (1987), pp. 229\\u2013246.\",\"279\":\"[16] T. Wang, C. Rudin, F. Doshi-Velez, Y. Liu, E. Klampfl, and P. MacNeille, Bayesian Or\\u2019s of And\\u2019s for interpretable classification with application to context aware recommender systems, tech. report, MIT, 2015.\",\"280\":\"Submitted.\",\"281\":\"Interpretable & Explorable Approximations of Black Box Models Himabindu Lakkaraju Stanford University himalv@cs.stanford.edu Ece Kamar Microsoft Research eckamar@microsoft.com Rich Caruana Microsoft Research rcaruana@microsoft.com Jure Leskovec Stanford University jure@cs.stanford.edu ABSTRACT We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation.\",\"282\":\"To this end, we develop a novel objective function which allows us to learn (with optimality guarantees), a small number of compact decision sets each of which explains the behavior of the black box model in unambiguous, well-defined regions of feature space.\",\"283\":\"Furthermore, our framework also is capable of accepting user input when generating these approximations, thus allowing users to interactively explore how the black-box model behaves in different subspaces that are of interest to the user.\",\"284\":\"To the best of our knowledge, this is the first approach which can produce global explanations of the behavior of any given black box model through joint optimization of unambiguity, fidelity, and interpretability, while also allowing users to explore model behavior based on their preferences.\",\"285\":\"Experimental evaluation with realworld datasets and user studies demonstrates that our approach can generate highly compact, easy-to-understand, yet accurate approximations of various kinds of predictive models compared to state-of-the-art baselines.\",\"286\":\"1 INTRODUCTION The successful adoption of predictive models in settings such as criminal justice and health care hinges on how much judges and doctors can understand and trust the functionality of these machine learning models.\",\"287\":\"Only if decision makers have a clear understanding of the behavior of predictive models, they can evaluate when and how much to depend on these models, detect potential biases in them, and develop strategies for further model refinement.\",\"288\":\"However, the increasing complexity of predictive models is making it harder to explain or reason about their behavior [8], thus, emphasizing the need for tools which can explain the complex behavior of predictive models in a faithful and interpretable manner.\",\"289\":\"Prior research on interpretable machine learning mainly focused on learning predictive models from scratch which were human understandable.\",\"290\":\"Examples of such models include decision trees [9], decision lists [6], decision sets [4], linear models, generalized additive models [7] etc.\",\"291\":\"More recently, Ribeiro et.\",\"292\":\"al. [8] and Wei et.\",\"293\":\"al. [3] proposed approaches to explain individual predictions of KDD\\u201917, Workshop on Fairness, Accountability, and Transparency in Machine Learning \\u00a9 Copyright held by the owner\\/author(s).\",\"294\":\".\",\"295\":\"any black box classifier.\",\"296\":\"Ribeiro et.\",\"297\":\"al. [8] proposed an approach which explains individual predictions of any classifier by generating locally interpretable models.\",\"298\":\"They then approximate the global behavior of the classifier by choosing certain representative instances and their corresponding locally interpretable models.\",\"299\":\"This approach, however, does not clearly specify which of the multiple locally interpretable models are applicable to which part of the feature space.\",\"300\":\"If Age <50 and Male =Yes: If Past-Depression =Yes and Insomnia =No and Melancholy =No, then Healthy If Past-Depression =Yes and Insomnia =Yes and Melancholy =Yes and Tiredness =Yes, then Depression If Age \\u2265 50 and Male =No: If Family-Depression =Yes and Insomnia =No and Melancholy =Yes and Tiredness =Yes, then Depression If Family-Depression =No and Insomnia =No and Melancholy =No and Tiredness =No, then Healthy Default: If Past-Depression =Yes and Tiredness =No and Exercise =No and Insomnia =Yes, then Depression If Past-Depression =No and Weight-Gain =Yes and Tiredness =Yes and Melancholy =Yes, then Depression If Family-Depression =Yes and Insomnia =Yes and Melancholy =Yes and Tiredness =Yes, then Depression Figure 1: Explanations generated by our approach on depression dataset when approximating a deep neural network Here, we study the problem of constructing global explanations of black box classifiers.\",\"301\":\"Our goal is to explain the behavior of any given black-box classifier as a whole (i.e., globally) instead of just reasoning about its individual predictions.\",\"302\":\"To this end, we propose a framework BETAwhich constructs a small number of compact decision sets (sets of if-then rules) each of which captures the behavior of the given black box model in certain parts of the feature space (see Figure 1).\",\"303\":\"To ensure that the resulting explanations are faithful to the original model, we choose approximations based on how well they mimic the original model in terms of assigning class labels to instances.\",\"304\":\"Our framework also unambiguously specifies the rationale used for assigning labels to instances in any part of the feature space by ensuring that each decision set and the corresponding decision rules explain non-overlapping parts of the feature space.\",\"305\":\"To ensure that the resulting explanations are interpretable, we not only employ an intuitive rule based representation but also focus on minimizing its complexity in terms of the number of rules, predicates etc. Our framework also allows users to explore how the original model behaves in subspaces characterized by different values of the features that are of interest to the user.\",\"306\":\"arXiv:1707.01154v1 [cs.AI] 4 Jul 2017 \\fTo address the problem at hand, we propose a novel optimization problem which incorporates all the aforementioned aspects.\",\"307\":\"While exactly optimizing our objective is an NP-hard problem, it has a specific structure which allows for provably near-optimal solutions.\",\"308\":\"In particular, we prove that our optimization problem is a non-normal, non-monotone submodular function with matroid constraints.\",\"309\":\"We then employ an efficient optimization procedure based on approximate local search [5] which provides the best known approximation guarantees (\\u223c 1\\/5) to solve our optimization problem.\",\"310\":\"Experimental results on a real-world depression diagnosis dataset indicate that our approach can generate much less complex and high fidelity approximations compared to state-of-the-art baselines.\",\"311\":\"We also carried out user studies in which we asked human subjects to reason about a black box model\\u2019s behavior using the approximations generated by our approach and other state-of-the-art baselines.\",\"312\":\"Results of this study demonstrate that the approximations generated by our approach allow humans to accurately and quickly reason about the behavior of complex predictive models.\",\"313\":\"2 OUR FRAMEWORK In this work, the goal of creating approximations which can meaningfully explain the behavior of any black box model is guided by the following properties: Fidelity: The approximation should correctly capture the black box model behavior in all parts of the feature space.\",\"314\":\"While different notions of fidelity can be defined, one possible way this can be achieved is through the labels assigned by the approximation matching the labels assigned by the black box model for most instances (ideally all instances) in the data.\",\"315\":\"Unambiguity: The approximation should provide a single, deterministic rationale for explaining the prediction of every instance in the data and consequently should unambiguously specify the rationale used for assigning labels to instances in any part of the feature space.\",\"316\":\"Interpretability: The approximation that we construct should be human-understandable.\",\"317\":\"While choosing an interpretable representation (e.g., rule based models, linear models, decision trees\\/sets) is a minimal requirement, it is not sufficient to ensure interpretability.\",\"318\":\"Cognitive limitations of humans place restrictions on the complexity of the approximations that are understandable to humans.\",\"319\":\"For example, a decision tree with a hundred levels cannot be considered interpretable.\",\"320\":\"Therefore, it is important to not only have an intuitive representation but also to have smaller complexity (e.g., fewer rules in case of rule based models, fewer features with nonzero coefficients in case of linear models).\",\"321\":\"Interactivity Users might want to understand the decision logic in subspaces characterized by certain feature values (e.g., How does the model behave for patients over the age of 50 vs. patients under the age of 30?).\",\"322\":\"In this case, a generic explanation of the behavior of the black box model may not be ideal \\u2013 the features the user is interested in may not even appear in this generic explanation.\",\"323\":\"This scenario highlights the need for customized approximations which allow users to explore the behavior of black box models based on their preferences.\",\"324\":\"2.1 Our Representation: Two Level Decision Sets We choose two level decision sets as the representation of our approximations.\",\"325\":\"The basic building block of this structure is a decision set which is a set of if-then rules that are unordered.\",\"326\":\"The two level decision set can be regarded as a set of multiple decision sets, each of which is embedded within an outer if-then structure, such that the inner if-then rules represent the decision logic employed by the black box model while labeling instances within the subspace characterized by the conditions in the outer if-then clauses.\",\"327\":\"Consequently, we refer to the conditions in the outer if-then rules as neighborhood descriptors and the inner if-then rules as decision logic rules.\",\"328\":\"While the expressive power of two level decision sets is the same as that of other rule based models (e.g., decision sets\\\\lists\\\\trees), the nesting of if-then clauses in a two level decision set representation enables the optimization algorithm (discussed later) to select neighborhod descriptors and decision logic rules such that higher fidelity can be obtained with minimal complexity thus resulting in more compact approximations compared to conventional decision sets (more details in experiments section).\",\"329\":\"In addition, two level decision set representation does not have the pitfalls associated with decision lists where understanding a particular rule requires reasoning about all the previously encountered rules because of the if-else-if construct [4].\",\"330\":\"Definition 1.\",\"331\":\"A two level decision set R is a set of rules {(q1,s1,c1) \\u00b7 \\u00b7 \\u00b7 (qM ,sM ,cM )} where qi and si are conjunctions of predicates of the form (f eature,operator,value) (eg., a\\u0434e \\u2265 50) and ci is a class label.\",\"332\":\"qi corresponds to the subspace descriptor and (si,ci ) together represent the inner if-then rules (decision logic rules) with si denoting the condition and ci denoting the class label.\",\"333\":\"A two level decision set assigns a label to an instance x as follows: if x satisfies exactly one of the rules i i.e., x satisfies qi \\u2227 si , then its label is the corresponding class label ci .\",\"334\":\"If x satisfies none of the rules in R, then its label is assigned using a default function and if x satisfies more than one rule in R then its label is assigned using a tie-breaking function.\",\"335\":\"1 In our experiments, we employ a default function which computes the majority class label (assigned by the black box model) of all the instances in the training data which do not satisfy any rule in R and assigns them to this majority label.\",\"336\":\"For each instance which is assigned to more than one rule in R, we break ties by choosing the rule which has a higher agreement rate with the black box model.\",\"337\":\"Other forms of default and tie-breaking functions can be easily incorporated into our framework.\",\"338\":\"1Note that the optimization problem that we formulate in Section 2.2.2 will ensure that the need to invoke default or tie-breaking functions is minimized.\",\"339\":\"2 \\f2.2 Black Box Explanations through Transparent Approximations Next, we show how to quantify the desiderata presented earlier in the context of two-level decision sets, then formulate it as an objective function and propose an optimization procedure.\",\"340\":\"2.2.1 Quantifying Fidelity, Unambiguity, and Interpretability.\",\"341\":\"Table 1 shows how we can quantify the properties discussed earlier w.r.t a two level decision set approximation R, a black box model B, and a dataset D = {x1,x2 \\u00b7 \\u00b7 \\u00b7xN } where xi captures the feature values of instance i.\",\"342\":\"We treat the black box model B as a function which takes an instance x \\u2208 D as input and returns a class label.\",\"343\":\"Quantifying Fidelity: disagreement(R) quantifies the infidelity of approximation R to the black box model B by summing up for each rule (q,s,c) in R, the number of instances which satisfy q \\u2227 s but for which the label assigned by the black box model B does not match the label c. Quantifying Unambiguity: For every pair of rules (qi,si,ci ) and (qj,sj,cj ) in R where i , j, we compute the number of instances which satisfy both qi \\u2227 si and qj \\u2227 sj , sum up all these counts.\",\"344\":\"This sum is denoted by ruleoverlap(R).\",\"345\":\"Furthermore, it is important that the approximation that we generate explain or cover as much of the feature space (ideally, all of it) as possible.\",\"346\":\"This notion is captured by cover(R), which is the number of those instances which satisfy the condition q \\u2227 s associated with some rule (q,s,c) in R. Quantifying Interpretability size(R) is the number of rules (triples of the form (q,s,c)) in the two level decision set R. maxwidth(R) is the maximum width computed over all the elements in R, where each element is either a condition of some decision logic rule s or a neighborhood descriptor q.numpreds(R) counts the number of predicates in R including those appearing in both the decision logic rules and neighborhood descriptors.\",\"347\":\"Note that the predicates of neighborhood descriptors are counted multiple times as a neighborhood descriptor q could potentially appear alongside multiple decision logic rules.\",\"348\":\"numdsets(R) is the number of unique neighborhood descriptors (outer if clauses) in R.\",\"349\":\"In a two-level decision set, each neighborhood descriptor characterizes a specific region of the feature space and the corresponding inner if-then rules specify the decision logic of the black box model within that region.\",\"350\":\"To make this distinction clear, we minimize the number of overlapping features.\",\"351\":\"For every pair of a unique neighborhood descriptor q and a decision logic rule s, we compute the number of features that occur in both q and s (f eatureoverlap(q,s)) and then sum up these counts.\",\"352\":\"The resulting sum is denoted as featureoverlap(R).\",\"353\":\"2.2.2 Optimization Problem.\",\"354\":\"We assume we are given as inputs a dataset D, labels assigned to instances in D by black box model B, a set of possible class labels C, a candidate set of conjunctions of predicates (Eg., Age \\u2265 50 and Gender = Female) ND from which we can pick the neighborhood descriptors, and another candidate set of conjunctions of predicates DL from which we can choose the decision logic rules.\",\"355\":\"In practice, a frequent itemset mining Table 1: Measures for Fidelity, Interpretability and Unambiguity Fidelity disa\\u0434reement(R) = M \\u00cd i=1 |{x | x \\u2208 D, x satisfies qi \\u2227 si, B(x ) , ci }| Unambiguity ruleoverlap(R) = M \\u00cd i=1 M \\u00cd j=1,i,j overlap(qi \\u2227 si, qj \\u2227 sj ) cover(R) = |{x | x \\u2208 D, x satisfies qi \\u2227 si where i \\u2208 {1 \\u00b7 \\u00b7 \\u00b7 M }}| Interpretability size(R): number of rules (triples of the form (q, s, c)) in R maxwidth(R) = max e\\u2208 M \\u00d0 i=1 (qi \\u222asi ) width(e) numpreds(R) = M \\u00cd i=1 width(si ) + width(qi ) numdsets(R) = |dset(R)| where dset(R) = M \\u00d0 i=1 qi f eatureoverlap(R) = \\u00cd q\\u2208dset(R) M \\u00cd i=1 f eatureoverlap(q, si ) algorithm such as apriori [1] can be used to generate the candidate sets of conjunctions of predicates.\",\"356\":\"Without any input from the user, both ND and DL are assigned to the same candidate set generated by Apriori.\",\"357\":\"On the other hand, if the user is interested in exploring the behavior of the black box model w.r.t some features U (eg., exercise and smoking) ND is initialized to conjunctions from the candidate set comprising only of the features in U.\",\"358\":\"In order to facilitate theoretical analysis, the metrics from Section 2.2.1 are expressed in the objective function either as non-negative reward functions or constraints.\",\"359\":\"To construct non-negative reward functions, penalty terms (metrics defined previously) are subtracted from their corresponding upper bound values (Pmax , Omax , O0 max , Fmax ) which are computed with respect to ND and DL.\",\"360\":\"f1(R) = Pmax \\u2212 numpreds(R), where Pmax = Pmax = 2 \\u2217 Wmax \\u2217 |ND | \\u2217 |DL | f2(R) = Omax \\u2212 f eatureoverlap(R), where Omax = Wmax \\u2217 |ND | \\u2217 |DL | f3(R) = O0 max \\u2212 ruleoverlap(R), where O0 max = N \\u00d7 (|ND | \\u2217 |DL |)2 f4(R) = cover(R) f5(R) = Fmax \\u2212 disa\\u0434reement(R), where Fmax = N \\u00d7 |ND | \\u2217 |DL | where Wmax is the maximum width of any rule in either candidate sets.\",\"361\":\"The resulting optimization problem is: arg max R\\u2286ND\\u00d7DL\\u00d7C 5 \\u00d5 i=1 \\u03bbi fi (R) (1) s.t. size(R) \\u2264 \\u03f51, maxwidth(R) \\u2264 \\u03f52, numdsets(R) \\u2264 \\u03f53 (2) \\u03bb1 \\u00b7 \\u00b7 \\u00b7 \\u03bb5 are non-negative weights which manage the relative influence of the terms in the objective.\",\"362\":\"These can be specified by an end user or can be set using cross validation.\",\"363\":\"The values of \\u03f51,\\u03f52,\\u03f53 are application dependent and need to be set by an end user.\",\"364\":\"Theorem 2.1.\",\"365\":\"The objective function in Eqn. 1 is non-normal, non-negative, non-monotone, submodular and the constraints of the optimization problem are matroids.\",\"366\":\"Proof (Sketch).\",\"367\":\"The objective function is non-negative: the first term in the functions f1, f2, f3, f5 is an upper bound on the value that can be taken by the second term ensuring non-negativity.\",\"368\":\"In the case of f4, the metric cover cannot be negative as it denotes the number of instances in the data that satisfy some rule in the approximation.\",\"369\":\"f1(\\u2205) = Pmax , 0.\",\"370\":\"Since one of the terms is non-normal and objective is a non-negative linear combination, 3 \\fAlgorithm 1 Optimization Procedure [5] 1: Input: Objective f , domain ND \\u00d7 DL \\u00d7 C, parameter \\u03b4, number of constraints k 2: V1 = ND \\u00d7 DL \\u00d7 C 3: for i \\u2208 {1, 2 \\u00b7 \\u00b7 \\u00b7 k + 1} do .\",\"371\":\"Approximation local search procedure 4: X = Vi ; n = |X |; Si = \\u2205 5: Let v be the element with the maximum value for f and set Si = v 6: while there exists a delete\\/update operation which increases the value of Si by a factor of at least (1 + \\u03b4 n4 ) do 7: Delete Operation: If e \\u2208 Si such that f (Si \\\\{e }) \\u2265 (1+ \\u03b4 n4 )f (Si ), then Si = Si \\\\e 8: 9: Exchange Operation If d \\u2208 X\\\\Si and ej \\u2208 Si (for 1 \\u2264 j \\u2264 k) such that 10: (Si \\\\ej ) \\u222a {d } (for 1 \\u2264 j \\u2264 k) satisfies all the k constraints and 11: f (Si \\\\{e1, e2 \\u00b7 \\u00b7 \\u00b7 ek } \\u222a {d }) \\u2265 (1 + \\u03b4 n4 )f (Si ), then Si = Si \\\\{e1, e2, \\u00b7 \\u00b7 \\u00b7 ek } \\u222a {d } 12: end while 13: Vi+1 = Vi \\\\Si 14: end for 15: return the solution corresponding to max{f (S1), f (S2), \\u00b7 \\u00b7 \\u00b7 f (Sk+1)} the objective function is non-normal.\",\"372\":\"In order to prove the objective is non-monotone, let us consider the function f1 and two approximations A and B such that A \\u2286 B i.e., B has at least as many rules as A. Therefore, by definition of numpreds metric, numpreds(B) \\u2265 numpreds(A) which implies that f1(A) \\u2265 f1(B).\",\"373\":\"Since f1 is non-monotone and so is the entire objective function.\",\"374\":\"Last, the functions f1 and f5 are modular and the other three functions in the objective turn out to be submodular.\",\"375\":\"The constraints of the optimization problem are matroids because they satisfy the following two properties: 1) empty set satisfies each of the constraints 2) If approximationsA, B such that |A| < |B| satisfies the constraints, then A \\u222a e where e \\u2208 B \\u2212 A also satisfies the constraints.\",\"376\":\"\\u0003 Corollary 2.2.\",\"377\":\"The optimization problem in Eqn. 1 is NP-Hard.\",\"378\":\"Proof (Sketch).\",\"379\":\"The objective function in Eqn. 1 is submodular and maximizing a submodular function is NP-Hard [2].\",\"380\":\"\\u0003 While exactly solving the optimization problem in Eqn. 1 is NP-Hard, the specific properties of the problem: non-monotonicity, submodularity, non-normality, non-negativity and the accompanying matroid constraints allow for applying algorithms with provable optimality guarantees.\",\"381\":\"We employ an optimization procedure based on approximate local search (see Algorithm 1) which provides the best known theoretical guarantees (\\u223c 1\\/5 approximation) for this class of problems.\",\"382\":\"3 EXPERIMENTAL EVALUATION We evaluate our framework on a Depression diagnosis [4] dataset collected by an online health records portal comprising of medical history, symptoms, and demographic information of about 33K individuals.\",\"383\":\"The class label of each individual is either depressed or healthy.\",\"384\":\"Baselines.\",\"385\":\"We benchmark the performance of our framework against the following baselines: 1) Locally interpretable model agnostic explanations (LIME) [8] 2) Interpretable Decision Sets (IDS) [4] 3) Bayesian Decision Lists (BDL) [6].\",\"386\":\"We employ IDS and BDL to approximate other black box models by training them with the labels of the black box models as the ground truth labels.\",\"387\":\"We also construct the following variants: 4) LIME-DS where each local linear model in the LIME approach is replaced with a decision set 5) BETA-LM where we group instances in the data based on neighborhood descriptors obtained using our approach and then fit a separate linear model for each of these neighborhoods.\",\"388\":\"Analyzing the Tradeoffs between Fidelity and Interpretability.\",\"389\":\"Fidelity and interpretability are competing objectives, where fidelity favors details and nuances while interpretability favors simplicity.\",\"390\":\"To understand how effectively different approaches trade-off fidelity with interpretability, we plot agreement rate vs. various metrics of interpretability (outlined in Section 2) for approximations generated by our framework and other baselines.\",\"391\":\"We compute agreement rate, fraction of instances in the data for which the label assigned by the approximation is the same as that of the black box model prediction, as a measure of fidelity.\",\"392\":\"Figures 2a and 2b show the plots of agreement rate vs. number of rules (size) and agreement rate vs. average number of predicates (ratio of numpreds to size) for the explanations constructed to approximate a 5 layer deep neural network using our model, LIME-DS, IDS, and BDL.\",\"393\":\"Our approximations consistently demonstrate higher agreement rates at lower values of the desired metrics.\",\"394\":\"For instance, at an average width of 10 predicates per rule, our approximation already reaches agreement rate of about 85% whereas other approaches require at least 20 predicates per rule to attain this agreement rate (Figure 2b).\",\"395\":\"We plot agreement rate vs. number of neighborhoods for the approximations generated by our approach and its linear variant, LIME and LIME-DS (see Figure 2c).\",\"396\":\"Our approximations achieve high fidelity (about 85% agreement rate) with as few as 5 neighborhoods whereas LIME requires choosing about 20 neighborhoods to achieve the same agreement rate.\",\"397\":\"(a) Number of Rules (b) Avg. Number of Predicates (c) Number of Neighborhoods Figure 2: Fidelity vs. Interpretability Trade Offs for Depression Diagnosis Data.\",\"398\":\"We also found that the approximations generated using IDS and our approach also result in low values of ruleoverlap (between 1 and 2%) and high values for cover (95 to 98%).\",\"399\":\"Decision list representation by design achieves the optimal values of zero for ruleoverlap and N for cover.\",\"400\":\"User Studies.\",\"401\":\"We designed an online user study with 33 participants, where each participant was randomly presented with the approximations (for a 5 layer deep neural network model) generated by: 1) our approach 2) IDS 3) BDL.\",\"402\":\"Participants were asked 5 questions, each of which was designed to test the user\\u2019s understanding of the model behavior in different parts of feature space.\",\"403\":\"An example question is: Consider a patient who is female and aged 65 years.\",\"404\":\"Based on the approximation shown above, can you be absolutely sure that this patient is Healthy?\",\"405\":\"If not, what other conditions need to hold for this patient to be labeled as Healthy?\",\"406\":\"These questions closely mimic decision making in real-world settings where decision makers would like to reason about model behavior in certain 4 \\fApproach Human Accuracy Avg.\",\"407\":\"Time (in secs.)\",\"408\":\"Our Approach - BETA 94.5% 160.1 (Non-Interactive) IDS 89.2% 231.1 BDL 83.7% 368.5 Our Approach - BETA 98.3% 78.3 (Interactive) Table 2: Results of User Study.\",\"409\":\"parts of the feature space.\",\"410\":\"We computed the accuracy of the answers provided by users.\",\"411\":\"We also recorded the time taken to answer each question and used this to computed the average time spent (in seconds) on each question.\",\"412\":\"Table 2 (top) show the results obtained using approximations from our model, IDS, and BDL.\",\"413\":\"It can be seen that user accuracy associated with our approach was higher than that of IDS, BDL.\",\"414\":\"Users were about 1.5 and 2.3 times faster when using our approximation compared to those constructed by IDS and BDL respectively.\",\"415\":\"We also measured the benefit obtained using interactivity, where the approximation presented to the user is customized w.r.t to the question the user is trying to answer.\",\"416\":\"For example, imagine the question above now asking about a patient who smokes and does not exercise.\",\"417\":\"Whenever a user is asked this question, we showed him\\/her an approximation where exercise and smoking appear in the neighborhood descriptors thus simulating the effect of the user trying to interactively explore the model w.r.t these features.\",\"418\":\"We recruited 11 participants for this study and we asked each of these participants the same 5 questions as those asked in task 1.\",\"419\":\"It can be seen that the time taken to answer questions is almost reduced in half compared to the setting where we showed users the same approximation each time.\",\"420\":\"Answers provided are also comparatively more accurate.\",\"421\":\"REFERENCES [1] R. Agrawal, R. Srikant, et al. Fast algorithms for mining association rules.\",\"422\":\"[2] S. Khuller, A. Moss, and J.\",\"423\":\"S. Naor.\",\"424\":\"The budgeted maximum coverage problem.\",\"425\":\"Information Processing Letters, 70(1):39\\u201345, 1999.\",\"426\":\"[3] P. W. Koh and P. Liang.\",\"427\":\"Understanding black-box predictions via influence functions.\",\"428\":\"arXiv preprint arXiv:1703.04730, 2017.\",\"429\":\"[4] H. Lakkaraju, S. H. Bach, and J. Leskovec.\",\"430\":\"Interpretable decision sets: A joint framework for description and prediction.\",\"431\":\"In KDD, 2016.\",\"432\":\"[5] J. Lee, V.\",\"433\":\"S. Mirrokni, V. Nagarajan, and M. Sviridenko.\",\"434\":\"Non-monotone submodular maximization under matroid and knapsack constraints.\",\"435\":\"In Proceedings of the fortyfirst annual ACM symposium on Theory of computing, pages 323\\u2013332.\",\"436\":\"ACM, 2009.\",\"437\":\"[6] B. Letham, C. Rudin, T. H. McCormick, D. Madigan, et al. Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model.\",\"438\":\"The Annals of Applied Statistics, 9(3):1350\\u20131371, 2015.\",\"439\":\"[7] Y. Lou, R. Caruana, and J. Gehrke.\",\"440\":\"Intelligible models for classification and regression.\",\"441\":\"In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 150\\u2013158.\",\"442\":\"ACM, 2012.\",\"443\":\"[8] M. T. Ribeiro, S. Singh, and C. Guestrin.\",\"444\":\"Why should i trust you?\",\"445\":\": Explaining the predictions of any classifier.\",\"446\":\"In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1135\\u20131144.\",\"447\":\"ACM, 2016.\",\"448\":\"[9] L. Rokach and O. Maimon.\",\"449\":\"Top-down induction of decision trees classifiers-a survey.\",\"450\":\"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 35(4):476\\u2013487, 2005.\",\"451\":\"5\",\"452\":\"Distill-and-Compare: Auditing Black-Box Models Using Transparent Model Distillation Sarah Tan Cornell University ht395@cornell.edu Rich Caruana Microsoft Research rcaruana@microsoft.com Giles Hooker Cornell University gjh27@cornell.edu Yin Lou Ant Financial yin.lou@antfin.com ABSTRACT Black-box risk scoring models permeate our lives, yet are typically proprietary or opaque.\",\"453\":\"We propose Distill-and-Compare, an approach to audit such models without probing the black-box model API or pre-defining features to audit.\",\"454\":\"To gain insight into black-box models, we treat them as teachers, training transparent student models to mimic the risk scores assigned by the black-box models.\",\"455\":\"We compare the mimic model trained with distillation to a second, un-distilled transparent model trained on ground-truth outcomes, and use differences between the two models to gain insight into the black-box model.\",\"456\":\"We demonstrate the approach on four data sets: COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club.\",\"457\":\"We also propose a statistical test to determine if a data set is missing key features used to train the black-box model.\",\"458\":\"Our test finds that the ProPublica data is likely missing key feature(s) used in COMPAS.\",\"459\":\"CCS CONCEPTS \\u2022 Computing methodologies \\u2192 Model verification and validation; \\u2022 Mathematics of computing \\u2192 Hypothesis testing and confidence interval computation; KEYWORDS Interpretability; Black-box models; Distillation; Fairness ACM Reference Format: Sarah Tan, Rich Caruana, Giles Hooker, and Yin Lou.\",\"460\":\"2018.\",\"461\":\"Distill-andCompare: Auditing Black-Box Models Using Transparent Model Distillation.\",\"462\":\"In 2018 AAAI\\/ACM Conference on AI, Ethics, and Society (AIES \\u201918), February 2\\u20133, 2018, New Orleans, LA, USA.\",\"463\":\"ACM, New York, NY, USA, 8 pages.\",\"464\":\"https: \\/\\/doi.org\\/10.1145\\/3278721.3278725 1 INTRODUCTION Risk scoring models have a long history of usage in criminal justice, finance, hiring, and other critical domains [13, 29].\",\"465\":\"They are designed to predict a future outcome, for example defaulting on a loan.\",\"466\":\"Worryingly, risk scoring models are increasingly used for high-stakes decisions, yet are typically proprietary or opaque.\",\"467\":\"Several approaches have been proposed [1, 2, 14, 18, 21, 36] to audit black-box risk scoring models: remove, permute, or obscure a protected feature, then see how the the model\\u2019s predictions change Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\",\"468\":\"Copyrights for components of this work owned by others than the author(s) must be honored.\",\"469\":\"Abstracting with credit is permitted.\",\"470\":\"To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and\\/or a fee.\",\"471\":\"Request permissions from permissions@acm.org.\",\"472\":\"AIES \\u201918, February 2\\u20133, 2018, New Orleans, LA, USA \\u00a9 2018 Copyright held by the owner\\/author(s).\",\"473\":\"Publication rights licensed to ACM.\",\"474\":\"ACM ISBN 978-1-4503-6012-8\\/18\\/02...$15.00 https:\\/\\/doi.org\\/10.1145\\/3278721.3278725 after retraining the model or probing the model API with the transformed data.\",\"475\":\"However, creators of proprietary risk scoring models often do not provide unrestricted access to model APIs, much less release the model form or training data.\",\"476\":\"Moreover, approaches that focus on one or two protected features defined in advance are less likely to detect biases that are not a priori known.\",\"477\":\"In this paper, we study a more realistic setting where we only have a data set labeled with the risk score (as produced by the risk scoring model), the ground-truth outcome, and some or all features; we are not able to probe the model API with new data.\",\"478\":\"We call this data set the audit data.\",\"479\":\"We add two potential complications: the audit data may not be the original training data, and the audit data may not have all features used to train the risk scoring model.\",\"480\":\"For example, ProPublica obtained data for their COMPAS study [5] not from the company that created COMPAS, but through a public records request to Broward County (BC), a US jurisdiction that used COMPAS in their criminal justice system [4].\",\"481\":\"ProPublica may not have had access to all the features BC used for COMPAS.\",\"482\":\"We propose Distill-and-Compare, an approach to audit black-box risk scoring models using audit data with both black-box risk scores and ground-truth outcomes, without pre-defining feature regions to audit.\",\"483\":\"First, we train a model on the audit data to mimic the black-box model.\",\"484\":\"Then we train another model to predict outcomes (Section 2.1).\",\"485\":\"To gain insight into the black-box model, we uncover feature regions where the two models are significantly different (Section 2.3), and ask \\u201cwhat could be happening in the black-box model, that could explain the differences we are seeing between the mimic and outcome models?\\u201d.\",\"486\":\"Finally, we use a statistical test (Section 2.2) to determine if the black-box model used additional features we do not have access to (i.e. features not in the audit data).\",\"487\":\"The contributions of this paper are: 1) We propose an approach to audit black-box risk scoring models under realistic conditions.\",\"488\":\"2) We show the importance of calibrating risk scores to remove audit data shift or scale post-processing that may been introduced by creators of risk scoring models.\",\"489\":\"3) We propose a statistical test to determine if the audit data is missing key features used to train the black-box model.\",\"490\":\"4) We apply the approach to audit four risk scoring models.\",\"491\":\"5) An ancillary contribution of this paper is a new confidence interval estimate for iGAM [10, 27, 28], a type of transparent model.\",\"492\":\"2 AUDIT APPROACH Our goal is to gain insight into a black-box risk scoring model.\",\"493\":\"We draw from model distillation and comparison technique to develop our approach.\",\"494\":\"Section 2.1.1 discusses related work.\",\"495\":\"2.1 Distill and Compare Model distillation was first introduced to transfer knowledge from a large, complex model (teacher) to a faster, simpler model (student) Paper AIES\\u201918, February 2\\u20133, 2018, New Orleans, LA, USA 303 \\f[7, 9, 22].\",\"496\":\"This was done by running unlabeled samples (either new unlabeled data or training data with labels discarded) through the teacher model to obtain the teacher\\u2019s outputs, then training the student model to mimic the teacher\\u2019s outputs.\",\"497\":\"We draw parallels to our setting, taking the risk scoring model to be the teacher and the audit data to be unlabeled samples ran through the teacher (risk scoring model) to obtain the teacher\\u2019s output (risk scores).\",\"498\":\"We train the mimic model to minimize mean squared error between the teacher and student, i.e., L(S, \\u02c6 S) = 1 T T \\u00d5 t=1 \\u0010 S(xt ) \\u2212 \\u02c6 S(xt ) \\u00112 , (1) where xt is the t-th sample in the audit data, S(xt ) is the output of the teacher model (risk scores) for sample xt , \\u02c6 S(xt ) is the output of the mimic model for sample xt , and T is the number of samples.\",\"499\":\"Throughout this paper, we will call the teacher model the black-box model and the student model the mimic model.\",\"500\":\"Next, we leverage the ground-truth outcome information.\",\"501\":\"We train our own risk scoring model on the audit data to predict the ground-truth outcome, i.e., L(O,O\\u0302) = 1 T T \\u00d5 t=1 n O(xt ) log \\u0010 P(O\\u0302(xt ) = 1) \\u0011 + (1 \\u2212 O(xt )) log \\u0010 P(O\\u0302(xt ) = 0) \\u0011o , (2) whereO(xt ) \\u2208 {0, 1} is the ground-truth outcome for samplext and O\\u0302(xt ) \\u2208 {0, 1} is the output of the model for sample xt .\",\"502\":\"Throughout this paper, we call this model the outcome model.\",\"503\":\"Note that the outcome model is not a mimic model.\",\"504\":\"Figure 1: Distill-and-Compare audit approach on a loan risk scoring model.\",\"505\":\"It is critical that both the mimic model and outcome model are trained using the same model class that allows for interpretation and comparison.\",\"506\":\"Not all model classes have the property that two models of that class can be compared.\",\"507\":\"For example, it is not obvious how to compare two decision trees, random forests or neural nets.\",\"508\":\"We want a model class that is as rich and complex as possible so that the mimic model can be faithful to the black-box model and the outcome model can accurately predict ground-truth outcomes.\",\"509\":\"However, this model class should still be transparent [17] so that we can examine its predictions across different feature regions.\",\"510\":\"In this paper, we use a particular transparent model class, iGAM (Section 2.3.1); other choices are possible.\",\"511\":\"The risk score and the ground-truth outcome are closely related\\u2014 the ground-truth outcome is what the black-box risk scoring model was meant to predict.\",\"512\":\"If the black-box model is accurate and generalizes to the audit data, it would predict the ground-truth outcomes in the audit data correctly; the converse is true if the black-box model is not accurate or does not generalize to the audit data.\",\"513\":\"Because both the mimic and outcome models are trained with the same model class on the same audit data using the same features, the more faithful the mimic model, and the more accurate the outcome model, the more likely it is that observed differences between the mimic and outcome models stem from differences between the black-box model and ground-truth outcomes.\",\"514\":\"This allows us to ask, \\u201cwhat could be happening in the black-box model, that could explain the differences we are seeing between the mimic and outcome models?\\u201d.\",\"515\":\"In addition, similarities between the mimic and outcome models (e.g., on COMPAS in Section 3.2, the Number of Priors feature is modeled very similarly by the two models) increases confidence that the mimic model is a faithful representation of the black-box model, and that any differences observed on other features are meaningful.\",\"516\":\"2.1.1 Related Work.\",\"517\":\"Several auditing approaches also use model distillation techniques to distill black-box models when they cannot be queried or to understand them [1, 2].\",\"518\":\"Other approaches also train their own outcome models, then uncover feature regions where the model is not accurate [3, 23, 24, 38].\",\"519\":\"Kim et al.\\u2019s iterative procedure [24] not only uncovers such regions but also modifies the model to improve accuracy in these regions.\",\"520\":\"However, they require repeated calls to the model; Agarwal et al. [3] and Kearns et al. [23] similarly require repeated calls or knowledge of the model.\",\"521\":\"Tramer et al. uncovered unexplained associations between black-box outputs and protected features on audit data [35].\",\"522\":\"Our approach is different from the above, as we avoid repeated calls to the black-box model API (that may not realistically be available), and instead utilize information on both risk scores and outcomes already available in some data sets in this domain (e.g. ProPublica COMPAS data).\",\"523\":\"Some other approaches also compare two models, but not risk scores and outcomes at the same time.\",\"524\":\"Wang et al. trained a model to predict outcomes and another to predict membership in a protected feature region [37].\",\"525\":\"Chouldechova and G\\u2019Sell trained two different outcome models then identified feature regions where the two models differed [12].\",\"526\":\"2.2 Testing for Missing Features If the audit data is missing features used by the black-box model, the audit data alone may be insufficient to audit the black-box model.\",\"527\":\"We propose a statistical test to check the likelihood of the audit data missing important features based on the following observation: If the black-box model used features that are missing from the audit data but are useful for predicting the ground-truth outcome, the error between the mimic model (learned on the audit data) and the risk score, || \\u02c6 S \\u2212 S||E, should be positively correlated with the error between the outcome model (learned on the audit data) and ground-truth outcome, ||O\\u0302 \\u2212 O||E.\",\"528\":\"where E is an error metric.\",\"529\":\"Since the test uses predictions from both the mimic and outcome models, the test is performed after both models are trained.\",\"530\":\"In Section 3.4, we perform the test on all risk scoring models we audit in this paper, to check if the audit results Paper AIES\\u201918, February 2\\u20133, 2018, New Orleans, LA, USA 304 \\fFeature Contribution, h i (x i ) Feature Contribution, h i (x i ) Figure 2: Eight features the Chicago Police says are used in their risk scoring model.\",\"531\":\"Best seen on screen.\",\"532\":\"Feature Contribution, h i (x i ) Feature Contribution, h i (x i ) Figure 3: Eight features the Chicago Police says are not used in their risk scoring model.\",\"533\":\"Best seen on screen.\",\"534\":\"are significantly affected by missing features.\",\"535\":\"Note that this test does not require the mimic and outcome models to be transparent.\",\"536\":\"2.3 Comparing Mimic and Outcome Models In this section, we provide technical details on how to train the mimic and outcome models so that they are comparable.\",\"537\":\"2.3.1 Choice of model class.\",\"538\":\"As noted in Section 2.1, we train the mimic model and outcome model using the same transparent model class\\u2014in this paper, iGAM [10, 27, 28].\",\"539\":\"We point the reader to [10, 27, 28] to learn more about iGAMs and to [34] for a distillation example where it was used as a student.\",\"540\":\"Briefly, iGAM has the form E[\\u0434(y)] = h0 + \\u00d5 i hi (xi ) + \\u00d5 i,j hij (xi,xj ), (3) where \\u0434 is the logistic function for classification and identity function for regression, h0 is the intercept, and the contribution of any one feature xi or pair of features xi and xj to the prediction can be visualized in graphs such as Figure 2 (with hi (xi ) on the y-axis) and Figure 5 (with regions colored by hij (xi,xj )).\",\"541\":\"For classical GAMs [20], h(\\u00b7) are fitted using splines; for iGAM, they are fitted using ensembles of shallow trees and centered for identifiability.\",\"542\":\"Crucially, since iGAM is an additive model, two iGAM models can be compared by simply taking a difference of their feature contributions h(\\u00b7), which we exploit in Section 2.3.3 to detect differences between the mimic and outcome models.\",\"543\":\"2.3.2 Calibrating model inputs.\",\"544\":\"Calibration is the process of matching predicted and empirical probabilities [15, 31].\",\"545\":\"If a risk score is well-calibrated, the relationship between the risk score and empirical probabilities will be linear (e.g., COMPAS and Stop-and-Frisk in the top row of Figure 6 in the Appendix).\",\"546\":\"While developing the method, we discovered that not all risk scores exhibit the desired linear relationship with outcomes in the audit data.\",\"547\":\"For example, the Chicago Police risk score (third column of Figure 6 in the Appendix) is rather flat for risk scores less than 350, then exhibits a sharp kink upwards.\",\"548\":\"One possible explanation for any nonlinear relationship is that the risk score was well-calibrated on its original training data, but the audit data has a different distribution (data shift) [32].\",\"549\":\"Another possible explanation is post-processing by model creators to reduce sensitivity in less important parts of the risk score scale and enhance separation in more important parts of the scale [26].\",\"550\":\"We make the reasonable assumption that risk scores should be monotonic and well-calibrated [26] and use this assumption to undo scale post-processing or audit data shift before training the mimic and outcome models.\",\"551\":\"Specifically, we learn a nonlinear transformation of the risk score (the blue line in Figure 6 in the Appendix), similar to isotonic regression [31], to make the risk scores and outcomes linearly related on a scale of choice.\",\"552\":\"The mimic model is then trained with the transformed risk scores as labels; the outcome model is trained with outcomes, unchanged.\",\"553\":\"This pre-training calibration step is necessary to compare the mimic and outcome models, as it makes their labels linearly related on a scale that their predicted labels will later be compared on.\",\"554\":\"We select this scale to be logit probability (since the predicted outcomes are already on this scale), and perform this calibration step for Chicago Police and Lending Club but not COMPAS and Stop-and-Frisk, since the latter two already exhibit the desired linear relationships.\",\"555\":\"See Appendix B for details.\",\"556\":\"2.3.3 Detecting differences.\",\"557\":\"To not mistake random noise for real differences between the mimic and outcome models, we control potential sources of noise during the training process.\",\"558\":\"To avoid data sample-specific effects, we train the mimic and outcome models on the same data sample.\",\"559\":\"Let shi (xi ) be feature xi \\u2019s contribution to the mimic model, and similarly ohi (xi ) for the outcome model.\",\"560\":\"We calculate the difference in feature xi \\u2019s contribution to the two models, shi (xi ) \\u2212ohi (xi ), and construct a confidence interval for this difference to tell if it is statistically significant.\",\"561\":\"One ancillary contribution of this paper is a new method to estimate confidence intervals for the iGAM model class, by employing a bootstrap-of-little-bags approach [33] to estimate the variance of hi (xi ) and shi (xi ) \\u2212 ohi (xi ).\",\"562\":\"See Appendix A for details.\",\"563\":\"The resulting confidence intervals are the dotted lines in Figures 2\\u20134.\",\"564\":\"3 RESULTS 3.1 Validating the Audit Approach In this section, we demonstrate Distill-and-Compare on risk scoring models where we have some information on how they were trained, and check that the approach can recover this information.\",\"565\":\"3.1.1 Stop-and-Frisk.\",\"566\":\"Using the New York Police Department\\u2019s Stop-and-Frisk1 data, Goel et al. [19] proposed a simple risk scoring 1http:\\/\\/www1.nyc.gov\\/site\\/nypd\\/stats\\/reports-analysis\\/stopfrisk.page Paper AIES\\u201918, February 2\\u20133, 2018, New Orleans, LA, USA 305 \\fFeature Contribution, h i (x i ) Age Race Number of Prior Counts Sex Figure 4: Feature contributions of four features to the COMPAS mimic model (in red) and outcome model (in green).\",\"567\":\"model for weapon possession: S = 3 \\u00d7 1PS + 1 \\u00d7 1AS + 1 \\u00d7 1Bul\\u0434e , where S is the risk score, PS denotes primary stop circumstance being presence of suspicious object, AS denotes secondary stop circumstance being sight of criminal activity, and Bul\\u0434e denotes bulge in clothing [19].\",\"568\":\"Since we know the risk scoring model\\u2019s functional form, we can verify that the mimic model correctly recovers these coefficients.\",\"569\":\"We apply the risk scoring model to label 2012 data (T=126,457, p=40) after following Goel et al.\\u2019s data pre-processing steps [19].\",\"570\":\"Result.\",\"571\":\"The mimic model recovers the coefficients (3, 1, 1) for the three features used in the risk scoring model (PS, AS, Bul\\u0434e) and 0 for the remaining features.\",\"572\":\"3.1.2 Chicago Police \\u201cStrategic Subject\\u201d List.\",\"573\":\"The Chicago Police Department released arrest data2 from 2012 to 2016 that was used to create a risk score for an individual being involved in a shooting incident as a victim or offender.\",\"574\":\"This data set contains 16 features, but only 8 are used by the black-box model, which gives us an opportunity to test if Distill-and-Compare can accurately detect which features are and are not used by a black-box model.\",\"575\":\"We trained a mimic model, intentionally including all 16 features.\",\"576\":\"Figure 2 shows the feature contributions of the mimic model (in red) and outcome model (in green) for the 8 features the Chicago Police says were used by the black-box model; Figure 3 shows the 8 features the Chicago Police says were not used in their model.\",\"577\":\"Result.\",\"578\":\"There is a striking difference between Figures 2 and 3: the mimic model (in red) assigns importance to the features in Figure 2, but does not assign any importance to the features in Figure 3.\",\"579\":\"This agrees with Chicago Police\\u2019s statement about which features were and were not used in the black-box model.\",\"580\":\"We also note that the outcome model (in green) does assign importance to the unused features (Figure 3), suggesting that there is signal available in the 8 unused features that the Chicago Police risk scoring model could have used, but chose not to use.\",\"581\":\"Race and sex are 2 of these 8 features, which the Chicago Police especially emphasized are not used.\",\"582\":\"These experiments show that mimic models can provide insights into black-box models, and demonstrate the advantages of using outcome information.\",\"583\":\"3.2 Auditing COMPAS COMPAS, a proprietary score developed to predict recidivism risk, has been the subject of scrutiny for racial bias [5, 8, 11, 13, 16, 25].\",\"584\":\"2https:\\/\\/data.cityofchicago.org\\/Public-Safety\\/Strategic-Subject-List\\/4aki-r3np We do not know what model class, input features or data were used to train COMPAS.\",\"585\":\"As described in Section 1, the COMPAS audit data3 was collected by ProPublica; it is likely different from the original COMPAS training data.\",\"586\":\"Figure 4 compares the COMPAS mimic model (in red) and outcome model (in green) for four features: Age, Race, Number of Priors, and Gender.\",\"587\":\"The dotted lines are 95% pointwise confidence intervals.\",\"588\":\"We observe the following: COMPAS agrees with ground-truth outcomes regarding the number of priors.\",\"589\":\"In the 3rd plot in Figure 4, the mimic model and outcome model agree on the impact of Number of Priors on risk; their confidence intervals overlap through most of its range.\",\"590\":\"COMPAS disagrees with ground-truth outcomes for some age and race groups.\",\"591\":\"The 1st and 2nd plots in Figure 4 show the effect of Age and Race on the mimic and outcome models.\",\"592\":\"The mimic model (red) and the outcome model (green) are very similar between ages 20 to 70, and their confidence intervals overlap.\",\"593\":\"For ages greater than 70, there is evidence that the models disagree as the confidence intervals do not overlap.\",\"594\":\"The mimic and outcome models are also different for ages 18 and 19: the mimic model predicts low risk for young individuals, but we see no evidence to support this in the outcome model, with risk appearing to be highest for young individuals.\",\"595\":\"The mimic model predicts that African Americans are even higher risk, and Caucasians lower risk, than the ground-truth outcomes suggest is warranted.\",\"596\":\"Note that the ground-truth outcomes might themselves be biased due to historical discrimination against African Americans.\",\"597\":\"Gender has opposite effects on COMPAS compared to groundtruth outcome.\",\"598\":\"In the 4th plot in Figure 4, we see a discrepancy between the mimic model and outcome model on Gender.\",\"599\":\"The mimic model predicts higher risk than warranted by ground-truth outcomes for females, and conversely for males.\",\"600\":\"Using differences to gain insight into COMPAS.\",\"601\":\"We now ask \\u201cwhat could be happening in COMPAS, that could explain the differences we are seeing between the mimic and outcome models?\\u201d: (1) Some feature regions may be underrepresented in the blackbox model\\u2019s training data and\\/or the audit data.\",\"602\":\"In this audit data, only 3% of samples are between 18 and 20 years old, only 0.5% are older than 70 years old, and only 19% are female, which makes learning accurate models in these regions harder.\",\"603\":\"3https:\\/\\/github.com\\/propublica\\/COMPAS-analysis Paper AIES\\u201918, February 2\\u20133, 2018, New Orleans, LA, USA 306 \\fHome Ownership Status Home Ownership Status Year Loan Issued Year Loan Issued Figure 5: Interaction between loan issue year and home ownership in Lending Club mimic model (in red) and outcome model (in green).\",\"604\":\"Regions colored by hij (xi,xj ).\",\"605\":\"(2) The black-box model may be deliberately simple for some feature regions.\",\"606\":\"For ages greater than 70, the outcome model has much wider confidence intervals than the mimic model.\",\"607\":\"The ground-truth outcomes are potentially high-variance in this region, yet the black-box model\\u2019s scoring function may have been kept deliberately simple for extreme feature values like this.\",\"608\":\"(3) The black-box model may have a very different form than the transparent model class.\",\"609\":\"The mimic model predicts low risk for young individuals, but there is no evidence to support this in the outcome model.\",\"610\":\"We trained an iGAM model with interactions between pairs of features, and observed strong interactions between very young age and other variables such as Gender, Charge Degree, and Length of Stay.\",\"611\":\"If COMPAS has a more simple form and does not model interactions well, this may explain why COMPAS needs to predict low risk for very young individuals (because it cannot otherwise predict a reduced risk via interactions of age with other variables).\",\"612\":\"(4) The black-box model may have used features missing from the audit data, that interact with the non-missing features.\",\"613\":\"We investigate this in Section 3.4.\",\"614\":\"While we cannot tell (without further investigation) the definitive reason that explains a particular difference between the mimic and outcome models, this has surfaced ideas about the black-box model and uncovered potentially problematic feature regions that we did not a priori know, but can now proceed to investigate further.\",\"615\":\"3.3 Auditing Lending Club Lending Club, an online peer-to-peer lending company, rates loans it finances on an A1-G5 scale.\",\"616\":\"We use a subset of five years (20072011) of loans4 that have matured, so that we have ground-truth on whether the loan defaulted.\",\"617\":\"We do not know what model class or input features Lending Club used to train their risk scoring model.\",\"618\":\"We believe the data sample we have is similar to the data they would have used to train their models.\",\"619\":\"According to Lending Club, their models are refreshed periodically.\",\"620\":\"We use this Lending Club example to discuss an insight gained into the black-box model from inspecting feature interactions in the transparent models.\",\"621\":\"Figure 5 shows the interaction of loan issue year 4https:\\/\\/www.lendingclub.com\\/info\\/download-data.action Table 1: Statistical test for likelihood of audit data missing key features used by black-box model.\",\"622\":\"Risk Score Pearson \\u03c1 Spearman \\u03c1 Kendall \\u03c4 COMPAS [0.10, 0.13] [0.10, 0.14] [0.08, 0.10] Lending Club [0.00, 0.03] [-0.01, 0.01] [-0.01, 0.01] Stop-and-Frisk [0.00, 0.01] [-0.03, 0.01] [-0.02, 0.01] Chicago Police [0.00, 0.01] [0.01, 0.03] [0.01, 0.02] and home ownership in the Lending Club mimic model (in red) and ground-truth outcome model (in green).\",\"623\":\"Having a home mortgage in 2007-2008 increases the loan default risk more than having a home mortgage in 2009 and beyond.\",\"624\":\"Recall that 2007-2008 is around the time of the subprime housing crisis.\",\"625\":\"Note the difference in ranges between the two plots\\u2014the range goes up to 0.2 for the outcome model (in green) whereas the range is much lower for the mimic model (in red).\",\"626\":\"One possible explanation for this difference is that the Lending Club risk scoring model is updated conservatively (with some lag time), instead of being rapidly updated as economic conditions and behavior change.\",\"627\":\"3.4 Which Audit Data Are Missing Features?\",\"628\":\"As black-box models may use additional features we do not have access to, we developed a test (Section 2.2) to assess the impact missing features could have on the audit.\",\"629\":\"Table 1 provides 95% confidence intervals for three correlation measures (linear and nonlinear) used in the test.\",\"630\":\"If zero is in the confidence interval, the error of the mimic model (trained on the audit data) is not correlated with the error of the outcome model (also trained on the audit data).\",\"631\":\"Then, it is unlikely that the audit data is missing key feature(s) that are a) predictive of outcomes (and hence will negatively affect the error of the outcome model if missing); and b) used in the black-box model (and hence will negatively affect the error of the mimic model if missing).\",\"632\":\"In Lending Club and Stop-and-Frisk we cannot distinguish these correlations from zero, suggesting that no key features are missing from the audit data.\",\"633\":\"For Chicago Police, the confidence intervals contain 0 or are very close to 0 (lower limit 0.01), hence there is little evidence of missing key features.\",\"634\":\"For COMPAS, there is evidence of positive correlation, indicating that the ProPublica data may be missing key features used in the COMPAS model.\",\"635\":\"This is supported by the findings in Section 3.5 that no mimic models trained on the ProPublica data, however powerful (e.g., random forests), could mimic COMPAS well.\",\"636\":\"3.5 Fidelity and Accuracy To quantitatively evaluate the audit approach, we report fidelity (how well the mimic model predicts the black-box model\\u2019s risk scores, measured in RMSE) and accuracy (how well the outcome model predicts the ground-truth outcomes, measured in AUC) for all the risk scoring models we audit in Table 2.\",\"637\":\"For comparison, we also train linear models (a simpler model class than iGAM) and random forests (more complex, but less interpretable).\",\"638\":\"For COMPAS, all model classes (linear model, iGAM, random forest) have roughly equal fidelity and accuracy.\",\"639\":\"Interestingly, none obtained RMSE lower than 2 on a 1-10 scale.\",\"640\":\"Comparing outcome Paper AIES\\u201918, February 2\\u20133, 2018, New Orleans, LA, USA 307 \\fTable 2: Fidelity of mimic model and accuracy of outcome model.\",\"641\":\"Lower RMSE is better, higher AUC is better.\",\"642\":\"Risk Score Metric Linear model iGAM iGAM w\\/ interactions Random Forest Fidelity of mimic model COMPAS RMSE (1-10) 2.11 \\u00b1 0.057 2.01 \\u00b1 0.045 2.00 \\u00b1 0.047 2.02 \\u00b1 0.053 Lending Club RMSE (2-36) 3.27 \\u00b1 0.037 2.60 \\u00b1 0.049 2.52 \\u00b1 0.051 2.48 \\u00b1 0.033 Chicago Police RMSE (0-500) 17.4 \\u00b1 0.102 17.2 \\u00b1 0.125 16.5 \\u00b1 0.130 14.0 \\u00b1 0.280 Stop-and-Frisk RMSE (0-5) 0.00 \\u00b1 2 \\u00d7 10\\u221215 0.00 \\u00b1 1 \\u00d7 10\\u22125 0.00 \\u00b1 2 \\u00d7 10\\u22125 0.01 \\u00b1 2 \\u00d7 10\\u22123 Accuracy of outcome model COMPAS AUC 0.73 \\u00b1 0.029 0.74 \\u00b1 0.027 0.75 \\u00b1 0.029 0.73 \\u00b1 0.026 Lending Club AUC 0.69 \\u00b1 0.006 0.69 \\u00b1 0.016 0.69 \\u00b1 0.014 0.68 \\u00b1 0.020 Chicago Police AUC 0.95 \\u00b1 0.007 0.95 \\u00b1 0.007 0.95 \\u00b1 0.007 0.93 \\u00b1 0.009 Stop-and-Frisk AUC 0.84 \\u00b1 0.020 0.85 \\u00b1 0.020 0.85 \\u00b1 0.020 0.87 \\u00b1 0.024 model AUCs across different model classes, iGAM\\u2019s results are generally comparable to (or slightly better than) more complex random forests (Table 2).\",\"643\":\"For the risk score mimic models, random forests are competitive for Lending Club and Chicago Police.\",\"644\":\"Linear mimic models are not far behind iGAMs for several risk scoring models (COMPAS, Chicago Police, Stop-and-Frisk), suggesting that the black-box model\\u2019s functional form might be very simple.\",\"645\":\"We know this to be true for Stop-and-Frisk from Section 3.1.1 where the model was a simple linear model.\",\"646\":\"3.6 Using Additional Data for Distillation One possible reason why COMPAS is challenging to mimic may be that the ProPublica data is missing key features.\",\"647\":\"This agrees with the results of the statistical test in Section 3.4.\",\"648\":\"Another possible reason is the small sample size (less than 7,000 samples).\",\"649\":\"One advantage of using a model distillation approach to inspect black-box models is that the approach may be able to benefit from additional unlabeled data if the black-box model can be queried to label the additional data [9].\",\"650\":\"We found an additional 3,000 individuals in the ProPublica data with COMPAS risk scores but no ground-truth outcomes.\",\"651\":\"Adding them to the training (not testing) data for the mimic model and retraining the mimic model, we find marginal improvement in the mimic model\\u2019s fidelity (from RMSE 2.0 to 1.98).\",\"652\":\"Doing the opposite\\u2014removing individuals from the training data in 1,000 increments\\u2014decreased the mimic model\\u2019s fidelity only marginally (to RMSE 2.1, training on only 1,000 individuals).\",\"653\":\"These analyses suggest that for COMPAS, missing key features is a more pressing issue than insufficient data.\",\"654\":\"4 DISCUSSION Sometimes we are interested in detecting bias on features intentionally excluded from the black-box model.\",\"655\":\"For example, a credit risk scoring model is probably not allowed to use race as an input.\",\"656\":\"Unfortunately, not using race does not prevent the model from learning to be biased.\",\"657\":\"Racial bias in a data set is likely to be in the outcomes \\u2014 the labels used for learning; not using race as an input feature does not remove the bias from the labels.\",\"658\":\"If race were uncorrelated with all other features (and combinations of features) provided to the model, then removing race would prevent the model from learning to be racially biased because it would not have any input features on which to model this bias.\",\"659\":\"Unfortunately, in any real-world, high-dimensional data set, there is massive correlation among the features, and a model trained to predict credit risk will learn to be biased from correlation of the excluded race feature with other features that likely remain in the model (e.g., income or education).\",\"660\":\"Hence, removing a protected feature like race or gender does not prevent a model from learning to be biased.\",\"661\":\"Instead, removing protected features make it harder to detect how the model is biased, or correct the bias, because the bias is now spread in a complex way among all the correlated features throughout the model instead of being localized to the protected features.\",\"662\":\"The main benefit of excluding protected features like race or gender from the inputs of a machine learning model is that it allows the group deploying the model to claim (incorrectly) that their model is not biased because it did not use these features.\",\"663\":\"When training a transparent model to mimic a black-box model, we intentionally include all features\\u2014even protected features like race and gender\\u2014specifically because we are interested in seeing what the mimic model could learn from them.\",\"664\":\"If, when the mimic model mimics the black-box model, it does not see any signal on the race or gender features and learns to model them as flat (zero) functions, this suggests whether the black-box model did or did not use these features, but also if the black-box model exhibits race or gender bias even if race or gender were not used as inputs.\",\"665\":\"5 CONCLUSION The Distill-and-Compare approach to auditing black-box models was motivated by a realistic setting where access to the black-box model API is not available.\",\"666\":\"Instead, only a data set labeled with the risk score (as produced by the risk scoring model) and the groundtruth outcome is available.\",\"667\":\"The efficacy of Distill-and-Compare increases when a model class that can be highly faithful to the black-box model and highly accurate at predicting the groundtruth outcomes is used, and when the audit data is not missing key features used in the black-box model.\",\"668\":\"A key advantage of using transparent models to audit black-box models is that we do not need to know in advance what to look for.\",\"669\":\"Many current auditing approaches focus on one or two protected features defined in advance, and thus are less likely to detect biases that are not a priori known.\",\"670\":\"The Distill-and-Compare audit approach using transparent models can hence be most useful for real-world, high-dimensional data with multiple, unknown sources of bias.\",\"671\":\"Paper AIES\\u201918, February 2\\u20133, 2018, New Orleans, LA, USA 308 \\fREFERENCES [1] Julius Adebayo and Lalana Kagal.\",\"672\":\"2016.\",\"673\":\"Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box Models.\",\"674\":\"In FATML Workshop.\",\"675\":\"[2] Philip Adler, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos Eduardo Scheidegger, Brandon Smith, and Suresh Venkatasubramanian.\",\"676\":\"2016.\",\"677\":\"Auditing Black-Box Models for Indirect Influence.\",\"678\":\"In ICDM.\",\"679\":\"[3] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach.\",\"680\":\"2018.\",\"681\":\"A reductions approach to fair classification.\",\"682\":\"In ICML. [4] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner.\",\"683\":\"2016.\",\"684\":\"How we analyzed the compas recidivism algorithm.\",\"685\":\"https:\\/\\/www.propublica.org\\/article\\/ how-we-analyzed-the-compas-recidivism-algorithm Accessed May 26, 2017.\",\"686\":\"[5] Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner.\",\"687\":\"2016.\",\"688\":\"Machine Bias: There\\u2019s software used across the country to predict future criminals.\",\"689\":\"And it\\u2019s biased against blacks.\",\"690\":\"https:\\/\\/www.propublica.org\\/article\\/machine-biasrisk-assessments-in-criminal-sentencing Accessed May 26, 2017.\",\"691\":\"[6] Susan Athey, Julie Tibshirani, and Stefan Wager.\",\"692\":\"2017. Generalized Random Forests.\",\"693\":\"arXiv preprint arXiv:1610.01271 (2017).\",\"694\":\"[7] Jimmy Ba and Rich Caruana.\",\"695\":\"2014.\",\"696\":\"Do Deep Nets Really Need to be Deep?.\",\"697\":\"In NIPS.\",\"698\":\"[8] Thomas Blomberg, William Bales, Karen Mann, Ryan Meldrum, and Joe Nedelec.\",\"699\":\"2010.\",\"700\":\"Validation of the COMPAS risk assessment classification instrument.\",\"701\":\"Technical Report.\",\"702\":\"Florida State University.\",\"703\":\"[9] Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil.\",\"704\":\"2006.\",\"705\":\"Model compression.\",\"706\":\"In KDD. [10] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad.\",\"707\":\"2015. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.\",\"708\":\"In KDD. [11] Alexandra Chouldechova.\",\"709\":\"2017.\",\"710\":\"Fair prediction with disparate impact: A study of bias in recidivism prediction instruments.\",\"711\":\"Big Data (2017).\",\"712\":\"[12] Alexandra Chouldechova and Max G\\u2019Sell.\",\"713\":\"2017.\",\"714\":\"Fairer and more accurate, but for whom?.\",\"715\":\"In FATML Workshop.\",\"716\":\"[13] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq.\",\"717\":\"2017. Algorithmic decision making and the cost of fairness.\",\"718\":\"In KDD. [14] A. Datta, S. Sen, and Y. Zick.\",\"719\":\"2016. Algorithmic Transparency via Quantitative Input Influence: Theory and Experiments with Learning Systems.\",\"720\":\"In IEEE Symposium on Security and Privacy.\",\"721\":\"[15] Morris H. DeGroot and Stephen E. Fienberg.\",\"722\":\"1983.\",\"723\":\"The Comparison and Evaluation of Forecasters.\",\"724\":\"Journal of the Royal Statistical Society.\",\"725\":\"Series D (1983).\",\"726\":\"[16] William Dieterich, Christina Mendoza, and Tim Brennan.\",\"727\":\"2016.\",\"728\":\"COMPAS risk scales: Demonstrating accuracy equity and predictive parity.\",\"729\":\"Technical Report.\",\"730\":\"Northpointe Inc.\",\"731\":\"[17] Finale Doshi-Velez and Been Kim.\",\"732\":\"2017.\",\"733\":\"Towards A Rigorous Science of Interpretable Machine Learning.\",\"734\":\"arXiv preprint arXiv:1702.08608 (2017).\",\"735\":\"[18] Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian.\",\"736\":\"2015.\",\"737\":\"Certifying and Removing Disparate Impact.\",\"738\":\"In KDD. [19] Sharad Goel, Justin M. Rao, and Ravi Shroff.\",\"739\":\"2016.\",\"740\":\"Precinct or prejudice?\",\"741\":\"Understanding racial disparities in New York City\\u2019s stop-and-frisk policy.\",\"742\":\"(2016).\",\"743\":\"[20] Trevor J Hastie and Robert J Tibshirani.\",\"744\":\"1990. Generalized additive models.\",\"745\":\"CRC press.\",\"746\":\"[21] Andreas Henelius, Kai Puolam\\u00e4ki, Henrik Bostr\\u00f6m, Lars Asker, and Panagiotis Papapetrou.\",\"747\":\"2014.\",\"748\":\"A peek into the black box: exploring classifiers by randomization.\",\"749\":\"Data Mining and Knowledge Discovery (2014).\",\"750\":\"[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\",\"751\":\"2014.\",\"752\":\"Distilling the Knowledge in a Neural Network.\",\"753\":\"In NIPS Deep Learning and Representation Learning Workshop.\",\"754\":\"[23] Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. 2018.\",\"755\":\"Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness.\",\"756\":\"In ICML. [24] Michael P Kim, Amirata Ghorbani, and James Zou.\",\"757\":\"2018.\",\"758\":\"Multiaccuracy: BlackBox Post-Processing for Fairness in Classification.\",\"759\":\"arXiv preprint arXiv:1805.12317 (2018).\",\"760\":\"[25] Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan.\",\"761\":\"2017.\",\"762\":\"Inherent trade-offs in the fair determination of risk scores.\",\"763\":\"In Innovations in Theoretical Computer Science.\",\"764\":\"[26] Manuel Lingo and Gerhard Winkler.\",\"765\":\"2008.\",\"766\":\"Discriminatory power-an obsolete validation criterion?\",\"767\":\"Journal of Risk Model Validation (2008).\",\"768\":\"[27] Yin Lou, Rich Caruana, and Johannes Gehrke.\",\"769\":\"2012. Intelligible models for classification and regression.\",\"770\":\"In KDD. [28] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker.\",\"771\":\"2013.\",\"772\":\"Accurate intelligible models with pairwise interactions.\",\"773\":\"In KDD. [29] Francisco Louzada, Anderson Ara, and Guilherme B Fernandes.\",\"774\":\"2016.\",\"775\":\"Classification methods applied to credit scoring: Systematic review and overall comparison.\",\"776\":\"Surveys in Operations Research and Management Science (2016).\",\"777\":\"[30] Lucas Mentch and Giles Hooker.\",\"778\":\"2016.\",\"779\":\"Quantifying uncertainty in random forests via confidence intervals and hypothesis tests.\",\"780\":\"The Journal of Machine Learning Research (2016).\",\"781\":\"[31] Alexandru Niculescu-Mizil and Rich Caruana.\",\"782\":\"2005.\",\"783\":\"Predicting Good Probabilities with Supervised Learning.\",\"784\":\"In ICML. [32] Richard D Riley, Joie Ensor, Kym IE Snell, Thomas PA Debray, Doug G Altman, Karel GM Moons, and Gary S Collins.\",\"785\":\"2016.\",\"786\":\"External validation of clinical prediction models using big datasets from e-health records or IPD meta-analysis: opportunities and challenges.\",\"787\":\"The BMJ (2016).\",\"788\":\"[33] Joseph Sexton and Petter Laake.\",\"789\":\"2009.\",\"790\":\"Standard Errors for Bagged and Random Forest Estimators.\",\"791\":\"Computational Statistics and Data Analysis (2009).\",\"792\":\"[34] Sarah Tan, Rich Caruana, Giles Hooker, and Albert Gordo.\",\"793\":\"2018.\",\"794\":\"Transparent Model Distillation.\",\"795\":\"arXiv preprint arXiv:1801.08640 (2018).\",\"796\":\"[35] Florian Tramer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Jean-Pierre Hubaux, Mathias Humbert, Ari Juels, and Huang Lin.\",\"797\":\"2017.\",\"798\":\"FairTest: Discovering Unwarranted Associations in Data-Driven Applications.\",\"799\":\"In IEEE European Symposium on Security and Privacy.\",\"800\":\"[36] Sandra Wachter, Brent Mittelstadt, and Chris Russell.\",\"801\":\"2018.\",\"802\":\"Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR.\",\"803\":\"Harvard Journal of Law & Technology (2018).\",\"804\":\"[37] Hao Wang, Berk Ustun, and Flavio P. Calmon.\",\"805\":\"2018.\",\"806\":\"On the Direction of Discrimination: An Information-Theoretic Analysis of Disparate Impact in Machine Learning.\",\"807\":\"In International Symposium on Information Theory.\",\"808\":\"[38] Zhe Zhang and Daniel B. Neill.\",\"809\":\"2017.\",\"810\":\"Identifying Significant Predictive Bias in Classifiers.\",\"811\":\"In FATML Workshop.\",\"812\":\"A A NEW CONFIDENCE INTERVAL ESTIMATE FOR IGAM It is not trivial to estimate confidence intervals for nonparametric learners such as trees [30]; iGAM\\u2019s base learners are shallow trees.\",\"813\":\"We employ a bootstrap-of-little-bags approach originally developed for bagged models in [33] to estimate the variance of feature xi \\u2019s contribution to the model, hi (xi ), and difference in feature xi \\u2019s contribution to the mimic and outcome models, shi (xi ) \\u2212 ohi (xi ).\",\"814\":\"Bootstrap-of-little-bags exploits two-level structured cross-validation (e.g. 15% of data points are selected for the test set, with the remaining 85% split into training (70%) and validation (15%) sets).\",\"815\":\"Repeating this inner splitting L times and outer splitting K times gives a total of KL bags on which we train the model.\",\"816\":\"Let hlk i (xi ) be feature xi \\u2019s contribution to the model in the lth inner and kth outer fold.\",\"817\":\"The variance of hi (xi ) can then be estimated as c Var(hi (xi )) = 1 K K \\u00d5 k=1 1 L L \\u00d5 l=1 hkl i (xi ) \\u2212 1 KL l \\u00d5 l=1 K \\u00d5 k=1 hkl i (xi ) !2 , and its mean hi (xi ) can be estimated by averaging hlk i (xi ) over KL bags.\",\"818\":\"We can now construct pointwise confidence intervals (CI) for feature contributions to iGAM models.\",\"819\":\"The 95% CI for feature xi \\u2019s contribution to the model, hi (xi ), is hi (xi ) \\u00b1 1.96 q c Var(hi (xi )) and the 95% CI for the difference in feature xi \\u2019s contribution to the mimic and outcome models, shi (xi ) \\u2212ohi (xi ), is shi (xi ) \\u2212ohi (xi ) \\u00b1 1.96 q c Var(shi (xi )) + c Var(ohi (xi )) \\u2212 2d Cov(shi (xi ),ohi (xi )), with d Cov(shi (xi ),ohi (xi )) also estimated using bootstrap-of-little-bags.\",\"820\":\"This variance estimate is conservative (meaning it overestimates true variability), however, given that we are trying to detect differences between the mimic and outcome models, overestimating means we are less likely to mistake random noise for real differences.\",\"821\":\"For large K and L, consistency of this estimate was established in [6].\",\"822\":\"Note that are pointwise, not uniform, CIs.\",\"823\":\"That is, using the feature Age as an example, these CIs capture the variability of the effect of Age at Age=50, not the entire effect of Age.\",\"824\":\"Uniform CIs can be constructed by adjusting the critical value of the CI.\",\"825\":\"Paper AIES\\u201918, February 2\\u20133, 2018, New Orleans, LA, USA 309 \\fB CALIBRATION PLOTS Fraction of Positive Outcomes, p logit( p ) % Data Risk Score Risk Score Risk Score Risk Score Figure 6: Empirical probability (y-axis) vs. risk score (x-axis) for COMPAS, Stop-and-Frisk, Chicago Police, and Lending Club on probability scale (top row) and logit probability scale (middle row).\",\"826\":\"The risk score distribution is in the bottom row.\",\"827\":\"The red lines on the logit probability scale (middle row) are best-fit straight lines.\",\"828\":\"A good fit (COMPAS and Stop-and-Frisk) suggests that the risk score and logit probability of outcomes (middle row) have a linear relationship.\",\"829\":\"In this case, the mimic model can be trained directly on the raw risk score.\",\"830\":\"When the relationship is not linear (Chicago Police and Lending Club), the risk score must be calibrated (Section 2.3.2).\",\"831\":\"The blue monotonic curves (middle row) are the nonlinear transformations learned during the calibration step.\",\"832\":\"This transformation is applied to the raw risk score to yield the transformed risk score (see Figure 7).\",\"833\":\"logit( p ) Transformed Risk Score Transformed Risk Score Figure 7: Logit empirical probability (y-axis) vs. transformed risk score (x-axis).\",\"834\":\"The red lines are best-fit straight lines.\",\"835\":\"A good fit suggests that the transformed risk score and logit probability of outcomes now have a linear relationship.\",\"836\":\"The mimic model can now be trained on the transformed risk score.\",\"837\":\"See Section 2.3.2 for more details.\",\"838\":\"Paper AIES\\u201918, February 2\\u20133, 2018, New Orleans, LA, USA 310\",\"839\":\"Robust and Stable Black Box Explanations Himabindu Lakkaraju 1 Nino Arsov 2 Osbert Bastani 3 Abstract As machine learning black boxes are increasingly being deployed in real-world applications, there has been a growing interest in developing post hoc explanations that summarize the behaviors of these black boxes.\",\"840\":\"However, existing algorithms for generating such explanations have been shown to lack stability and robustness to distribution shifts.\",\"841\":\"We propose a novel framework for generating robust and stable explanations of black box models based on adversarial training.\",\"842\":\"Our framework optimizes a minimax objective that aims to construct the highest fidelity explanation with respect to the worst-case over a set of adversarial perturbations.\",\"843\":\"We instantiate this algorithm for explanations in the form of linear models and decision sets by devising the required optimization procedures.\",\"844\":\"To the best of our knowledge, this work makes the first attempt at generating post hoc explanations that are robust to a general class of adversarial perturbations that are of practical interest.\",\"845\":\"Experimental evaluation with realworld and synthetic datasets demonstrates that our approach substantially improves robustness of explanations without sacrificing their fidelity on the original data distribution.\",\"846\":\"1.\",\"847\":\"Introduction Over the past decade, there has been an increasing interest in leveraging machine learning (ML) models to aid decision making in critical domains such as healthcare and criminal justice.\",\"848\":\"However, the successful adoption of these models in the real world relies heavily on how well decision makers are able to understand and trust their functionality (DoshiVelez & Kim, 2017; Lipton, 2016).\",\"849\":\"Decision makers must have a clear understanding of the model behavior so they can diagnose errors and potential biases in these models, 1 Harvard University 2 Macedonian Academy of Arts & Sciences 3 University of Pennsylvania.\",\"850\":\"Correspondence to: Himabindu Lakkaraju <hlakkaraju@hbs.edu>.\",\"851\":\"Proceedings of the 37th International Conference on Machine Learning, Online, PMLR 119, 2020.\",\"852\":\"Copyright 2020 by the author(s).\",\"853\":\"and decide when and how to employ them.\",\"854\":\"However, the proprietary nature and increasing complexity of machine learning models poses a severe challenge to understanding these complex black boxes, motivating the need for tools that can explain them in a faithful and interpretable manner.\",\"855\":\"Several different kinds of approaches have been proposed to produce interpretable post hoc explanations of black box models.\",\"856\":\"For instance, LIME and SHAP (Ribeiro et al., 2016; Lundberg & Lee, 2017b) explain individual predictions of any given black box classifier via local approximations.\",\"857\":\"On the other hand, approaches such as MUSE (Lakkaraju et al., 2019b) focus on explaining the high-level global behavior of any given black box.\",\"858\":\"However, recent work has shown that post hoc explanation methods are unstable (i.e., small perturbations to the input can substantially change the constructed explanations), as well as not robust to distribution shifts (i.e., explanations constructed using a given data distribution may not be valid on others) (Ghorbani et al., 2019; Lakkaraju & Bastani, 2020).\",\"859\":\"A key reason why many post hoc explanation methods are not robust is that they construct explanations by optimizing fidelity on a given covariate distribution p(x) (Ribeiro et al., 2018; 2016; Lakkaraju et al., 2019b)\\u2014i.e., choose the explanation that makes the same predictions as the black box on p(x).\",\"860\":\"To see why these approaches may fail to be robust, consider a covariate distribution p(x1, x2) where x1 and x2 are perfectly correlated, and an outcome y = I[x1 \\u2265 0].\",\"861\":\"Suppose we have a black box B\\u2217 (x1, x2) = I[x2 \\u2265 0], and an explanation E\\u0302(x1, x2) = I[x1 \\u2265 0].\",\"862\":\"Since x1 and x2 are perfectly correlated, the explanation has perfect fidelity\\u2014i.e., Pp(x1,x2)[E\\u0302(x1, x2) = B\\u2217 (x1, x2)] = 1.\",\"863\":\"(1) Thus, E\\u0302 appears to be a good explanation of B\\u2217 .\",\"864\":\"However, if the underlying covariate distribution changes\\u2014e.g., to p0 (x1, x2) where x1 and x2 are independent\\u2014then E\\u0302 no longer has high fidelity.\",\"865\":\"The lack of robustness is problematic because many of the undesirable behaviors of black box models that can be diagnosed using interpretability relate to distribution shifts.\",\"866\":\"For instance, it has been shown that interpretability can help users in assessing whether a model would transfer well to a new domain (Ribeiro et al., 2016)\\u2014e.g., from one \\fRobust and Stable Black Box Explanations hospital to another (Bastani, 2018); Caruana et al. (2015) show that experts use interpretable models to identify spurious relationships which do not hold if the underlying data changes\\u2014e.g., if a patient has asthma, he is not likely to die from pneumonia; these are intrinsically distribution shift issues.\",\"867\":\"Thus, for the explanations to shed light on these kinds of issues in the black box, high fidelity on the original distribution alone may be insufficient; instead, it also needs to achieve high fidelity on the relevant shifted distributions.\",\"868\":\"To further complicate the problem, we often do not know in advance what are the relevant distribution shifts.\",\"869\":\"Therefore, constructing explanations that are robust to a general class of possible shifts is of great importance.\",\"870\":\"We propose a novel algorithmic framework, RObust Post hoc Explanations (ROPE) for constructing black box explanations that are not only stable but also robust to shifts in the underlying data distribution.\",\"871\":\"To the best of our knowledge, our work is the first attempt at generating robust post hoc explanations for black boxes.\",\"872\":\"ROPE focuses on two notions of robustness.\",\"873\":\"The first is adversarial robustness (Ghorbani et al., 2019), which intuitively says that if the inputs are adversarially perturbed (by small amounts), then the explanation should not change significantly.\",\"874\":\"The second is distributional robustness (Namkoong & Duchi, 2016), which is similar to adversarial robustness but considers perturbations to the input distribution rather than individual inputs.\",\"875\":\"While ROPE considers distributional and adversarial robustness, these properties also improve stability.\",\"876\":\"This is due to the fact that explanations designed to be robust to input perturbations are not likely to vary drastically with small changes in inputs.\",\"877\":\"First, we propose a novel minimax objective that can be used to construct robust black box explanations for a given family of interpretable models.\",\"878\":\"This objective encodes the goal of returning the highest fidelity explanation with respect to the worst-case over a set of distribution shifts.\",\"879\":\"Second, we propose a set of distribution shifts that captures our intuition about the kinds of shifts to which interpretations should be robust.\",\"880\":\"In particular, this set includes shifts that contain perturbations to a small number of covariates.\",\"881\":\"For instance, robustness to these shifts ensure that the marginal dependence of the black box on a single covariate is preserved in the explanation, since the explanation must be robust to changes in that covariate alone.\",\"882\":\"Third, we propose algorithms for optimizing this objective in two settings: (i) explanations such as linear models with continuous parameters that can be optimized using gradient descent, in which case we use adversarial training (Goodfellow et al., 2015), and (ii) explanations such as decision sets with discrete parameters, in which case we use a sampling-based approximation in conjunction with submodular optimization (Lakkaraju et al., 2016).\",\"883\":\"We evaluated our approach ROPE on real-world data from healthcare, criminal justice, and education, focusing on datasets that include some kind of distribution shift\\u2014i.e., individuals from two different subgroups (e.g., patients from two different counties).\",\"884\":\"Our results demonstrate that the explanations constructed using ROPE are substantially more robust to distribution shifts than those generated by stateof-the-art post hoc explanation techniques such as LIME, SHAP, and MUSE.\",\"885\":\"Furthermore, the fidelity of ROPE explanations is equal or higher than the fidelity of the explanations generated by state-of-the-art methods even on the original data distribution, thus demonstrating that ROPE improves robustness of explanations without sacrificing their fidelity on the original data distributions.\",\"886\":\"In addition, we used synthetic data to analyze how the degree of distribution shift affects fidelity of the explanations constructed by our approach and other baselines.\",\"887\":\"Finally, we performed an experiment where the \\u201cblack box\\u201d models are themselves interpretable, and showed that ROPE explanations constructed based on shifted data are substantially more similar to the black box than the explanations output by other baselines.\",\"888\":\"2.\",\"889\":\"Related Work Post hoc explanations.\",\"890\":\"Many approaches have been proposed to directly learn interpretable models (Breiman, 2017; Tibshirani, 1997; Letham et al., 2015; Lakkaraju et al., 2016; Caruana et al., 2015; Kim & Bastani, 2019); however, complex models such as deep neural networks and random forests typically achieve higher accuracy than simpler interpretable models (Ribeiro et al., 2016); thus, it is often desirable to use complex models and then construct post hoc explanations to understand their behavior.\",\"891\":\"A variety of post hoc explanation techniques have been proposed, which differ in their access to the complex model (i.e., black box vs. access to internals), scope of approximation (e.g., global vs. local), search technique (e.g., perturbation-based vs. gradient-based), explanation families (e.g., linear vs. non-linear), etc.\",\"892\":\"For instance, in addition to LIME (Ribeiro et al., 2016) and SHAP (Lundberg & Lee, 2017a), several other local explanation methods have been proposed that compute saliency maps which capture importance of each feature for an individual prediction by computing the gradient with respect to the input (Simonyan et al., 2014; Sundararajan et al., 2017; Selvaraju et al., 2017; Smilkov et al., 2017).\",\"893\":\"An alternate approach is to provide a global explanation summarizing the black box as a whole (Lakkaraju et al., 2019a; Bastani et al., 2017), typically using an interpretable model.\",\"894\":\"There has also been recent work on exploring vulnerabilities of black box explanations (Adebayo et al., 2018; Slack et al., 2020; Lakkaraju & Bastani, 2020; Rudin, 2019; Dombrowski et al., 2019)\\u2014e.g., Ghorbani et al. (2019) demon\\fRobust and Stable Black Box Explanations strated that post hoc explanations can be unstable, changing drastically even with small perturbations to inputs.\",\"895\":\"However, none of the prior work has studied the problem of constructing robust explanations.\",\"896\":\"Distribution shift.\",\"897\":\"Distribution shift refers to settings where there is a mismatch between the training and test distributions.\",\"898\":\"A lot of work in this space has focused on covariate shift, where the covariate distribution p(x) changes but the outcome distribution p(y | x) remains the same.\",\"899\":\"This problem has been studied in the context of learning predictive models (Quionero-Candela et al., 2009; Jiang & Zhai, 2007).\",\"900\":\"Proposed solutions include importance weighting (Shimodaira, 2000), invariant representation learning (Ben-David et al., 2007; Tzeng et al., 2017), online learning (Cesa-Bianchi & Lugosi, 2006), and learning adversarially robust models (Teo et al., 2007; Graepel & Herbrich, 2004; Decoste & Scho\\u0308lkopf, 2002).\",\"901\":\"However, none of these approaches are applicable in our setting since they assume either that the underlying predictive model is not a black box, that data from the shifted distribution is available, or that the black box can be adaptively retrained.\",\"902\":\"Adversarial robustness.\",\"903\":\"Due to the discovery that deep neural networks are not robust (Szegedy et al., 2014), there has been recent interest in adversarial training (Goodfellow et al., 2015; Bastani et al., 2016; Sinha et al., 2018; Shaham et al., 2018), which optimizes a minimax objective that captures the worst-case over a given set of perturbations to the input data.\",\"904\":\"At a high level, these algorithms are based on gradient descent; at each gradient step, they solve an optimization problem to find the worst-case perturbation, and then compute the gradient at this perturbation.\",\"905\":\"For instance, for L\\u221e robustness (i.e., perturbations of bounded L\\u221e norm), Goodfellow et al. (2015) propose to approximate the optimization problem using a single gradient step, called the signed-gradient update; Shaham et al. (2018) generalizes this approach to arbitrary norms.\",\"906\":\"We propose a set of perturbations that capture our intuition about the kinds of distribution shifts that explanations should be robust to; for this set of shifts, we show how approximations along the lines of these previous approaches correspond to solving a linear program on every step to compute the gradient.\",\"907\":\"3.\",\"908\":\"Our Framework Here, we describe our framework for constructing robust explanations.\",\"909\":\"We assume we are given a black box model B\\u2217 : X \\u2192 Y, where X \\u2286 Rn is the space of covariates and Y is the space of labels.\",\"910\":\"Our goal is to construct a global explanation for the computation performed by B\\u2217 .\",\"911\":\"To construct such an explanation, one approach would be to learn an interpretable model that approximates B\\u2217 .\",\"912\":\"In particular, given a family E of interpretable models, a distribution p(x) over X, and a loss function ` : Y \\u00d7 Y \\u2192 R, this approach constructs an explanation as follows: E\\u0302(x) = arg min E\\u2208E Ep(x)[`(E(x), B\\u2217 (x))].\",\"913\":\"(2) In other words, E\\u0302 minimizes the error (as defined by `) relative to the black box B\\u2217 .\",\"914\":\"Intuitively, if E\\u0302 is a good approximation of B\\u2217 , then the computation performed by B\\u2217 should be mirrored by the computation performed by E\\u0302.\",\"915\":\"The problem with Eq. 2 is that it only guarantees that E\\u0302 is a good approximation of B\\u2217 according to the distribution p(x).\",\"916\":\"If the underlying data distribution changes, then E\\u0302 may no longer be a good approximation of B\\u2217 .\",\"917\":\"3.1.\",\"918\":\"Robust & Stable Explanations To construct explanations that are robust to shifts in the data distribution p, we first consider the general setting where we are given a set of distribution shifts that we want our explanations to be robust to; we describe a practical choice in Section 3.2.\",\"919\":\"We initially focus on distributional robustness; we connect it to adversarial robustness below.\",\"920\":\"Definition 3.1. Let p be a distribution over X, and let \\u03b4 \\u2208 Rn .\",\"921\":\"The \\u03b4-shifted distribution is p\\u03b4(x) = p(x \\u2212 \\u03b4).\",\"922\":\"In other words, p\\u03b4 places probability mass on covariates that are shifted by \\u03b4 compared to p. Definition 3.2. Let p be a distribution over X.\",\"923\":\"Given \\u2206 \\u2286 Rn , the set of \\u2206-small shifts is the set {p\\u03b4 | \\u03b4 \\u2208 \\u2206} of \\u03b4-shifted distributions.\",\"924\":\"For computational tractability, we assume: Assumption 3.3.\",\"925\":\"The set \\u2206 of shifts is a convex polytope.\",\"926\":\"Given a set of distribution shifts, our goal is to compute the best explanation that is robust to these shifts: Definition 3.4.\",\"927\":\"Given \\u2206 \\u2286 Rn , s0 \\u2208 N, and \\u03b4max \\u2208 R>0, the optimal robust explanation for (s0, \\u03b4max)-small shifts is E\\u0302 = arg min E\\u2208E max \\u03b4\\u2208\\u2206 Ep\\u03b4(x)[`(E(x), B\\u2217 (x))].\",\"928\":\"(3) That is, E\\u0302 optimizes the worst-case loss over shifts p\\u03b4.\",\"929\":\"Computing the worst-case over shifts p\\u03b4 can be intractable; instead, we use an upper bound on the objective in Eq. 3.\",\"930\":\"Lemma 3.5.\",\"931\":\"We have max \\u03b4\\u2208\\u2206 Ep\\u03b4(x)[`(E(x), B\\u2217 (x))] \\u2264 Ep(x) \\u0014 max \\u03b4\\u2208\\u2206 `(E(x + \\u03b4), B\\u2217 (x + \\u03b4)) \\u0015 .\",\"932\":\"Robust and Stable Black Box Explanations Proof: Note that max \\u03b4\\u2208\\u2206 Ep\\u03b4(x)[`(E(x), B\\u2217 (x))] = max \\u03b4\\u2208\\u2206 Z X `(E(x), B\\u2217 (x))p(x \\u2212 \\u03b4)dx = max \\u03b4\\u2208\\u2206 Z X `(E(x0 + \\u03b4), B\\u2217 (x0 + \\u03b4))p(x0 )dx0 \\u2264 Z X max \\u03b4\\u2208\\u2206 `(E(x0 + \\u03b4), B\\u2217 (x0 + \\u03b4))p(x0 )dx0 , \\u2264 Ep(x) \\u0014 max \\u03b4\\u2208\\u2206 `(E(x + \\u03b4), B\\u2217 (x + \\u03b4)) \\u0015 This lemma gives us a surrogate objective that we can optimize in place of the one in Eq. 3\\u2014i.e., E\\u0302 = arg min E\\u2208E Ep(x) \\u0014 max \\u03b4\\u2208\\u2206 `(E(x + \\u03b4), B\\u2217 (x + \\u03b4)) \\u0015 .\",\"933\":\"(4) In particular, this approach connects distributional robustness to adversarial robustness\\u2014Eq.\",\"934\":\"4 is the standard objective used to achieve adversarial robustness to input perturbations \\u03b4 \\u2208 \\u2206 (Goodfellow et al., 2015).\",\"935\":\"3.2.\",\"936\":\"General Class of Distribution Shifts Next, we propose a choice of \\u2206 that captures distributions shifts we believe to be of importance in practical applications.\",\"937\":\"We begin with a concrete setting that motivates our choice, but our choice includes shifts beyond this setting.\",\"938\":\"In particular, consider the case where X = {0, 1}d is a vector of indicators.\",\"939\":\"Our intuition is that when examining an explanation, users often want to understand how the model predictions change when a handful of components of an input x \\u2208 X change.\",\"940\":\"For instance, this intuition captures the case of counterfactual explanations, where the goal is to identify a small number of covariates that can be changed to affect the outcome (Zhang et al., 2018).\",\"941\":\"It also captures certain intuitions underlying fairness and causality, where we care about how the model changes when a covariate such as gender or ethnicity changes (Lakkaraju & Bastani, 2020; Rosenbaum & Rubin, 1983; Pearl, 2009).\",\"942\":\"Finally, it also encompasses the shifts considered in measures of variable importance (Hastie et al., 2001)\\u2014in particular, variable importance measures how the explanation changes when a single component of the input x is changed.\",\"943\":\"We can use the following choice to capture our intuition: \\u22061 = {\\u03b4 \\u2208 {\\u22121, 0, 1}n | k\\u03b4k0 \\u2264 s0} for s0 \\u2208 N.\",\"944\":\"However, this set is nonconvex.\",\"945\":\"We can approximate this constraint using the following set: \\u22062 = {\\u03b4 \\u2208 Rn | k\\u03b4k0 \\u2264 s0 \\u2227 k\\u03b4k\\u221e \\u2264 1}.\",\"946\":\"In particular, the constraint k\\u03b4k\\u221e \\u2264 1 ensures that \\u22121 \\u2264 \\u03b4i \\u2264 1 for each i \\u2208 {1, ..., n}.\",\"947\":\"Finally, we can replace the L0 norm with the L1 norm: \\u22063 = {\\u03b4 \\u2208 Rn | k\\u03b4k1 \\u2264 s0 \\u2227 k\\u03b4k\\u221e \\u2264 1}.\",\"948\":\"(5) This overapproximation is a heuristic based on the fact that the L1 loss induces sparsity in regression (Tibshirani, 1997).\",\"949\":\"More generally, we consider a shift from p to a distribution p0 such that p0 places probability mass on the same inputs x as p, except a small number of components of x are systematically changed by a small amount: \\u02dc \\u2206(s0, \\u03b4max) = {\\u03b4 \\u2208 Rn | k\\u03b4k0 \\u2264 s0 \\u2227 k\\u03b4k\\u221e \\u2264 \\u03b4max}, where s0 \\u2208 N and \\u03b4max \\u2208 R\\u2014i.e., \\u03b4 \\u2208 \\u02dc \\u2206(s0, \\u03b4max) is a sparse vector whose components are not too large.\",\"950\":\"However, \\u02dc \\u2206(s0, \\u03b4max) is nonconvex.\",\"951\":\"As above, for computational tractability, we approximate it using \\u2206(s0, \\u03b4max) = {\\u03b4 \\u2208 Rn | k\\u03b4k1 \\u2264 s0 \\u2227 k\\u03b4k\\u221e \\u2264 \\u03b4max}.\",\"952\":\"It is easy to see that \\u02dc \\u2206(s0, \\u03b4max) \\u2286 \\u2206(s0, \\u03b4max), so this choice overapproximates the set of shifts.\",\"953\":\"In particular, this choice \\u2206(s0, \\u03b4max) is a polytope, so it satisfies Assumption 3.3.\",\"954\":\"The set defined in Eq. 5 is \\u22063 = \\u2206(s0, 1).\",\"955\":\"A particular benefit of \\u2206(s0, \\u03b4max) is that the marginal dependencies of B\\u2217 on a component xi of an input x \\u2208 X is preserved in E\\u0302\\u2014i.e., if we unilaterally change xi by a small amount, B\\u2217 and E\\u0302 change in the same way.\",\"956\":\"Formally: Proposition 3.6. Suppose Y = R, `(y, y0 ) = |y \\u2212 y0 |, and \\u2206 = \\u2206(s0, \\u03b4max), and consider an explanation E\\u0302 with error Ep(x) \\u0014 max \\u03b4\\u2208\\u2206 `(E\\u0302(x + \\u03b4), B\\u2217 (x + \\u03b4)) \\u0015 \\u2264 \\u000f.\",\"957\":\"Then, letting \\u03b1 be the the one-hot encoding of i (i.e., \\u03b1i = 1 and \\u03b1j = 0 if i 6= j), for any c \\u2208 R such that |c| \\u2264 \\u03b4max, Ep(x) h (E\\u0302(x + c\\u03b1) \\u2212 E\\u0302(x)) \\u2212 (B\\u2217 (x + c\\u03b1) \\u2212 B\\u2217 (x)) i \\u2264 2\\u000f.\",\"958\":\"Proof: Note that Ep(x) h (E\\u0302(x + c\\u03b1) \\u2212 E\\u0302(x)) \\u2212 (B\\u2217 (x + c\\u03b1) \\u2212 B\\u2217 (x)) i \\u2264 Ep(x) h E\\u0302(x + c\\u03b1) \\u2212 B\\u2217 (x + c\\u03b1) i + Ep(x) h E\\u0302(x) \\u2212 B\\u2217 (x) i \\u2264 2\\u000f since c\\u03b1 \\u2208 \\u2206 As shown in Section 1, this property is not satisfied by standard measures of fidelity, since an explanation with perfect fidelity (i.e., Eq. 1) may use completely different covariates from the black box.\",\"959\":\"Robust and Stable Black Box Explanations 3.3.\",\"960\":\"Constructing Robust Linear Explanations We consider the case where E is the space of linear functions, or more generally, any model family that can be optimized using gradient descent.\",\"961\":\"Then, we can use adversarial training to optimize Eq. 4 (Goodfellow et al., 2015; Shaham et al., 2018).\",\"962\":\"The key idea behind adversarial training is to learn a model f\\u2217 \\u2208 F that is robust with respect to a worst-case set of perturbations to the input data\\u2014i.e., f\\u2217 = arg min f\\u2208F Ep(x,y) \\u0014 max \\u03b4\\u2208\\u2206 `(f(x + \\u03b4), y) \\u0015 .\",\"963\":\"We can straightforwardly adapt this formalism to our setting by replacing F with E and y with B\\u2217 (x).\",\"964\":\"In particular, suppose that E\\u03b8 \\u2208 E is parameterized by \\u03b8 \\u2208 \\u0398, where \\u0398 \\u2286 Rd and J(\\u03b8; x) is defined as follows: J(\\u03b8; x) = `(E\\u03b8(x), B\\u2217 (x)).\",\"965\":\"Then, Eq. 4 becomes \\u03b8\\u0302 = arg min \\u03b8\\u2208\\u0398 Ep(x) \\u0014 max \\u03b4\\u2208\\u2206 J(\\u03b8; x + \\u03b4)) \\u0015 .\",\"966\":\"(6) The adversarial training approach optimizes Eq. 6 by using stochastic gradient descent (Goodfellow et al., 2015; Shaham et al., 2018)\\u2014for a single sample x \\u223c p(x), the stochastic gradient estimate of the objective in Eq. 6 is \\u2207\\u03b8 max \\u03b4\\u2208\\u2206 J(\\u03b8; x + \\u03b4) \\u2248 \\u2207\\u03b8J(\\u03b8; x + \\u03b4\\u2217 ), where \\u03b4\\u2217 = arg max \\u03b4\\u2208\\u2206 J(\\u03b8, x + \\u03b4).\",\"967\":\"(7) To solve Eq. 7, we use the Taylor approximation J(\\u03b8; x + \\u03b4) \\u2248 J(\\u03b8; x) + \\u2207xJ(\\u03b8; x)> \\u03b4.\",\"968\":\"Using this approximation, Eq. 7 becomes \\u03b4\\u2217 = arg max \\u03b4\\u2208\\u2206 J(\\u03b8, x + \\u03b4) \\u2248 arg max \\u03b4\\u2208\\u2206 \\b J(\\u03b8; x) + \\u2207xJ(\\u03b8; x)> \\u03b4 = arg max \\u03b4\\u2208\\u2206 \\u2207xJ(\\u03b8; x)> \\u03b4, (8) where in the last line, we dropped the term J(\\u03b8; x) since it is constant with respect to \\u03b4.\",\"969\":\"Since we have assumed \\u2206 is a polytope, Eq. 8 is a linear program with free variables \\u03b4.\",\"970\":\"3.4.\",\"971\":\"Constructing Robust Rule-Based Explanations Here, we describe how we can construct robust rule-based explanations (Lakkaraju et al., 2016; Letham et al., 2015; Lakkaraju et al., 2019b)\\u2014e.g., decision sets (Lakkaraju et al., 2016; 2019b), decision lists (Letham et al., 2015), decision trees (Quinlan, 1986).\",\"972\":\"Any rule based model can be expressed as a decision set (Lakkaraju & Rudin, 2017), so we focus on these models.\",\"973\":\"Unlike explanations with continuous parameters, we can no longer use gradient descent to optimize Eq. 4.\",\"974\":\"Instead, we optimize it using a sampling-based heuristic.\",\"975\":\"We assume we are given a distribution p0(\\u03b4) over shifts \\u03b4 \\u2208 \\u2206.\",\"976\":\"Then, we approximate the maximum in Eq. 4 using k samples: max \\u03b4\\u2208\\u2206 F(\\u03b4) \\u2248 max \\u03b4j \\u223cp0(\\u03b4) F(\\u03b4j ), where F(\\u03b4) is a general objective and j \\u2208 {1, ..., k}.\",\"977\":\"In particular, our optimization problem becomes E\\u0302 = arg min E\\u2208E Ep(x) \\u0014 max \\u03b4j \\u223cp0(\\u03b4) `(E(x + \\u03b4j ), B\\u2217 (x + \\u03b4j )) \\u0015 .\",\"978\":\"(9) Next, a decision set E = {(s1, c1), (s2, c2) \\u00b7 \\u00b7 \\u00b7 (sm, cm)} \\u2286 S \\u00d7 C is a set of rules of the form (s, c) where s is a conjunction of predicates of the form (feature, operator, value) (e.g., age \\u2265 45) and c \\u2208 Y is a label.\",\"979\":\"Typically, we consider the case where Y is a finite set.\",\"980\":\"Existing algorithms (Lakkaraju et al., 2019b; 2016) for constructing decision set explanations primarily optimize for the following three goals: (i) maximizing the coverage of E\\u2014i.e., for x \\u2208 X, maximizing the probability that one of the rules (s, c) \\u2208 E has a condition s that is satisfied by x, (ii) minimizing the disagreement between E and B\\u2217 \\u2014i.e., minimizing the probability that E(x) 6= B\\u2217 (x), and (iii) minimizing the complexity of E\\u2014e.g., E has fewer rules.\",\"981\":\"In particular, these algorithms optimize the following objective: E\\u0302 = arg max E\\u2286S\\u00d7C {\\u2212disagree(E) + \\u03bb \\u00b7 cover(E)} (10) subj.\",\"982\":\"to |E| \\u2264 \\u03b1, where disagree(E) = m X i=1 Pp(x)(si(x) \\u2192 B\\u2217 (x) 6= ci) cover(E) = Pp(x)(\\u2203(s, c) \\u2208 E s.t. s(x) = true).\",\"983\":\"Here, we let s(x) = true if x satisfies s and s(x) = false otherwise.\",\"984\":\"In disagree(E), the event in the probability says if predicate si applies to x, then B\\u2217 (x) 6= ci.\",\"985\":\"To adapt this approach to solving Eq. 9, we modify the disagreement to take the worst-case over \\u03b4j \\u223c p0(\\u03b4): disagree(E) = m X i=1 Pp(x) si(x) \\u2192 \\u2203\\u03b4j \\u223c p0(\\u03b4) .\",\"986\":\"B\\u2217 (x + \\u03b4j ) 6= ci \\u0001 .\",\"987\":\"where j \\u2208 {1, ..., k}.\",\"988\":\"Here, we have used an approximation where we only check if si applies to the unperturbed input x; this choice enables our submodularity guarantee.\",\"989\":\"Robust and Stable Black Box Explanations Theorem 3.7. Suppose that p(x) = Uniform(Xtrain), where Xtrain \\u2286 X is a training set, is the empirical training distribution.\",\"990\":\"Then, the optimization problem Eq. 10 is nonmonotone and submodular with cardinality constraints.\",\"991\":\"Proof: To show non-monotonicity, it suffices to show that at least one term in the objective Eq. 10 is non-monotone.\",\"992\":\"Every time a new rule is added, the value of disagree either remains the same or increases, since the newly added rule may potentially label new instances incorrectly, but does not decrease the number of instances already labeled incorrectly by previously chosen rules.\",\"993\":\"Therefore, disagree(A) \\u2264 disagree(B) if A \\u2286 B, so \\u2212disagree(A) \\u2265 \\u2212disagree(B), which implies disagree term is non-monotone.\",\"994\":\"Thus, the entire linear combination is non-monotone.\",\"995\":\"To prove that the objective in Eq. 10 is submodular, we need to: (i) introduce a (large enough) constant C into the objective function to ensure that C \\u2212 disagree(E) is never negative,1 and (ii) prove that each of the its terms are submodular.\",\"996\":\"The cover term is clearly submodular\\u2014i.e., more data points will be covered when we add a new rule to a smaller set of rules compared to a larger set.\",\"997\":\"It is also easy to check that the disagree term is modular\\/additive (and therefore submodular).\",\"998\":\"Lastly, the constraint in Eq. 10 is a cardinality constraint.\",\"999\":\"Since the objective of Eqn.\",\"1000\":\"10 is non-monotone and submodular with cardinality constraints (Theorem 3.7), exactly solving it is NP-Hard (Khuller et al., 1999).\",\"1001\":\"So, we use approximate local search algorithm (Lee et al., 2009) to optimize Eq. 10.\",\"1002\":\"This algorithm provides the best known theoretical guarantees for this class of problems\\u2014i.e., (k + 2 + 1\\/k + \\u03b4)\\u22121 , where k is the number of constraints (k = 1 in our case) and \\u03b4 > 0.\",\"1003\":\"4.\",\"1004\":\"Experiments As part of our evaluation, we first use real-world data to assess the robustness of the post hoc explanations constructed using our algorithm and compare it to state-of-the-art baselines.\",\"1005\":\"Second, on synthetic data, we analyze how varying the degree of distribution shift impacts the fidelity of our explanations.\",\"1006\":\"Third, we ascertain the correctness of explanations generated using our framework\\u2014in particular, in cases where the black box is also an interpretable model B\\u2217 \\u2208 E, we study how closely the constructed explanations resemble the ground truth black box model.\",\"1007\":\"4.1.\",\"1008\":\"Experimental Setup Datasets.\",\"1009\":\"We analyzed three real-world datasets from criminal justice, healthcare, and education domains (Lakkaraju 1 Note that adding such a constant does not impact the solution to the optimization problem.\",\"1010\":\"et al., 2016).\",\"1011\":\"Our first dataset contains bail outcomes from two different state courts in the U.S. 1990-2009.\",\"1012\":\"It includes criminal history, demographic attributes, information about current offenses, and other details on 31K defendants who were released on bail.\",\"1013\":\"Each defendant in the dataset is labeled as either high risk or low risk depending on whether they committed new crimes when released on bail.\",\"1014\":\"Our second dataset contains academic performance records of about 19K students who were set to graduate high school in 2012 from two different school districts in the U.S.\",\"1015\":\"It includes information about grades, absence rates, suspensions, and tardiness scores from grades 6 to 8 for each of these students.\",\"1016\":\"Each student is assigned a class label indicating whether the student graduated high school on time.\",\"1017\":\"Our third dataset contains electronic health records of about 22K patients who visited hospitals in two different counties in California between 2010-2012.\",\"1018\":\"It includes demographic information, symptoms, current and past medical conditions, and family history of each patient.\",\"1019\":\"Each patient is assigned a class label which indicates whether the patient has been diagnosed with diabetes.\",\"1020\":\"Distribution shifts.\",\"1021\":\"Each of our datasets contains two different subgroups\\u2014e.g., our bail outcomes dataset contains defendants from two different states.\",\"1022\":\"We randomly choose data from one of these subgroups (e.g., a particular state) to be the training data, and data from the other subgroup to be the shifted data.\",\"1023\":\"In particular, we apply each algorithm on the training data to construct explanations, and evaluate these explanations on the shifted data.\",\"1024\":\"Our explanations.\",\"1025\":\"Our framework ROPE can be applied in a variety of configurations.\",\"1026\":\"We consider four: (i) ROPE logistic: We construct a single global logistic regression model using our framework to approximate any given black box.\",\"1027\":\"(ii) ROPE dset: We construct a single global decision set using our framework to approximate any given black box.\",\"1028\":\"(iii) ROPE logistic multi: We construct multiple local explanations.\",\"1029\":\"In particular, we first cluster the data into K subgroups (details below), and use ROPE to fit a robust logistic regression model to approximate the given black box for each subgroup.\",\"1030\":\"We also compute the centroid of each subgroup to serve as a representative sample.\",\"1031\":\"(iv) ROPE dset multi: Similar to ROPE logistic multi, except that we fit a decision set.\",\"1032\":\"Baselines.\",\"1033\":\"We compare our framework to the following state-of-the-art post hoc explanation techniques: (i) LIME (Ribeiro et al., 2016), (ii) SHAP (Lundberg & Lee, 2017a), and (iii) MUSE (Lakkaraju et al., 2019b).\",\"1034\":\"LIME and SHAP are model-agnostic, local explanation techniques that explain an individual prediction of a black box by training a linear model on data near that prediction.\",\"1035\":\"LIME and SHAP can be adapted to produce global explanations of any given black box using a submodular pick procedure (Ribeiro \\fRobust and Stable Black Box Explanations Algorithms Bail Academic Health Train Shift % Drop Train Shift % Drop Train Shift % Drop LIME 0.79 0.64 18.99% 0.68 0.57 16.18% 0.81 0.69 14.81% SHAP 0.76 0.66 13.16% 0.67 0.59 11.94% 0.83 0.68 18.07% MUSE 0.75 0.59 21.33% 0.66 0.51 22.73% 0.79 0.61 22.78% ROPE logistic 0.61 0.59 3.28% 0.57 0.57 0.00% 0.70 0.68 2.86% ROPE dset 0.64 0.61 4.69% 0.65 0.63 3.08% 0.73 0.69 5.48% ROPE logistic multi 0.79 0.74 6.33% 0.70 0.69 1.43% 0.82 0.76 7.32% ROPE dset multi 0.82 0.77 6.1% 0.73 0.71 2.74% 0.84 0.78 7.14% Table 1.\",\"1036\":\"Fidelity values of all the explanations are reported on both training data and shifted data, along with percentage drop in fidelity from training data to shifted data.\",\"1037\":\"Smaller values of percentage drop correspond to more robust explanations.\",\"1038\":\"Figure 1. Impact of changes in covariate correlations (left), means (middle), and variances (right) on percentage drop in fidelities.\",\"1039\":\"Lower values of percentage drop indicate higher robustness.\",\"1040\":\"Standard errors too small to be included.\",\"1041\":\"et al., 2016), which chooses a few representative points from the dataset and combines their corresponding local models to form a global explanation.\",\"1042\":\"In our evaluation, we use the global explanations of LIME and SHAP constructed using this technique.\",\"1043\":\"MUSE is a model-agnostic, global explanation technique; it provides global explanations in the form of two-level decision sets.\",\"1044\":\"Parameters.\",\"1045\":\"In case of LIME, SHAP, ROPE logistic multi, and ROPE dset multi, there is a parameter K which corresponds to the number of local explanations that need to be generated; K can also be thought as the number of subgroups in the data.\",\"1046\":\"We use Bayesian Information Criterion (BIC) to choose K.\",\"1047\":\"For a given dataset, we use the same K for all these techniques to ensure they construct explanations of the same size.\",\"1048\":\"For MUSE, we set all the parameters using the procedure in Lakkaraju et al. (2019b); to ensure these explanations are similar in size to the others, we fix the number of outer rules to be K. Finally, when using ROPE to construct rule-based explanations, there is a term \\u03bb in our objective (Eq. 10); we fix \\u03bb = 5.\",\"1049\":\"Black boxes.\",\"1050\":\"We generate post hoc explanations of deep neural networks (DNNs), gradient boosted trees, random forests, and SVMs.\",\"1051\":\"Here, we present results for a 5-layer DNN; remaining results are included in the Appendix.\",\"1052\":\"Results presented below are representative of those for other model families.\",\"1053\":\"Metrics.\",\"1054\":\"We use fidelity to measure performance\\u2014i.e., the fraction of inputs x in the given dataset for which E\\u0302(x) = B\\u2217 (x) (Lakkaraju et al., 2019b).\",\"1055\":\"Fidelity is straightforward to compute for MUSE, ROPE logistic, and ROPE dset since they construct an explanation in the form of a single interpretable model.\",\"1056\":\"However, the explanations constructed by LIME, SHAP, ROPE logistic multi, and ROPE dset multi consist of a collection of local models.\",\"1057\":\"In these cases, we need to determine which local model to use for each input x.\",\"1058\":\"By construction, each local model E\\u0302i is associated with a representative input xi, for i \\u2208 {1, ..., K}.\",\"1059\":\"Thus, we compute the distance kx \\u2212 xik for each i, and return E\\u0302i\\u2217 (x) where xi\\u2217 is closest to x.\",\"1060\":\"4.2.\",\"1061\":\"Robustness to Real Distribution Shifts We assess the robustness of explanations constructed using each approach on real-world datasets.\",\"1062\":\"In particular, we compute the fidelity of the explanations on both the training data and the shifted data, as well as the percentage change between the two.\",\"1063\":\"A large drop in fidelity from the training data to the shifted data indicates that the explanation is not robust.\",\"1064\":\"Ideally, explanations should have high fidelity on both the training data (indicating it is a good approximation of the black box model) and on the shifted data (indicating it is robust to distribution shift).\",\"1065\":\"Results for all three real-world datasets are shown in Table 1.\",\"1066\":\"As can be seen, all the explanations constructed using our framework ROPE have a much smaller drop in fidelity (0% to 7%) compared to those generated using the baselines.\",\"1067\":\"These results demonstrate that our approach significantly \\fRobust and Stable Black Box Explanations improves robustness.\",\"1068\":\"MUSE explanations have the largest percentage drop (21% to 23%), likely because MUSE relies entirely on the training data.\",\"1069\":\"In contrast, both LIME and SHAP employ input perturbations when constructing explanations (Ribeiro et al., 2016; Lundberg & Lee, 2017b), resulting in somewhat increased robustness compared to MUSE.\",\"1070\":\"Nevertheless, LIME and SHAP still demonstrate a considerable drop (13% to 19%), so they are still not very robust.\",\"1071\":\"The reason is because these approaches do not optimize a minimax objective that encodes robustness such as ours.\",\"1072\":\"Thus, these results validate our approach.\",\"1073\":\"In addition, Table 1 shows the actual fidelities on both training data and shifted data.\",\"1074\":\"As can be seen, the fidelities of ROPE logistic and ROPE dset are lower than the other approaches; these results are expected since ROPE logistic and ROPE dset only use a single logistic regression and a single decision set model, respectively, to approximate the entire black box.\",\"1075\":\"On the other hand, ROPE logistic multi and ROPE dset multi achieve fidelities that are equal or better than the other baselines.\",\"1076\":\"These results demonstrate that ROPE achieves robustness without sacrificing fidelity on the original training distribution.\",\"1077\":\"Thus, our approach strictly outperforms the baseline approaches.\",\"1078\":\"4.3. Impact of Degree of Distribution Shift on Fidelity Next, we assess how different kinds of distribution shifts impact the fidelity of explanations constructed using our framework and the baselines using synthetic data.\",\"1079\":\"We study the effects of three different kinds of shifts: (i) changes in the correlations between different components of the covariates, (ii) changes in the means of the covariates, and (iii) changes in the variances of the covariates.\",\"1080\":\"Shifts in correlation.\",\"1081\":\"We first describe our study for shifted data of type (i) above.\",\"1082\":\"We generate a synthetic dataset with 5K samples.\",\"1083\":\"The covariate dimension is randomly chosen between 2 and 10.\",\"1084\":\"Each data point is sampled x \\u223c N(\\u00b5, \\u03a3), where \\u00b5i = 0, \\u03a3ii = 1 and \\u03a3ij = \\u03b2, where \\u03b2 is uniformly random in [\\u22121, 1]\\u2014i.e., the correlation between any two components of the covariates is \\u03b2.\",\"1085\":\"The label for each data point is chosen randomly.\",\"1086\":\"We train a 5 layer DNN B\\u2217 on this dataset, and construct explanations for B\\u2217 .\",\"1087\":\"To generate shifted data, we generate a new dataset with the same approach as above but using a different correlation \\u03b20 = \\u03b2 + \\u03b1, where we vary \\u03b1.\",\"1088\":\"Then, we compute the percentage drop in fidelity of the explanations from the training data to each of the shifted datasets.\",\"1089\":\"We show results averaged over 100 runs in Figure 1 (left); the x-axis shows |\\u03b1|, and the y-axis shows the percentage drop.\",\"1090\":\"As can be seen, MUSE exhibits the highest drop in fidelity, followed closely by LIME and SHAP.\",\"1091\":\"In contrast, the ROPE explanations are substantially more robust, incurring less than a 10% drop in fidelity.\",\"1092\":\"Mean shifts.\",\"1093\":\"For shifts of type (ii) above, we follow the same procedure, except we use \\u03b2 = 0 for both the training and shifted datasets (i.e., uncorrelated covariates), and choose \\u00b5 randomly in [\\u22125, 5].\",\"1094\":\"To generate shifted data, we use a different \\u00b50 = \\u00b5 + \\u03b1.\",\"1095\":\"Results averaged across 100 runs are shown in Figure 1 (middle).\",\"1096\":\"ROPE is still the most robust, though LIME and SHAP are closer to ROPE than to MUSE.\",\"1097\":\"Explanations generated by MUSE are not robust even to small changes in covariate means.\",\"1098\":\"Variance shifts.\",\"1099\":\"For shifts of type (iii) above, we follow the same procedure, except we use \\u03b2 = 0, and choose \\u03a3ii = \\u03c3, where \\u03c3 is randomly chosen from [1, 10].\",\"1100\":\"To generate shifted data, we use a different \\u03c30 = \\u03c3+\\u03b1.\",\"1101\":\"Results averaged across 100 runs are shown in Figure 1 (right).\",\"1102\":\"The results are similar to the case of mean shifts.\",\"1103\":\"4.4. Evaluating Correctness of Explanations Here, we evaluate the correctness of the constructed explanations\\u2014i.e., how closely an explanation resembles the black box.\",\"1104\":\"To this end, we first train \\u201cblack box\\u201d models B\\u2217 \\u2208 E that are interpretable using the training data from each of our real-world datasets.\",\"1105\":\"Then, we construct an explanation E\\u0302 for B\\u2217 using the shifted data.\",\"1106\":\"If E\\u0302 resembles B\\u2217 structurally, then the underlying explanation technique is generating explanations that are correct despite being constructed based on shifted data.\",\"1107\":\"Logistic regression black box.\",\"1108\":\"We first train a logistic regression (LR) \\u201cblack box\\u201d B\\u2217 , and then use LIME, SHAP, ROPE logistic, and ROPE logistic multi to construct explanations E\\u0302 for B\\u2217 .\",\"1109\":\"We define the coefficient mismatch to measure the correctness.\",\"1110\":\"For ROPE logistic, it is computed as kE\\u0302 \\u2212 B\\u2217 k\\u2014i.e., the L2 distance between the weight vectors of E\\u0302 and B\\u2217 ; smaller distances mean the explanation more closely resembles the black box.\",\"1111\":\"The remaining approaches construct multiple logistic regression models\\u2014 one E\\u0302i for each representative input xi, for i \\u2208 {1, ..., K}.\",\"1112\":\"To measure the coefficient mismatch, we assign a weight wi to each xi that equals the fraction of inputs x that are assigned to xi (i.e., xi is the closest representative).\",\"1113\":\"Then, we measure coefficient mismatch as PK i=1 wi \\u00b7 kE\\u0302i \\u2212 B\\u2217 k.\",\"1114\":\"We also consider the case where B\\u2217 is a collection of multiple logistic regression (Multiple LR) models\\u2014one B\\u2217 i for each of the K subgroups.\",\"1115\":\"We construct explanations using LIME, SHAP, ROPE logistic, and ROPE logistic multi, and measure the coefficient mismatch as PK i=1 wi \\u00b7 kE\\u0302i \\u2212 B\\u2217 e k; In case of ROPE logistic, E\\u0302i = E\\u0302.\",\"1116\":\"Results for the bail dataset are shown in Table 2.\",\"1117\":\"When B\\u2217 is a single logistic regression (LR), ROPE logistic and ROPE logistic multi explanations achieve the best performance and are about 38.2% more structurally similar to B\\u2217 than the baselines.\",\"1118\":\"When B\\u2217 is multiple logistic regressions (Mul\\fRobust and Stable Black Box Explanations Algorithms Black Boxes LR Multiple LR DS Multiple DS Coefficient Coefficient Rule Feature Rule Feature Mismatch Mismatch Match Match Match Match LIME 4.37 5.01 \\u2013 \\u2013 \\u2013 \\u2013 SHAP 4.28 4.96 \\u2013 \\u2013 \\u2013 \\u2013 MUSE \\u2013 \\u2013 4.39 11.81 4.42 9.23 ROPE logistic 2.69 4.73 \\u2013 \\u2013 \\u2013 \\u2013 ROPE dset \\u2013 \\u2013 6.23 15.87 4.78 11.23 ROPE logistic multi 2.70 2.93 \\u2013 \\u2013 \\u2013 \\u2013 ROPE dset multi \\u2013 \\u2013 6.25 16.18 7.09 16.78 Table 2.\",\"1119\":\"Correctness of explanations on the bail dataset: Smaller coefficient mismatch and larger rule\\/feature match are better\\u2014i.e., the explanation more closely resembles the black boxes.\",\"1120\":\"ROPE dset multi and ROPE logistic multi uniformly outperform all the baselines.\",\"1121\":\"tiple LR), the coefficient mismatch of ROPE logistic multi is at least 38.05% lower than the baselines.\",\"1122\":\"We obtained similar results for the academic and health datasets.\",\"1123\":\"Decision set black box.\",\"1124\":\"As before, we train a decision set (DS) \\u201cblack box\\u201d B\\u2217 on the real-world training data, and then construct an explanation E\\u0302 based on the shifted data using MUSE, ROPE dset, and ROPE dset multi.\",\"1125\":\"We consider two measures of correctness for ROPE dset: (i) rule match: the number dr(E\\u0302, B\\u2217 ) of rules present in both E\\u0302 and B\\u2217 , and (ii) feature match: the number of features df (E\\u0302, B\\u2217 ) present in both E\\u0302 and B\\u2217 .\",\"1126\":\"As before, for ROPE dset multi and MUSE, we use the weighted measure PK i=1 wi \\u00b7 d(E\\u0302i, B\\u2217 ), where d = dr and d = df for rule match and feature match respectively.\",\"1127\":\"Higher rule and feature matches indicate that E\\u0302 better resembles B\\u2217 .\",\"1128\":\"We also consider the case where B\\u2217 consists of multiple decision sets (Multiple DS)\\u2014one B\\u2217 i for each of the K subgroups.\",\"1129\":\"On the bail dataset, ROPE dset multi has 42.3% (resp., 60.4%) higher rule match than MUSE when B\\u2217 corresponds to DS (resp., Multiple DS), and has at least 37% higher feature match than the baselines.\",\"1130\":\"4.5. Evaluating Stability of Explanations Finally, we evaluate the stability of the constructed explanations\\u2014i.e., how much do the explanations change if the input data is perturbed by a small amount.\",\"1131\":\"To this end, we first generate a synthetic dataset D with 5000 samples as described in Section 4.3.\",\"1132\":\"Then, we generate the perturbed dataset D0 by adding a small amount of Gaussian noise to data points\\u2014i.e., x0 = x + \\u000f, where \\u000f \\u223c N(0, 0.2).2 We then train LR, Multiple LR, DS, and Multiple DS \\u201cblack boxes\\u201d B\\u2217 , and use LIME, SHAP, ROPE logistic, ROPE logistic multi, ROPE dset, ROPE dset multi to construct explanations E\\u0302 for the corresponding B\\u2217 .\",\"1133\":\"We use the orig2 We experimented with other choices of variance in the range (0.2, 1.0] and found similar results.\",\"1134\":\"inal dataset D both to train each black box B\\u2217 as well as to construct its explanation E\\u0302.\",\"1135\":\"Then, for each black box B\\u2217 , we use the perturbed dataset D0 to construct an additional explanation E\\u03020 .\",\"1136\":\"Since D0 is obtained by making small changes to instances in D, E and E0 should be structurally similar if the explanation technique used to construct them generates stable explanations.\",\"1137\":\"We measure structural similarity of E and E0 \\u2014similar to the results in Table 2, we compute their coefficient mismatch in the case of LR and Multiple LR, and rule and feature match in case of DS and Multiple DS.\",\"1138\":\"We find that explanations E and E0 constructed using ROPE are 18.21% to 21.08% more structurally similar than those constructed using LIME, SHAP, or MUSE.\",\"1139\":\"Thus, our results demonstrate that ROPE explanations are much more stable than those constructed using baselines.\",\"1140\":\"5.\",\"1141\":\"Conclusions & Future Work In this paper, we proposed a novel framework based on adversarial training for constructing explanations that are robust to distribution shifts and are stable.\",\"1142\":\"Experimental results have demonstrated that our framework can be used to construct explanations that are far more robust to distribution shifts than those constructed using other state-of-the-art techniques.\",\"1143\":\"Our work paves way for several interesting future research directions.\",\"1144\":\"First, it would be interesting to extend our techniques to other classes of explanations such as saliency maps.\",\"1145\":\"Second, it would also be interesting to design adversarial attacks that can potentially exploit any vulnerabilities in our framework to generate unstable and incorrect explanations.\",\"1146\":\"Acknowledgements This work is supported in part by Google and NSF Award CCF-1910769.\",\"1147\":\"The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.\",\"1148\":\"Robust and Stable Black Box Explanations References Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., and Kim, B. Sanity checks for saliency maps.\",\"1149\":\"In Advances in Neural Information Processing Systems, pp. 9505\\u20139515, 2018.\",\"1150\":\"Bastani, H. Predicting with proxies: Transfer learning in high dimension.\",\"1151\":\"arXiv preprint arXiv:1812.11097, 2018.\",\"1152\":\"Bastani, O., Ioannou, Y., Lampropoulos, L., Vytiniotis, D., Nori, A., and Criminisi, A. Measuring neural net robustness with constraints.\",\"1153\":\"In Advances in neural information processing systems, pp. 2613\\u20132621, 2016.\",\"1154\":\"Bastani, O., Kim, C., and Bastani, H. Interpretability via model extraction.\",\"1155\":\"arXiv preprint arXiv:1706.09773, 2017.\",\"1156\":\"Ben-David, S., Blitzer, J., Crammer, K., and Pereira, F.\",\"1157\":\"Analysis of representations for domain adaptation.\",\"1158\":\"In Advances in neural information processing systems, pp. 137\\u2013144, 2007.\",\"1159\":\"Breiman, L. Classification and regression trees.\",\"1160\":\"Routledge, 2017.\",\"1161\":\"Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., and Elhadad, N. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission.\",\"1162\":\"In Knowledge Discovery and Data Mining (KDD), 2015.\",\"1163\":\"Cesa-Bianchi, N.\",\"1164\":\"and Lugosi, G. Prediction, learning, and games.\",\"1165\":\"Cambridge university press, 2006.\",\"1166\":\"Decoste, D.\",\"1167\":\"and Scho\\u0308lkopf, B. Training invariant support vector machines.\",\"1168\":\"Machine learning, 46(1-3):161\\u2013190, 2002.\",\"1169\":\"Dombrowski, A.-K., Alber, M., Anders, C.\",\"1170\":\"J., Ackermann, M., Mu\\u0308ller, K.-R., and Kessel, P.\",\"1171\":\"Explanations can be manipulated and geometry is to blame.\",\"1172\":\"arXiv preprint arXiv:1906.07983, 2019.\",\"1173\":\"Doshi-Velez, F.\",\"1174\":\"and Kim, B. Towards a rigorous science of interpretable machine learning.\",\"1175\":\"arXiv preprint arXiv:1702.08608, 2017.\",\"1176\":\"Ghorbani, A., Abid, A., and Zou, J. Interpretation of neural networks is fragile.\",\"1177\":\"In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3681\\u20133688, 2019.\",\"1178\":\"Goodfellow, I., Shlens, J., and Szegedy, C.\",\"1179\":\"Explaining and harnessing adversarial examples.\",\"1180\":\"In International Conference on Learning Representations, 2015.\",\"1181\":\"Graepel, T. and Herbrich, R. Invariant pattern recognition by semidefinite programming machines.\",\"1182\":\"In NIPS, pp. 33, 2004.\",\"1183\":\"Hastie, T., Tibshirani, R., and Friedman, J.\",\"1184\":\"The Elements of Statistical Learning.\",\"1185\":\"Springer New York Inc., 2001.\",\"1186\":\"Jiang, J.\",\"1187\":\"and Zhai, C.\",\"1188\":\"A two-stage approach to domain adaptation for statistical classifiers.\",\"1189\":\"In CIKM, pp. 401\\u2013 410, 2007.\",\"1190\":\"Khuller, S., Moss, A., and Naor, J.\",\"1191\":\"S.\",\"1192\":\"The budgeted maximum coverage problem.\",\"1193\":\"Information Processing Letters, 70(1):39\\u201345, 1999.\",\"1194\":\"Kim, C.\",\"1195\":\"and Bastani, O. Learning interpretable models with causal guarantees.\",\"1196\":\"arXiv preprint arXiv:1901.08576, 2019.\",\"1197\":\"Lakkaraju, H. and Bastani, O. \\u201dhow do i fool you?\\u201d: Manipulating user trust via misleading black box explanations.\",\"1198\":\"In AIES, 2020.\",\"1199\":\"Lakkaraju, H. and Rudin, C.\",\"1200\":\"Learning cost-effective and interpretable treatment regimes.\",\"1201\":\"In Artificial Intelligence and Statistics, pp. 166\\u2013175, 2017.\",\"1202\":\"Lakkaraju, H., Bach, S. H., and Leskovec, J.\",\"1203\":\"Interpretable decision sets: A joint framework for description and prediction.\",\"1204\":\"In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pp. 1675\\u2013 1684, 2016.\",\"1205\":\"Lakkaraju, H., Kamar, E., Caruana, R., and Leskovec, J. Faithful and customizable explanations of black box models.\",\"1206\":\"In AAAI Conference on Artificial Intelligence, Ethics, and Society (AIES), 2019a.\",\"1207\":\"Lakkaraju, H., Kamar, E., Caruana, R., and Leskovec, J. Faithful and customizable explanations of black box models.\",\"1208\":\"In Proceedings of the 2019 AAAI\\/ACM Conference on AI, Ethics, and Society, pp. 131\\u2013138.\",\"1209\":\"ACM, 2019b.\",\"1210\":\"Lee, J., Mirrokni, V. S., Nagarajan, V., and Sviridenko, M. Non-monotone submodular maximization under matroid and knapsack constraints.\",\"1211\":\"In Proceedings of the ACM Symposium on Theory of Computing (STOC), pp. 323\\u2013 332, 2009.\",\"1212\":\"Letham, B., Rudin, C., McCormick, T. H., and Madigan, D.\",\"1213\":\"Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model.\",\"1214\":\"Annals of Applied Statistics, 2015.\",\"1215\":\"Lipton, Z.\",\"1216\":\"C.\",\"1217\":\"The mythos of model interpretability.\",\"1218\":\"arXiv preprint arXiv:1606.03490, 2016.\",\"1219\":\"Lundberg, S. M. and Lee, S.-I. A unified approach to interpreting model predictions.\",\"1220\":\"In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R.\",\"1221\":\"(eds.\",\"1222\":\"), Neural Information Processing Systems (NIPS), pp. 4765\\u20134774.\",\"1223\":\"Curran Associates, Inc., 2017a.\",\"1224\":\"Robust and Stable Black Box Explanations Lundberg, S. M. and Lee, S.-I. A unified approach to interpreting model predictions.\",\"1225\":\"In Advances in Neural Information Processing Systems, pp. 4765\\u20134774, 2017b.\",\"1226\":\"Namkoong, H. and Duchi, J.\",\"1227\":\"C.\",\"1228\":\"Stochastic gradient methods for distributionally robust optimization with fdivergences.\",\"1229\":\"In Advances in neural information processing systems, pp. 2208\\u20132216, 2016.\",\"1230\":\"Pearl, J.\",\"1231\":\"Causality.\",\"1232\":\"Cambridge university press, 2009.\",\"1233\":\"Quinlan, J. R. Induction of decision trees.\",\"1234\":\"Machine learning, 1(1):81\\u2013106, 1986.\",\"1235\":\"Quionero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N.\",\"1236\":\"D.\",\"1237\":\"Dataset Shift in Machine Learning.\",\"1238\":\"The MIT Press, 2009.\",\"1239\":\"ISBN 0262170051, 9780262170055.\",\"1240\":\"Ribeiro, M. T., Singh, S., and Guestrin, C.\",\"1241\":\"\\u201dwhy should i trust you?\\u201d: Explaining the predictions of any classifier.\",\"1242\":\"In Knowledge Discovery and Data Mining (KDD), 2016.\",\"1243\":\"Ribeiro, M. T., Singh, S., and Guestrin, C. Anchors: Highprecision model-agnostic explanations.\",\"1244\":\"In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\",\"1245\":\"Rosenbaum, P. R. and Rubin, D. B. The central role of the propensity score in observational studies for causal effects.\",\"1246\":\"Biometrika, 70(1):41\\u201355, 1983.\",\"1247\":\"Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead.\",\"1248\":\"Nature Machine Intelligence, 1(5):206, 2019.\",\"1249\":\"Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. Grad-cam: Visual explanations from deep networks via gradient-based localization.\",\"1250\":\"In Proceedings of the IEEE international conference on computer vision, pp. 618\\u2013626, 2017.\",\"1251\":\"Shaham, U., Yamada, Y., and Negahban, S.\",\"1252\":\"Understanding adversarial training: Increasing local stability of supervised models through robust optimization.\",\"1253\":\"Neurocomputing, 307:195\\u2013204, 2018.\",\"1254\":\"Shimodaira, H. Improving predictive inference under covariate shift by weighting the log-likelihood function.\",\"1255\":\"Journal of statistical planning and inference, 90(2):227\\u2013244, 2000.\",\"1256\":\"Simonyan, K., Vedaldi, A., and Zisserman, A.\",\"1257\":\"Deep inside convolutional networks: Visualising image classification models and saliency maps.\",\"1258\":\"In International Conference on Learning Representations (ICLR), 2014.\",\"1259\":\"Sinha, A., Namkoong, H., and Duchi, J. Certifying some distributional robustness with principled adversarial training.\",\"1260\":\"In ICLR, 2018.\",\"1261\":\"Slack, D., Hilgard, S., Jia, E., Singh, S., and Lakkaraju, H. How can we fool lime and shap?\",\"1262\":\"adversarial attacks on post hoc explanation methods.\",\"1263\":\"2020.\",\"1264\":\"Smilkov, D., Thorat, N., Kim, B., Vie\\u0301gas, F. B., and Wattenberg, M. SmoothGrad: removing noise by adding noise.\",\"1265\":\"In ICML Workshop on Visualization for Deep Learning, 2017.\",\"1266\":\"Sundararajan, M., Taly, A., and Yan, Q. Axiomatic attribution for deep networks.\",\"1267\":\"In International Conference on Machine Learning (ICML), 2017.\",\"1268\":\"Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing properties of neural networks, 2014.\",\"1269\":\"Teo, C. H., Globerson, A., Roweis, S. T., and Smola, A.\",\"1270\":\"J.\",\"1271\":\"Convex learning with invariances.\",\"1272\":\"In NIPS, pp. 1489\\u2013 1496, 2007.\",\"1273\":\"Tibshirani, R. The lasso method for variable selection in the cox model.\",\"1274\":\"Statistics in medicine, 16(4):385\\u2013395, 1997.\",\"1275\":\"Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adversarial discriminative domain adaptation.\",\"1276\":\"In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7167\\u20137176, 2017.\",\"1277\":\"Zhang, X., Solar-Lezama, A., and Singh, R. Interpreting neural network judgments via minimal, stable, and symbolic corrections.\",\"1278\":\"In Advances in Neural Information Processing Systems, pp. 4874\\u20134885, 2018.\",\"1279\":\"Treeview and Disentangled Representations for Explaining Deep Neural Networks Decisions Prasanna Sattigeri Karthikeyan Natesan Ramamurthy IBM Research AI Thomas J. Watson Research Center Yorktown Heights NY 10598 {psattig,knatesa}@us.ibm.com Jayaraman J. Thiagarajan Bhavya Kailkhura Center for Applied Scientific Computing Lawrence Livermore National Laboratory Livermore, CA 94550 {jjayaram,kailkhura1}@llnl.gov Abstract\\u2014With the advent of highly predictive but opaque deep learning models, it has become more important than ever to understand and explain the predictions of such models.\",\"1280\":\"Many popular approaches define interpretability as the inverse of complexity and achieve interpretability at the cost of accuracy.\",\"1281\":\"This introduces a risk of producing interpretable but misleading explanations.\",\"1282\":\"As humans, we are prone to engage in this kind of behavior [11].\",\"1283\":\"In this paper, we take the view that the complexity of the explanations should correlate with complexity of the decision.\",\"1284\":\"We propose to build a Treeview representation of the complex model using disentangled representations, which reveals the iterative rejection of unlikely class labels until the correct association is predicted.\",\"1285\":\"I.\",\"1286\":\"INTERPRETABILITY IN MACHINE LEARNING Analysis and inference from rich, multivariate data is a ubiquitous problem in science and engineering.\",\"1287\":\"The quality of inference depends on the quality of relevant features extracted, a.k.a. data representation.\",\"1288\":\"While recent deep learning approaches have revolutionized representation learning, there is an increase in the number of researchers that complain about the difficulty in interpreting the decisions of such complex models.\",\"1289\":\"Despite their remarkable capacity to learn complex functions from data, there is a fair amount of distrust in applying such models particularly to domains such as as medicine, criminal justice, and finance, where humans are directly impacted.\",\"1290\":\"To complicate matters further, the term interpretability itself does not have a clear definition [11], and can be understood at various levels.\",\"1291\":\"This poses a real problem in the universal acceptance of machine learning solutions, and deserves to be tackled head on.\",\"1292\":\"The simplest, yet a popular, definition of interpretability is as the inverse of complexity - e.g., there is a long standing argument that linear models are more interpretable than nonlinear models because of their simplicity in the parameter space.\",\"1293\":\"There have been deliberate efforts to develop models that are interpretable in the recent past [14], [12], [17] .\",\"1294\":\"Also, the recently developed locally interpretable model-agnostic explanations (LIME) approach poses interpretation as a sparse local fitting problem [13].\",\"1295\":\"However, a simple model is less expressive by definition and hence one cannot express a complex process such as human cognition using a simple model with guaranteed generalization and high accuracy.\",\"1296\":\"One of the fundamental problems seems to be that the language for interpreting machine learning models has not matured, and it still relies on incomplete ideas such as simplicity.\",\"1297\":\"Furthermore, drawing parallels with human decision making, a complex decision will take a lot of words for a satisfactory explanation as opposed to a simple decision, and the explanation will inevitably involve abstractions which will be revealed on a need-to-know basis.\",\"1298\":\"This paper attempts to take a step in the direction of a developing a tool for interpreting complex models, the TreeView which matches the sophistication of the decision to the explanation generated.\",\"1299\":\"In particular, we propose to explore deep learning models using sequential elimination via feature-space partitions.\",\"1300\":\"This is essentially a process of understanding via hierarchical partitioning and association of feature space where the most undesirable options are discarded and the scope of options is progressively narrowed down until exactly one option is left.\",\"1301\":\"We employ disentangled generative models [3], [10] which enable unsupervised learning of the high level concepts.\",\"1302\":\"These concepts are then mapped to the feature-space partitions to visualize the decision process in the data input space improving the ease of the use of the process.\",\"1303\":\"II.\",\"1304\":\"PROPOSED APPROACH A typical supervised learning algorithm consists of two stages.\",\"1305\":\"The first stage is feature learning or extraction where the input data space X is transformed into a representation or feature space Y.\",\"1306\":\"The second stage is predictive learning which maps the feature space Y to the label space Z.\",\"1307\":\"For example, Y could amplify the factors that are discriminatory across classes thereby aiding the predictive inference.\",\"1308\":\"The transformations learned in these two stages can be concisely denoted as TXY : X \\u2192 Y and TY Z : Y \\u2192 Z, whereas the full map is given as TXZ = TXY \\u25e6 TY Z.\",\"1309\":\"The representation space Y that is important for the high performance of the learning task, is also opaque and un-interpretable if the feature learning algorithm is complex and non-linear.\",\"1310\":\"Our goal is to provide interpretable explanations for this space using the proposed TreeView model.\",\"1311\":\"In this section, we describe the proposed approach in the context of understanding features learned by a deep neural network (see figure 1 for the workflow).\",\"1312\":\"Unraveling the 284 978-0-7381-3126-9\\/20\\/$31.00 \\u00a92020 IEEE Asilomar 2020 2020 54th Asilomar Conference on Signals, Systems, and Computers | 978-0-7381-3126-9\\/20\\/$31.00 \\u00a92020 IEEE | DOI: 10.1109\\/IEEECONF51394.2020.9443487 Authorized licensed use limited to: Dalhousie University.\",\"1313\":\"Downloaded on September 27,2021 at 14:57:05 UTC from IEEE Xplore.\",\"1314\":\"Restrictions apply.\",\"1315\":\"Fig. 1: The proposed approach uses a disentangled generative models to visualize the decision path in the input space.\",\"1316\":\"The top path shows the construction of the meta-features.\",\"1317\":\"The features importance\\u2019s of decision tree, learned on top these meta-features are then mapped back to latent space of disentangled generative models to visualize the decision path.\",\"1318\":\"mechanics of the hidden layer representations can provide interesting insights into the trained model.\",\"1319\":\"This is a main distinction of our approach compared to other existing methods that attempt to learn a simpler surrogate (e.g. linear models) to explain the predictions for individual examples.\",\"1320\":\"We decompose the feature space Y into K factors using disentangled generative models trained explicitly on this space.\",\"1321\":\"Let us denote the feature dimensions corresponding to these factors using the sets {S1, .\",\"1322\":\".\",\"1323\":\".\",\"1324\":\", SK}.\",\"1325\":\"We obtain K different clusterings of samples, one for each of the subspaces of Y given by Si.\",\"1326\":\"We then construct a K\\u2212dimensional meta-feature for each sample as a collection of its K cluster labels.\",\"1327\":\"The TreeView framework uses these meta-features used in a decision tree and thier mapping to a disentangled generative models to create an easily interpretable visualization of the mechanics of the learned deep network.\",\"1328\":\"A. Identifying Factors The hidden neurons in a deep neural network learn different aspects of the training data samples that facilitate the task.\",\"1329\":\"The same aspect of the training data can be captured by different neurons because of the distributed nature of learned representations.\",\"1330\":\"Hence it is reasonable to expect that this feature space can be clustered into factors, each of which represents one aspect of the input space at some level of abstraction.\",\"1331\":\"We elaborate this by extending the notation provided before.\",\"1332\":\"For layer l, the activations are given by the matrix Yl \\u2208 RNlxT , where Nl and T are the number of filters and training samples respectively.\",\"1333\":\"We cluster Yl into K factors, {Fl i}K i=1, such that each factor is comprised of a set of hidden neurons that have similar distribution of activations across the whole training set.\",\"1334\":\"B. Constructing Meta-Features The second important step is to create meta-features that are easier to interpret, but still perform well in the learning task.\",\"1335\":\"We construct the surrogate model as follows: First, we consider the activations in each factor i, Fl i \\u2208 RNlxT , and cluster the T samples in this factor into L groups.\",\"1336\":\"We then create a metafeature matrix M \\u2208 RKxT by aggregating the K cluster labels for each sample.\",\"1337\":\"Subsequently, we train a predictor Pl i that directly predicts the cluster label for a sample using the input space examples.\",\"1338\":\"In our case, the predictor used is a random forest classifier.\",\"1339\":\"This will enable the analyst to understand each factor using the input examples directly, and circumvent the need to train an approximate model (e.g. linear).\",\"1340\":\"A decision tree surrogate is finally created for the neural network classifier using the meta-features.\",\"1341\":\"C. Mapping Meta-Features to High Level Concepts To visualize the meta-features we first learn latent high level concepts using an disentangled generative model and then learning a mapping from latent space to meta-features.\",\"1342\":\"The individual dimensions of a disentangled latent space are able to better capture high level concepts in the data.\",\"1343\":\"Specifically, we employ DIP-VAE [10] a variant of Variational Autoencoder (VAE) that encourages disentanglement in the latent space of the VAE.\",\"1344\":\"The generative process looks as following: sample a latent z from a prior p(z) which is then passed through a decoder to generate the observation x 285 Authorized licensed use limited to: Dalhousie University.\",\"1345\":\"Downloaded on September 27,2021 at 14:57:05 UTC from IEEE Xplore.\",\"1346\":\"Restrictions apply.\",\"1347\":\"Fig. 2: Treeview visualization for a sample from UCI Image Segmentation Dataset which is correctly classified by the neural network.\",\"1348\":\"Fig. 3: Latent traversal plot showing the mapping of the latent dimensions and the lesion properties.\",\"1349\":\"The row corresponds to different latent dimensions.\",\"1350\":\"The columns corresponds to manipulation of the latent dimensions between range \\u221210 and 10.\",\"1351\":\"286 Authorized licensed use limited to: Dalhousie University.\",\"1352\":\"Downloaded on September 27,2021 at 14:57:05 UTC from IEEE Xplore.\",\"1353\":\"Restrictions apply.\",\"1354\":\"Fig. 4: Treeview representation of the decision path.\",\"1355\":\"Each row corresponds to a sample and the row depicts the sequence of factors (x-axis label) used by the decision tree to arrive a the decision.\",\"1356\":\"The y-axis label shows the true class label and the title shows the most likely label at each stage of the decision tree sampled from the observation likelihood p(x|z).\",\"1357\":\"The inference problem is to obtain the latent representations given the sample.\",\"1358\":\"In VAE, a posterior distribution over the latent given the sample ( q(z|x)) is parametrized using a encoder network and learned maximizing the Evidence Lower Bound (ELBO) [8].\",\"1359\":\"DIP-VAE encouraged disentanglement in the latent space by matching the aggregated posterior (q(z) = R q(z|x)p(x)dx)) to the prior distribution p(z).\",\"1360\":\"This is aimed at rendering the dimensions of the latent representations independent of each other.\",\"1361\":\"We then learn K predictors that aim to predict the cluster label of a sample in each factor space using the latent representations as the input features.\",\"1362\":\"The feature importance values each of these prediction task is then used to map the factor to the latent space.\",\"1363\":\"D. TreeView Design Consider a test example whose prediction we want to interpret using the Treeview visualization.\",\"1364\":\"Assuming that its true label is known, we compute the meta-feature using the predictors corresponding to each of the factors.\",\"1365\":\"This metafeature is used with the decision tree surrogate to predict the label and the sequence of nodes visited in the decision tree during this prediction is traced.\",\"1366\":\"In addition, the relative ranks of the high level concepts in the factor-specific predictor Pl i are shown to allow the user to create a mental map between the class labels and input data.\",\"1367\":\"A factor\\u2019s influence on any sample can be visualized in the data input space by reconstructing the sample through the decoder after scaling of the latent representation by the factor\\u2019s feature importance.\",\"1368\":\"III.\",\"1369\":\"USE CASES A.\",\"1370\":\"Use Case I - Known Disentangled Input Features We first demonstrate the usage and effectiveness of the proposed approach using the UCI Image segmentation dataset [2] where the input features are already in form that corresponds to high level concepts and are disentangled, thus does not need the learning of a disentangled generative model.\",\"1371\":\"The dataset contains 2310 instances, each of which belong to one of the 7 outdoor image categories.\",\"1372\":\"Each instance is a 3 \\u00d7 3 region that were hand segmented and is characterized by 19 attributes that 287 Authorized licensed use limited to: Dalhousie University.\",\"1373\":\"Downloaded on September 27,2021 at 14:57:05 UTC from IEEE Xplore.\",\"1374\":\"Restrictions apply.\",\"1375\":\"show describe either the semantic or statistical properties of the instance.\",\"1376\":\"The deep neural network used for classification consisted of 3-hidden layers with sizes 128 \\u2212 64 \\u2212 64.\",\"1377\":\"At each hidden layer the dropout rate was fixed at 0.1.\",\"1378\":\"Random forests were used to predict the meta-feature for each of the factors which in turn were fed into a decision tree classifier.\",\"1379\":\"Figure 2 shows the TreeView visualization for a correctly classified example.\",\"1380\":\"While the root factor rejects the hypothesis for the classes Grass, Path, Sky, Window (indicated by a red bounded box) right away, a sequence of three more factors were required to reject the hypotheses for Cement.\",\"1381\":\"B. Use Case II - Unknown Disentangled Input Features We now show the usage of the proposed Treeview representation on the disease classification task of the ISIC 2018 Challenge [5].\",\"1382\":\"The dataset used in the challenge contains 10,015 publicly available dermoscopic images [1], [16] labeled with 7 disease classes; melanoma, melanocytic nevus, basal cell carcinoma, actinic keratosis, benign keratosis, dermatofibroma and vascular lesion.\",\"1383\":\"The goal of the task is to predict the correct class label accurately.\",\"1384\":\"The ABCD signature [7]; asymmetry, border, color and diameter are widely used properties by dermatologists for visual preliminary examination.\",\"1385\":\"Here, we aim to explain a well performing deep neural network decisions with visual cues that capture similar cues.\",\"1386\":\"Following the methodology described in [9], we train a Densenet201 [4] model that achieves an accuracy of 86% on the internal held-out validation set.\",\"1387\":\"We train the DIP-VAE-II variant with a 10-dimensional latent space and the observation likelihood is set to Gaussian distribution.\",\"1388\":\"The encoder and decoder consist of 7 convolution and deconvolution layers, respectively [15].\",\"1389\":\"Batchnorm [6] is applied all the layers except the last layers in the encoder and the decoder.\",\"1390\":\"Figure 3 shows the effect of change in a latent dimension on the generated image.\",\"1391\":\"As it can be observed from this plot, each dimension can be mapped properties such as lesion border, shape, etc which are important for the disease classification task.\",\"1392\":\"The penultimate layer activations of the Densenet201 are used to identify the factors.\",\"1393\":\"We set the desired number of factors to 50 and the number of data clusters in each factor to 10.\",\"1394\":\"Thus, each sample is now represented by a 50 dimension categorical meta-feature.\",\"1395\":\"A decision tree with a maximum depth of 10 is learned that allows us to express a prediction of a class label as a sequence of factor used for decision making.\",\"1396\":\"We train 50 ExtraTreeClassifier to predict the sample cluster id in each of 50 feature space partitions using DIP-VAE latent space representations.\",\"1397\":\"Figure 4 shows treeviews generated for a sample images.\",\"1398\":\"Each row corresponds to a sample and the row depicts the sequence of factors (x-axis label) used by the decision tree to arrive a the decision.\",\"1399\":\"The y-axis label shows the true class label and the title shows the most likely label at each stage of the decision tree.\",\"1400\":\"As observed by the plot, this representation can be used to visualize the factors that influence a decision.\",\"1401\":\"For example, the forth row shows that for this sample and class label combination the sense of boundary appears to be important.\",\"1402\":\"A fine-grained analysis of the high-level concepts in the input space can provide more insights into the connection between the input and label spaces, thereby enabling the analyst to validate their mental map between the spaces.\",\"1403\":\"The Treeview visualization allows a convenient transition between factors, class labels, and the input data space, while staying relevant to the features inferred using the neural network.\",\"1404\":\"REFERENCES [1] Noel Codella, Veronica Rotemberg, Philipp Tschandl, M. Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, Harald Kittler, and Allan Halpern.\",\"1405\":\"Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the International Skin Imaging Collaboration (ISIC).\",\"1406\":\"arXiv:1902.03368, March 2019.\",\"1407\":\"[2] Dheeru Dua and Casey Graff.\",\"1408\":\"UCI machine learning repository, 2017.\",\"1409\":\"[3] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner.\",\"1410\":\"beta-vae: Learning basic visual concepts with a constrained variational framework.\",\"1411\":\"In International Conference on Learning Representations, 2017.\",\"1412\":\"[4] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.\",\"1413\":\"Densely connected convolutional networks.\",\"1414\":\"In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\\u20134708, 2017.\",\"1415\":\"[5] International Skin Imaging Collaboration.\",\"1416\":\"ISIC 2018: Skin lesion analysis towards melanoma detection, 2018.\",\"1417\":\"Available: https:\\/\\/challenge2018.\",\"1418\":\"isic-archive.com\\/.\",\"1419\":\"[6] Sergey Ioffe and Christian Szegedy.\",\"1420\":\"Batch normalization: Accelerating deep network training by reducing internal covariate shift.\",\"1421\":\"arXiv preprint arXiv:1502.03167, 2015.\",\"1422\":\"[7] Jeremy Kawahara, Sara Daneshvar, Giuseppe Argenziano, and Ghassan Hamarneh.\",\"1423\":\"Seven-point checklist and skin lesion classification using multitask multimodal neural nets.\",\"1424\":\"IEEE journal of biomedical and health informatics, 23(2):538\\u2013546, 2018.\",\"1425\":\"[8] Diederik P Kingma and Max Welling.\",\"1426\":\"Auto-encoding variational bayes.\",\"1427\":\"arXiv preprint arXiv:1312.6114, 2013.\",\"1428\":\"[9] Newton M. Kinyanjui, Timothy Odonga, Celia Cintas, Noel C. F. Codella, Rameswar Panda, Prasanna Sattigeri, and Kush R. Varshney.\",\"1429\":\"Fairness of classifiers across skin tones in dermatology.\",\"1430\":\"In Anne L. Martel, Purang Abolmaesumi, Danail Stoyanov, Diana Mateus, Maria A. Zuluaga, S. Kevin Zhou, Daniel Racoceanu, and Leo Joskowicz, editors, Medical Image Computing and Computer Assisted Intervention \\u2013 MICCAI 2020, pages 320\\u2013329, Cham, 2020.\",\"1431\":\"Springer International Publishing.\",\"1432\":\"[10] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan.\",\"1433\":\"Variational inference of disentangled latent concepts from unlabeled observations.\",\"1434\":\"Intl.\",\"1435\":\"Conf.\",\"1436\":\"on Learning Representations, 2017.\",\"1437\":\"[11] Zachary C Lipton.\",\"1438\":\"The mythos of model interpretability.\",\"1439\":\"IEEE Spectrum, 2016.\",\"1440\":\"[12] Dmitry Malioutov and Kush Varshney.\",\"1441\":\"Exact rule learning via boolean compressed sensing.\",\"1442\":\"In Proceedings of the 30th international conference on machine learning, pages 765\\u2013773, 2013.\",\"1443\":\"[13] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.\",\"1444\":\"Model-agnostic interpretability of machine learning.\",\"1445\":\"arXiv preprint arXiv:1606.05386, 2016.\",\"1446\":\"[14] Ronald L Rivest.\",\"1447\":\"Learning decision lists.\",\"1448\":\"Machine learning, 2(3):229\\u2013 246, 1987.\",\"1449\":\"[15] Jayaraman J Thiagarajan, Bindya Venkatesh, Deepta Rajan, and Prasanna Sattigeri.\",\"1450\":\"Improving reliability of clinical models using prediction calibration.\",\"1451\":\"In Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, and Graphs in Biomedical Image Analysis, pages 71\\u201380.\",\"1452\":\"Springer, 2020.\",\"1453\":\"[16] Philipp Tschandl, Cliff Rosendahl, and Harald Kittler.\",\"1454\":\"Data descriptor: The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions.\",\"1455\":\"Sci.\",\"1456\":\"Data, 5:180161, August 2018.\",\"1457\":\"[17] Berk Ustun and Cynthia Rudin.\",\"1458\":\"Methods and models for interpretable linear classification.\",\"1459\":\"arXiv preprint arXiv:1405.4047, 2014.\",\"1460\":\"288 Authorized licensed use limited to: Dalhousie University.\",\"1461\":\"Downloaded on September 27,2021 at 14:57:05 UTC from IEEE Xplore.\",\"1462\":\"Restrictions apply.\"}}", "query": "Prognostic modelling using machine learning techniques has been used to predict the risk of kidney graft failure after transplantation. Despite the clinically suitable prediction performance of the models, their decision logic cannot be interpreted by physicians, hindering clinical adoption. eXplainable Artificial Intelligence (XAI) is an emerging research discipline to investigate methods for explaining machine learning models which are regarded as \u2018black-box\u2019 models. In this paper, we present a novel XAI approach to study the influence of time on information gain of donor and recipient factors in kidney graft survival prediction. We trained the most accurate models regardless of their transparency level on subsequent non-overlapping temporal cohorts and extracted faithful decision trees from the models as global surrogate explanations. Comparative exploration of the decision trees reveals insightful information about how the information gain of the input features changes over time.", "history": "{\"filename\":{\"847\":\"ROPE (robust MUSE).pdf\",\"286\":\"BETA2.pdf\",\"866\":\"ROPE (robust MUSE).pdf\",\"854\":\"ROPE (robust MUSE).pdf\",\"1444\":\"TreeView2.pdf\",\"289\":\"BETA2.pdf\",\"445\":\"BETA2.pdf\",\"16\":\"two level boolean rules.pdf\",\"281\":\"BETA2.pdf\",\"1450\":\"TreeView2.pdf\",\"890\":\"ROPE (robust MUSE).pdf\",\"18\":\"two level boolean rules.pdf\",\"297\":\"BETA2.pdf\",\"582\":\"distill-and-compare.pdf\",\"1241\":\"ROPE (robust MUSE).pdf\",\"1049\":\"ROPE (robust MUSE).pdf\",\"620\":\"distill-and-compare.pdf\",\"485\":\"distill-and-compare.pdf\",\"605\":\"distill-and-compare.pdf\",\"514\":\"distill-and-compare.pdf\",\"515\":\"distill-and-compare.pdf\",\"614\":\"distill-and-compare.pdf\",\"513\":\"distill-and-compare.pdf\",\"505\":\"distill-and-compare.pdf\",\"536\":\"distill-and-compare.pdf\",\"668\":\"distill-and-compare.pdf\",\"311\":\"BETA2.pdf\",\"855\":\"ROPE (robust MUSE).pdf\",\"512\":\"distill-and-compare.pdf\",\"893\":\"ROPE (robust MUSE).pdf\"},\"sentence\":{\"847\":8,\"286\":5,\"866\":27,\"854\":15,\"1444\":165,\"289\":8,\"445\":164,\"16\":16,\"281\":0,\"1450\":171,\"890\":51,\"18\":18,\"297\":16,\"582\":130,\"1241\":402,\"1049\":210,\"620\":168,\"485\":33,\"605\":153,\"514\":62,\"515\":63,\"614\":162,\"513\":61,\"505\":53,\"536\":84,\"668\":216,\"311\":30,\"855\":16,\"512\":60,\"893\":54},\"text\":{\"847\":\"Introduction Over the past decade, there has been an increasing interest in leveraging machine learning (ML) models to aid decision making in critical domains such as healthcare and criminal justice.\",\"286\":\"1 INTRODUCTION The successful adoption of predictive models in settings such as criminal justice and health care hinges on how much judges and doctors can understand and trust the functionality of these machine learning models.\",\"866\":\"For instance, it has been shown that interpretability can help users in assessing whether a model would transfer well to a new domain (Ribeiro et al., 2016)\\u2014e.g., from one \\fRobust and Stable Black Box Explanations hospital to another (Bastani, 2018); Caruana et al. (2015) show that experts use interpretable models to identify spurious relationships which do not hold if the underlying data changes\\u2014e.g., if a patient has asthma, he is not likely to die from pneumonia; these are intrinsically distribution shift issues.\",\"854\":\"However, the proprietary nature and increasing complexity of machine learning models poses a severe challenge to understanding these complex black boxes, motivating the need for tools that can explain them in a faithful and interpretable manner.\",\"1444\":\"Model-agnostic interpretability of machine learning.\",\"289\":\"Prior research on interpretable machine learning mainly focused on learning predictive models from scratch which were human understandable.\",\"445\":\": Explaining the predictions of any classifier.\",\"16\":\"Human interpretability has high importance in a wide range of applications such as medicine and business [4, 8], where results from prediction models are generally presented to a human decision maker\\/agent who makes the final decision.\",\"281\":\"Interpretable & Explorable Approximations of Black Box Models Himabindu Lakkaraju Stanford University himalv@cs.stanford.edu Ece Kamar Microsoft Research eckamar@microsoft.com Rich Caruana Microsoft Research rcaruana@microsoft.com Jure Leskovec Stanford University jure@cs.stanford.edu ABSTRACT We propose Black Box Explanations through Transparent Approximations (BETA), a novel model agnostic framework for explaining the behavior of any black-box classifier by simultaneously optimizing for fidelity to the original model and interpretability of the explanation.\",\"1450\":\"Improving reliability of clinical models using prediction calibration.\",\"890\":\"Many approaches have been proposed to directly learn interpretable models (Breiman, 2017; Tibshirani, 1997; Letham et al., 2015; Lakkaraju et al., 2016; Caruana et al., 2015; Kim & Bastani, 2019); however, complex models such as deep neural networks and random forests typically achieve higher accuracy than simpler interpretable models (Ribeiro et al., 2016); thus, it is often desirable to use complex models and then construct post hoc explanations to understand their behavior.\",\"18\":\"As an example, medical diagnosis models [8] may predict a high risk of certain diseases for a patient; a doctor then needs to know the underlying factors to compare with his\\/her domain knowledge, take the correct action, and communicate with the patient.\",\"297\":\"al. [8] proposed an approach which explains individual predictions of any classifier by generating locally interpretable models.\",\"582\":\"These experiments show that mimic models can provide insights into black-box models, and demonstrate the advantages of using outcome information.\",\"1241\":\"\\u201dwhy should i trust you?\\u201d: Explaining the predictions of any classifier.\",\"1049\":\"Black boxes.\",\"620\":\"We use this Lending Club example to discuss an insight gained into the black-box model from inspecting feature interactions in the transparent models.\",\"485\":\"To gain insight into the black-box model, we uncover feature regions where the two models are significantly different (Section 2.3), and ask \\u201cwhat could be happening in the black-box model, that could explain the differences we are seeing between the mimic and outcome models?\\u201d.\",\"605\":\"(2) The black-box model may be deliberately simple for some feature regions.\",\"514\":\"This allows us to ask, \\u201cwhat could be happening in the black-box model, that could explain the differences we are seeing between the mimic and outcome models?\\u201d.\",\"515\":\"In addition, similarities between the mimic and outcome models (e.g., on COMPAS in Section 3.2, the Number of Priors feature is modeled very similarly by the two models) increases confidence that the mimic model is a faithful representation of the black-box model, and that any differences observed on other features are meaningful.\",\"614\":\"While we cannot tell (without further investigation) the definitive reason that explains a particular difference between the mimic and outcome models, this has surfaced ideas about the black-box model and uncovered potentially problematic feature regions that we did not a priori know, but can now proceed to investigate further.\",\"513\":\"Because both the mimic and outcome models are trained with the same model class on the same audit data using the same features, the more faithful the mimic model, and the more accurate the outcome model, the more likely it is that observed differences between the mimic and outcome models stem from differences between the black-box model and ground-truth outcomes.\",\"505\":\"It is critical that both the mimic model and outcome model are trained using the same model class that allows for interpretation and comparison.\",\"536\":\"2.3 Comparing Mimic and Outcome Models In this section, we provide technical details on how to train the mimic and outcome models so that they are comparable.\",\"668\":\"A key advantage of using transparent models to audit black-box models is that we do not need to know in advance what to look for.\",\"311\":\"We also carried out user studies in which we asked human subjects to reason about a black box model\\u2019s behavior using the approximations generated by our approach and other state-of-the-art baselines.\",\"855\":\"Several different kinds of approaches have been proposed to produce interpretable post hoc explanations of black box models.\",\"512\":\"If the black-box model is accurate and generalizes to the audit data, it would predict the ground-truth outcomes in the audit data correctly; the converse is true if the black-box model is not accurate or does not generalize to the audit data.\",\"893\":\"An alternate approach is to provide a global explanation summarizing the black box as a whole (Lakkaraju et al., 2019a; Bastani et al., 2017), typically using an interpretable model.\"},\"relevance\":{\"847\":1,\"286\":0,\"866\":0,\"854\":1,\"1444\":0,\"289\":1,\"445\":0,\"16\":1,\"281\":1,\"1450\":0,\"890\":1,\"18\":1,\"297\":0,\"582\":1,\"1241\":0,\"1049\":0,\"620\":1,\"485\":1,\"605\":0,\"514\":1,\"515\":1,\"614\":0,\"513\":1,\"505\":0,\"536\":0,\"668\":true,\"311\":true,\"855\":true,\"512\":true,\"893\":true}}"}