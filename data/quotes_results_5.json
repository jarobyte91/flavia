{"sentences": "{\"filename\":{\"0\":\"ref 19-Imprtnt.pdf\",\"1\":\"ref 19-Imprtnt.pdf\",\"2\":\"ref 19-Imprtnt.pdf\",\"3\":\"ref 19-Imprtnt.pdf\",\"4\":\"ref 19-Imprtnt.pdf\",\"5\":\"ref 19-Imprtnt.pdf\",\"6\":\"ref 19-Imprtnt.pdf\",\"7\":\"ref 19-Imprtnt.pdf\",\"8\":\"ref 19-Imprtnt.pdf\",\"9\":\"ref 19-Imprtnt.pdf\",\"10\":\"ref 19-Imprtnt.pdf\",\"11\":\"ref 19-Imprtnt.pdf\",\"12\":\"ref 19-Imprtnt.pdf\",\"13\":\"ref 19-Imprtnt.pdf\",\"14\":\"ref 19-Imprtnt.pdf\",\"15\":\"ref 19-Imprtnt.pdf\",\"16\":\"ref 19-Imprtnt.pdf\",\"17\":\"ref 19-Imprtnt.pdf\",\"18\":\"ref 19-Imprtnt.pdf\",\"19\":\"ref 19-Imprtnt.pdf\",\"20\":\"ref 19-Imprtnt.pdf\",\"21\":\"ref 19-Imprtnt.pdf\",\"22\":\"ref 19-Imprtnt.pdf\",\"23\":\"ref 19-Imprtnt.pdf\",\"24\":\"ref 19-Imprtnt.pdf\",\"25\":\"ref 19-Imprtnt.pdf\",\"26\":\"ref 19-Imprtnt.pdf\",\"27\":\"ref 19-Imprtnt.pdf\",\"28\":\"ref 19-Imprtnt.pdf\",\"29\":\"ref 19-Imprtnt.pdf\",\"30\":\"ref 19-Imprtnt.pdf\",\"31\":\"ref 19-Imprtnt.pdf\",\"32\":\"ref 19-Imprtnt.pdf\",\"33\":\"ref 19-Imprtnt.pdf\",\"34\":\"ref 19-Imprtnt.pdf\",\"35\":\"ref 19-Imprtnt.pdf\",\"36\":\"ref 19-Imprtnt.pdf\",\"37\":\"ref 19-Imprtnt.pdf\",\"38\":\"ref 19-Imprtnt.pdf\",\"39\":\"ref 19-Imprtnt.pdf\",\"40\":\"ref 19-Imprtnt.pdf\",\"41\":\"ref 19-Imprtnt.pdf\",\"42\":\"ref 19-Imprtnt.pdf\",\"43\":\"ref 19-Imprtnt.pdf\",\"44\":\"ref 19-Imprtnt.pdf\",\"45\":\"ref 19-Imprtnt.pdf\",\"46\":\"ref 19-Imprtnt.pdf\",\"47\":\"ref 19-Imprtnt.pdf\",\"48\":\"ref 19-Imprtnt.pdf\",\"49\":\"ref 19-Imprtnt.pdf\",\"50\":\"ref 19-Imprtnt.pdf\",\"51\":\"ref 19-Imprtnt.pdf\",\"52\":\"ref 19-Imprtnt.pdf\",\"53\":\"ref 19-Imprtnt.pdf\",\"54\":\"ref 19-Imprtnt.pdf\",\"55\":\"ref 19-Imprtnt.pdf\",\"56\":\"ref 19-Imprtnt.pdf\",\"57\":\"ref 19-Imprtnt.pdf\",\"58\":\"ref 19-Imprtnt.pdf\",\"59\":\"ref 19-Imprtnt.pdf\",\"60\":\"ref 19-Imprtnt.pdf\",\"61\":\"ref 19-Imprtnt.pdf\",\"62\":\"ref 19-Imprtnt.pdf\",\"63\":\"ref 19-Imprtnt.pdf\",\"64\":\"ref 19-Imprtnt.pdf\",\"65\":\"ref 19-Imprtnt.pdf\",\"66\":\"ref 19-Imprtnt.pdf\",\"67\":\"ref 19-Imprtnt.pdf\",\"68\":\"ref 19-Imprtnt.pdf\",\"69\":\"ref 19-Imprtnt.pdf\",\"70\":\"ref 19-Imprtnt.pdf\",\"71\":\"ref 19-Imprtnt.pdf\",\"72\":\"ref 19-Imprtnt.pdf\",\"73\":\"ref 19-Imprtnt.pdf\",\"74\":\"ref 19-Imprtnt.pdf\",\"75\":\"ref 19-Imprtnt.pdf\",\"76\":\"ref 19-Imprtnt.pdf\",\"77\":\"ref 19-Imprtnt.pdf\",\"78\":\"ref 19-Imprtnt.pdf\",\"79\":\"ref 19-Imprtnt.pdf\",\"80\":\"ref 19-Imprtnt.pdf\",\"81\":\"ref 19-Imprtnt.pdf\",\"82\":\"ref 19-Imprtnt.pdf\",\"83\":\"ref 19-Imprtnt.pdf\",\"84\":\"ref 19-Imprtnt.pdf\",\"85\":\"ref 19-Imprtnt.pdf\",\"86\":\"ref 19-Imprtnt.pdf\",\"87\":\"ref 19-Imprtnt.pdf\",\"88\":\"ref 19-Imprtnt.pdf\",\"89\":\"ref 19-Imprtnt.pdf\",\"90\":\"ref 19-Imprtnt.pdf\",\"91\":\"ref 19-Imprtnt.pdf\",\"92\":\"ref 19-Imprtnt.pdf\",\"93\":\"ref 19-Imprtnt.pdf\",\"94\":\"ref 19-Imprtnt.pdf\",\"95\":\"ref 19-Imprtnt.pdf\",\"96\":\"ref 19-Imprtnt.pdf\",\"97\":\"ref 19-Imprtnt.pdf\",\"98\":\"ref 19-Imprtnt.pdf\",\"99\":\"ref 19-Imprtnt.pdf\",\"100\":\"ref 19-Imprtnt.pdf\",\"101\":\"ref 19-Imprtnt.pdf\",\"102\":\"ref 19-Imprtnt.pdf\",\"103\":\"ref 19-Imprtnt.pdf\",\"104\":\"ref 19-Imprtnt.pdf\",\"105\":\"ref 19-Imprtnt.pdf\",\"106\":\"ref 19-Imprtnt.pdf\",\"107\":\"ref 19-Imprtnt.pdf\",\"108\":\"ref 19-Imprtnt.pdf\",\"109\":\"ref 19-Imprtnt.pdf\",\"110\":\"ref 19-Imprtnt.pdf\",\"111\":\"ref 19-Imprtnt.pdf\",\"112\":\"ref 19-Imprtnt.pdf\",\"113\":\"ref 19-Imprtnt.pdf\",\"114\":\"ref 19-Imprtnt.pdf\",\"115\":\"ref 19-Imprtnt.pdf\",\"116\":\"ref 19-Imprtnt.pdf\",\"117\":\"ref 19-Imprtnt.pdf\",\"118\":\"ref 19-Imprtnt.pdf\",\"119\":\"ref 19-Imprtnt.pdf\",\"120\":\"ref 19-Imprtnt.pdf\",\"121\":\"ref 19-Imprtnt.pdf\",\"122\":\"ref 19-Imprtnt.pdf\",\"123\":\"ref 19-Imprtnt.pdf\",\"124\":\"ref 19-Imprtnt.pdf\",\"125\":\"ref 19-Imprtnt.pdf\",\"126\":\"ref 19-Imprtnt.pdf\",\"127\":\"ref 19-Imprtnt.pdf\",\"128\":\"ref 19-Imprtnt.pdf\",\"129\":\"ref 19-Imprtnt.pdf\",\"130\":\"ref 19-Imprtnt.pdf\",\"131\":\"ref 19-Imprtnt.pdf\",\"132\":\"ref 19-Imprtnt.pdf\",\"133\":\"ref 19-Imprtnt.pdf\",\"134\":\"ref 19-Imprtnt.pdf\",\"135\":\"ref 19-Imprtnt.pdf\",\"136\":\"ref 19-Imprtnt.pdf\",\"137\":\"ref 19-Imprtnt.pdf\",\"138\":\"ref 19-Imprtnt.pdf\",\"139\":\"ref 19-Imprtnt.pdf\",\"140\":\"ref 19-Imprtnt.pdf\",\"141\":\"ref 19-Imprtnt.pdf\",\"142\":\"ref 19-Imprtnt.pdf\",\"143\":\"ref 19-Imprtnt.pdf\",\"144\":\"ref 19-Imprtnt.pdf\",\"145\":\"ref 19-Imprtnt.pdf\",\"146\":\"ref 19-Imprtnt.pdf\",\"147\":\"ref 19-Imprtnt.pdf\",\"148\":\"ref 19-Imprtnt.pdf\",\"149\":\"ref 19-Imprtnt.pdf\",\"150\":\"ref 19-Imprtnt.pdf\",\"151\":\"ref 19-Imprtnt.pdf\",\"152\":\"ref 19-Imprtnt.pdf\",\"153\":\"ref 19-Imprtnt.pdf\",\"154\":\"ref 19-Imprtnt.pdf\",\"155\":\"ref 19-Imprtnt.pdf\",\"156\":\"ref 19-Imprtnt.pdf\",\"157\":\"ref 19-Imprtnt.pdf\",\"158\":\"ref 19-Imprtnt.pdf\",\"159\":\"ref 19-Imprtnt.pdf\",\"160\":\"ref 19-Imprtnt.pdf\",\"161\":\"ref 19-Imprtnt.pdf\",\"162\":\"ref 19-Imprtnt.pdf\",\"163\":\"ref 19-Imprtnt.pdf\",\"164\":\"ref 19-Imprtnt.pdf\",\"165\":\"ref 19-Imprtnt.pdf\",\"166\":\"ref 19-Imprtnt.pdf\",\"167\":\"ref 19-Imprtnt.pdf\",\"168\":\"ref 19-Imprtnt.pdf\",\"169\":\"ref 19-Imprtnt.pdf\",\"170\":\"ref 19-Imprtnt.pdf\",\"171\":\"ref 19-Imprtnt.pdf\",\"172\":\"ref 19-Imprtnt.pdf\",\"173\":\"ref 19-Imprtnt.pdf\",\"174\":\"ref 19-Imprtnt.pdf\",\"175\":\"ref 19-Imprtnt.pdf\",\"176\":\"ref 19-Imprtnt.pdf\",\"177\":\"ref 19-Imprtnt.pdf\",\"178\":\"ref 19-Imprtnt.pdf\",\"179\":\"ref 19-Imprtnt.pdf\",\"180\":\"ref 19-Imprtnt.pdf\",\"181\":\"ref 19-Imprtnt.pdf\",\"182\":\"ref 19-Imprtnt.pdf\",\"183\":\"ref 19-Imprtnt.pdf\",\"184\":\"ref 19-Imprtnt.pdf\",\"185\":\"ref 19-Imprtnt.pdf\",\"186\":\"ref 19-Imprtnt.pdf\",\"187\":\"ref 19-Imprtnt.pdf\",\"188\":\"ref 19-Imprtnt.pdf\",\"189\":\"ref 19-Imprtnt.pdf\",\"190\":\"ref 19-Imprtnt.pdf\",\"191\":\"ref 19-Imprtnt.pdf\",\"192\":\"ref 19-Imprtnt.pdf\",\"193\":\"ref 19-Imprtnt.pdf\",\"194\":\"ref 19-Imprtnt.pdf\",\"195\":\"ref 19-Imprtnt.pdf\",\"196\":\"ref 19-Imprtnt.pdf\",\"197\":\"ref 19-Imprtnt.pdf\",\"198\":\"ref 19-Imprtnt.pdf\",\"199\":\"ref 19-Imprtnt.pdf\",\"200\":\"ref 19-Imprtnt.pdf\",\"201\":\"ref 19-Imprtnt.pdf\",\"202\":\"ref 19-Imprtnt.pdf\",\"203\":\"ref 19-Imprtnt.pdf\",\"204\":\"ref 19-Imprtnt.pdf\",\"205\":\"ref 19-Imprtnt.pdf\",\"206\":\"ref 19-Imprtnt.pdf\",\"207\":\"ref 19-Imprtnt.pdf\",\"208\":\"ref 19-Imprtnt.pdf\",\"209\":\"ref 19-Imprtnt.pdf\",\"210\":\"ref 19-Imprtnt.pdf\",\"211\":\"ref 19-Imprtnt.pdf\",\"212\":\"ref 19-Imprtnt.pdf\",\"213\":\"ref 19-Imprtnt.pdf\",\"214\":\"ref 19-Imprtnt.pdf\",\"215\":\"ref 19-Imprtnt.pdf\",\"216\":\"ref 19-Imprtnt.pdf\",\"217\":\"ref 19-Imprtnt.pdf\",\"218\":\"ref 19-Imprtnt.pdf\",\"219\":\"ref 19-Imprtnt.pdf\",\"220\":\"ref 19-Imprtnt.pdf\",\"221\":\"ref 19-Imprtnt.pdf\",\"222\":\"ref 19-Imprtnt.pdf\",\"223\":\"ref 19-Imprtnt.pdf\",\"224\":\"ref 19-Imprtnt.pdf\",\"225\":\"ref 19-Imprtnt.pdf\",\"226\":\"ref 19-Imprtnt.pdf\",\"227\":\"ref 19-Imprtnt.pdf\",\"228\":\"ref 19-Imprtnt.pdf\",\"229\":\"ref 19-Imprtnt.pdf\",\"230\":\"ref 19-Imprtnt.pdf\",\"231\":\"ref 19-Imprtnt.pdf\",\"232\":\"ref 19-Imprtnt.pdf\",\"233\":\"ref 19-Imprtnt.pdf\",\"234\":\"ref 19-Imprtnt.pdf\",\"235\":\"ref 19-Imprtnt.pdf\",\"236\":\"ref 19-Imprtnt.pdf\",\"237\":\"ref 19-Imprtnt.pdf\",\"238\":\"ref 19-Imprtnt.pdf\",\"239\":\"ref 19-Imprtnt.pdf\",\"240\":\"ref 19-Imprtnt.pdf\",\"241\":\"ref 19-Imprtnt.pdf\",\"242\":\"ref 19-Imprtnt.pdf\",\"243\":\"ref 19-Imprtnt.pdf\",\"244\":\"ref 19-Imprtnt.pdf\",\"245\":\"ref 19-Imprtnt.pdf\",\"246\":\"ref 19-Imprtnt.pdf\",\"247\":\"ref 19-Imprtnt.pdf\",\"248\":\"ref 19-Imprtnt.pdf\",\"249\":\"ref 19-Imprtnt.pdf\",\"250\":\"ref 19-Imprtnt.pdf\",\"251\":\"ref 19-Imprtnt.pdf\",\"252\":\"ref 19-Imprtnt.pdf\",\"253\":\"ref 19-Imprtnt.pdf\",\"254\":\"ref 19-Imprtnt.pdf\",\"255\":\"ref 19-Imprtnt.pdf\",\"256\":\"ref 19-Imprtnt.pdf\",\"257\":\"ref 19-Imprtnt.pdf\",\"258\":\"ref 19-Imprtnt.pdf\",\"259\":\"ref 19-Imprtnt.pdf\",\"260\":\"ref 19-Imprtnt.pdf\",\"261\":\"ref 19-Imprtnt.pdf\",\"262\":\"ref 19-Imprtnt.pdf\",\"263\":\"ref 19-Imprtnt.pdf\",\"264\":\"ref 19-Imprtnt.pdf\",\"265\":\"ref 19-Imprtnt.pdf\",\"266\":\"ref 19-Imprtnt.pdf\",\"267\":\"ref 19-Imprtnt.pdf\",\"268\":\"ref 19-Imprtnt.pdf\",\"269\":\"ref 19-Imprtnt.pdf\",\"270\":\"ref 19-Imprtnt.pdf\",\"271\":\"ref 19-Imprtnt.pdf\",\"272\":\"ref 19-Imprtnt.pdf\",\"273\":\"ref 19-Imprtnt.pdf\",\"274\":\"ref 19-Imprtnt.pdf\",\"275\":\"ref 19-Imprtnt.pdf\",\"276\":\"ref 19-Imprtnt.pdf\",\"277\":\"ref 19-Imprtnt.pdf\",\"278\":\"ref 19-Imprtnt.pdf\",\"279\":\"ref 19-Imprtnt.pdf\",\"280\":\"ref 19-Imprtnt.pdf\",\"281\":\"ref 19-Imprtnt.pdf\",\"282\":\"ref 19-Imprtnt.pdf\",\"283\":\"ref 19-Imprtnt.pdf\",\"284\":\"ref 19-Imprtnt.pdf\",\"285\":\"ref 19-Imprtnt.pdf\",\"286\":\"ref 19-Imprtnt.pdf\",\"287\":\"ref 19-Imprtnt.pdf\",\"288\":\"ref 19-Imprtnt.pdf\",\"289\":\"ref 19-Imprtnt.pdf\",\"290\":\"ref 19-Imprtnt.pdf\",\"291\":\"ref 19-Imprtnt.pdf\",\"292\":\"ref 19-Imprtnt.pdf\",\"293\":\"ref 19-Imprtnt.pdf\",\"294\":\"ref 19-Imprtnt.pdf\",\"295\":\"ref 19-Imprtnt.pdf\",\"296\":\"ref 19-Imprtnt.pdf\",\"297\":\"ref 19-Imprtnt.pdf\",\"298\":\"ref 19-Imprtnt.pdf\",\"299\":\"ref 19-Imprtnt.pdf\",\"300\":\"ref 19-Imprtnt.pdf\",\"301\":\"ref 19-Imprtnt.pdf\",\"302\":\"ref 19-Imprtnt.pdf\",\"303\":\"ref 19-Imprtnt.pdf\",\"304\":\"ref 19-Imprtnt.pdf\",\"305\":\"ref 19-Imprtnt.pdf\",\"306\":\"ref 19-Imprtnt.pdf\",\"307\":\"ref 19-Imprtnt.pdf\",\"308\":\"ref 19-Imprtnt.pdf\",\"309\":\"ref 19-Imprtnt.pdf\",\"310\":\"ref 19-Imprtnt.pdf\",\"311\":\"ref 19-Imprtnt.pdf\",\"312\":\"ref 19-Imprtnt.pdf\",\"313\":\"ref 19-Imprtnt.pdf\",\"314\":\"ref 19-Imprtnt.pdf\",\"315\":\"ref 19-Imprtnt.pdf\",\"316\":\"ref 19-Imprtnt.pdf\",\"317\":\"ref 19-Imprtnt.pdf\",\"318\":\"ref 19-Imprtnt.pdf\",\"319\":\"ref 19-Imprtnt.pdf\",\"320\":\"ref 19-Imprtnt.pdf\",\"321\":\"ref 19-Imprtnt.pdf\",\"322\":\"ref 19-Imprtnt.pdf\",\"323\":\"ref 19-Imprtnt.pdf\",\"324\":\"ref 19-Imprtnt.pdf\",\"325\":\"ref 19-Imprtnt.pdf\",\"326\":\"ref 19-Imprtnt.pdf\",\"327\":\"ref 19-Imprtnt.pdf\",\"328\":\"ref 19-Imprtnt.pdf\",\"329\":\"ref 19-Imprtnt.pdf\",\"330\":\"ref 19-Imprtnt.pdf\",\"331\":\"ref 19-Imprtnt.pdf\",\"332\":\"ref 19-Imprtnt.pdf\",\"333\":\"ref 19-Imprtnt.pdf\",\"334\":\"ref 19-Imprtnt.pdf\",\"335\":\"ref 19-Imprtnt.pdf\",\"336\":\"ref 19-Imprtnt.pdf\",\"337\":\"ref 19-Imprtnt.pdf\",\"338\":\"ref 19-Imprtnt.pdf\",\"339\":\"ref 19-Imprtnt.pdf\",\"340\":\"ref 19-Imprtnt.pdf\",\"341\":\"ref 19-Imprtnt.pdf\",\"342\":\"ref 19-Imprtnt.pdf\",\"343\":\"ref 19-Imprtnt.pdf\",\"344\":\"ref 19-Imprtnt.pdf\",\"345\":\"ref 19-Imprtnt.pdf\",\"346\":\"ref 19-Imprtnt.pdf\",\"347\":\"ref 19-Imprtnt.pdf\",\"348\":\"ref 19-Imprtnt.pdf\",\"349\":\"ref 19-Imprtnt.pdf\",\"350\":\"ref 19-Imprtnt.pdf\",\"351\":\"ref 19-Imprtnt.pdf\",\"352\":\"ref 19-Imprtnt.pdf\",\"353\":\"ref 19-Imprtnt.pdf\",\"354\":\"ref 19-Imprtnt.pdf\",\"355\":\"ref 19-Imprtnt.pdf\",\"356\":\"ref 19-Imprtnt.pdf\",\"357\":\"ref 19-Imprtnt.pdf\",\"358\":\"ref 19-Imprtnt.pdf\",\"359\":\"ref 19-Imprtnt.pdf\",\"360\":\"ref 19-Imprtnt.pdf\",\"361\":\"ref 19-Imprtnt.pdf\",\"362\":\"ref 19-Imprtnt.pdf\",\"363\":\"ref 19-Imprtnt.pdf\",\"364\":\"ref 19-Imprtnt.pdf\",\"365\":\"ref 19-Imprtnt.pdf\",\"366\":\"ref 19-Imprtnt.pdf\",\"367\":\"ref 19-Imprtnt.pdf\",\"368\":\"ref 19-Imprtnt.pdf\",\"369\":\"ref 19-Imprtnt.pdf\",\"370\":\"ref 19-Imprtnt.pdf\",\"371\":\"ref 19-Imprtnt.pdf\",\"372\":\"ref 19-Imprtnt.pdf\",\"373\":\"ref 19-Imprtnt.pdf\",\"374\":\"ref 19-Imprtnt.pdf\",\"375\":\"ref 19-Imprtnt.pdf\",\"376\":\"ref 19-Imprtnt.pdf\",\"377\":\"ref 19-Imprtnt.pdf\",\"378\":\"ref 19-Imprtnt.pdf\",\"379\":\"ref 19-Imprtnt.pdf\",\"380\":\"ref 19-Imprtnt.pdf\",\"381\":\"ref 19-Imprtnt.pdf\",\"382\":\"ref 19-Imprtnt.pdf\",\"383\":\"ref 19-Imprtnt.pdf\",\"384\":\"ref 19-Imprtnt.pdf\",\"385\":\"ref 19-Imprtnt.pdf\",\"386\":\"ref 19-Imprtnt.pdf\",\"387\":\"ref 19-Imprtnt.pdf\",\"388\":\"ref 19-Imprtnt.pdf\",\"389\":\"ref 19-Imprtnt.pdf\",\"390\":\"ref 19-Imprtnt.pdf\",\"391\":\"ref 19-Imprtnt.pdf\",\"392\":\"ref 19-Imprtnt.pdf\",\"393\":\"ref 19-Imprtnt.pdf\",\"394\":\"ref 19-Imprtnt.pdf\",\"395\":\"ref 19-Imprtnt.pdf\",\"396\":\"ref 19-Imprtnt.pdf\",\"397\":\"ref 19-Imprtnt.pdf\",\"398\":\"ref 19-Imprtnt.pdf\",\"399\":\"ref 19-Imprtnt.pdf\",\"400\":\"ref 19-Imprtnt.pdf\",\"401\":\"ref 19-Imprtnt.pdf\",\"402\":\"ref 19-Imprtnt.pdf\",\"403\":\"ref 19-Imprtnt.pdf\",\"404\":\"ref 19-Imprtnt.pdf\",\"405\":\"ref 19-Imprtnt.pdf\",\"406\":\"ref 19-Imprtnt.pdf\",\"407\":\"ref 06_imprnt.pdf\",\"408\":\"ref 06_imprnt.pdf\",\"409\":\"ref 06_imprnt.pdf\",\"410\":\"ref 06_imprnt.pdf\",\"411\":\"ref 06_imprnt.pdf\",\"412\":\"ref 06_imprnt.pdf\",\"413\":\"ref 06_imprnt.pdf\",\"414\":\"ref 06_imprnt.pdf\",\"415\":\"ref 06_imprnt.pdf\",\"416\":\"ref 06_imprnt.pdf\",\"417\":\"ref 06_imprnt.pdf\",\"418\":\"ref 06_imprnt.pdf\",\"419\":\"ref 06_imprnt.pdf\",\"420\":\"ref 06_imprnt.pdf\",\"421\":\"ref 06_imprnt.pdf\",\"422\":\"ref 06_imprnt.pdf\",\"423\":\"ref 06_imprnt.pdf\",\"424\":\"ref 06_imprnt.pdf\",\"425\":\"ref 06_imprnt.pdf\",\"426\":\"ref 06_imprnt.pdf\",\"427\":\"ref 06_imprnt.pdf\",\"428\":\"ref 06_imprnt.pdf\",\"429\":\"ref 06_imprnt.pdf\",\"430\":\"ref 06_imprnt.pdf\",\"431\":\"ref 06_imprnt.pdf\",\"432\":\"ref 06_imprnt.pdf\",\"433\":\"ref 06_imprnt.pdf\",\"434\":\"ref 06_imprnt.pdf\",\"435\":\"ref 06_imprnt.pdf\",\"436\":\"ref 06_imprnt.pdf\",\"437\":\"ref 06_imprnt.pdf\",\"438\":\"ref 06_imprnt.pdf\",\"439\":\"ref 06_imprnt.pdf\",\"440\":\"ref 06_imprnt.pdf\",\"441\":\"ref 06_imprnt.pdf\",\"442\":\"ref 06_imprnt.pdf\",\"443\":\"ref 06_imprnt.pdf\",\"444\":\"ref 06_imprnt.pdf\",\"445\":\"ref 06_imprnt.pdf\",\"446\":\"ref 06_imprnt.pdf\",\"447\":\"ref 06_imprnt.pdf\",\"448\":\"ref 06_imprnt.pdf\",\"449\":\"ref 06_imprnt.pdf\",\"450\":\"ref 06_imprnt.pdf\",\"451\":\"ref 06_imprnt.pdf\",\"452\":\"ref 06_imprnt.pdf\",\"453\":\"ref 06_imprnt.pdf\",\"454\":\"ref 06_imprnt.pdf\",\"455\":\"ref 06_imprnt.pdf\",\"456\":\"ref 06_imprnt.pdf\",\"457\":\"ref 06_imprnt.pdf\",\"458\":\"ref 06_imprnt.pdf\",\"459\":\"ref 06_imprnt.pdf\",\"460\":\"ref 06_imprnt.pdf\",\"461\":\"ref 06_imprnt.pdf\",\"462\":\"ref 06_imprnt.pdf\",\"463\":\"ref 06_imprnt.pdf\",\"464\":\"ref 06_imprnt.pdf\",\"465\":\"ref 06_imprnt.pdf\",\"466\":\"ref 06_imprnt.pdf\",\"467\":\"ref 06_imprnt.pdf\",\"468\":\"ref 06_imprnt.pdf\",\"469\":\"ref 06_imprnt.pdf\",\"470\":\"ref 06_imprnt.pdf\",\"471\":\"ref 06_imprnt.pdf\",\"472\":\"ref 06_imprnt.pdf\",\"473\":\"ref 06_imprnt.pdf\",\"474\":\"ref 06_imprnt.pdf\",\"475\":\"ref 06_imprnt.pdf\",\"476\":\"ref 06_imprnt.pdf\",\"477\":\"ref 06_imprnt.pdf\",\"478\":\"ref 06_imprnt.pdf\",\"479\":\"ref 06_imprnt.pdf\",\"480\":\"ref 06_imprnt.pdf\",\"481\":\"ref 06_imprnt.pdf\",\"482\":\"ref 06_imprnt.pdf\",\"483\":\"ref 06_imprnt.pdf\",\"484\":\"ref 06_imprnt.pdf\",\"485\":\"ref 06_imprnt.pdf\",\"486\":\"ref 06_imprnt.pdf\",\"487\":\"ref 06_imprnt.pdf\",\"488\":\"ref 06_imprnt.pdf\",\"489\":\"ref 06_imprnt.pdf\",\"490\":\"ref 06_imprnt.pdf\",\"491\":\"ref 06_imprnt.pdf\",\"492\":\"ref 06_imprnt.pdf\",\"493\":\"ref 06_imprnt.pdf\",\"494\":\"ref 06_imprnt.pdf\",\"495\":\"ref 06_imprnt.pdf\",\"496\":\"ref 06_imprnt.pdf\",\"497\":\"ref 06_imprnt.pdf\",\"498\":\"ref 06_imprnt.pdf\",\"499\":\"ref 06_imprnt.pdf\",\"500\":\"ref 06_imprnt.pdf\",\"501\":\"ref 06_imprnt.pdf\",\"502\":\"ref 06_imprnt.pdf\",\"503\":\"ref 06_imprnt.pdf\",\"504\":\"ref 06_imprnt.pdf\",\"505\":\"ref 06_imprnt.pdf\",\"506\":\"ref 06_imprnt.pdf\",\"507\":\"ref 06_imprnt.pdf\",\"508\":\"ref 06_imprnt.pdf\",\"509\":\"ref 06_imprnt.pdf\",\"510\":\"ref 06_imprnt.pdf\",\"511\":\"ref 06_imprnt.pdf\",\"512\":\"ref 06_imprnt.pdf\",\"513\":\"ref 06_imprnt.pdf\",\"514\":\"ref 06_imprnt.pdf\",\"515\":\"ref 06_imprnt.pdf\",\"516\":\"ref 06_imprnt.pdf\",\"517\":\"ref 06_imprnt.pdf\",\"518\":\"ref 06_imprnt.pdf\",\"519\":\"ref 06_imprnt.pdf\",\"520\":\"ref 06_imprnt.pdf\",\"521\":\"ref 06_imprnt.pdf\",\"522\":\"ref 06_imprnt.pdf\",\"523\":\"ref 06_imprnt.pdf\",\"524\":\"ref 06_imprnt.pdf\",\"525\":\"ref 06_imprnt.pdf\",\"526\":\"ref 06_imprnt.pdf\",\"527\":\"ref 06_imprnt.pdf\",\"528\":\"ref 06_imprnt.pdf\",\"529\":\"ref 06_imprnt.pdf\",\"530\":\"ref 06_imprnt.pdf\",\"531\":\"ref 06_imprnt.pdf\",\"532\":\"ref 06_imprnt.pdf\",\"533\":\"ref 06_imprnt.pdf\",\"534\":\"ref 06_imprnt.pdf\",\"535\":\"ref 06_imprnt.pdf\",\"536\":\"ref 06_imprnt.pdf\",\"537\":\"ref 06_imprnt.pdf\",\"538\":\"ref 06_imprnt.pdf\",\"539\":\"ref 06_imprnt.pdf\",\"540\":\"ref 06_imprnt.pdf\",\"541\":\"ref 06_imprnt.pdf\",\"542\":\"ref 06_imprnt.pdf\",\"543\":\"ref 06_imprnt.pdf\",\"544\":\"ref 06_imprnt.pdf\",\"545\":\"ref 06_imprnt.pdf\",\"546\":\"ref 06_imprnt.pdf\",\"547\":\"ref 06_imprnt.pdf\",\"548\":\"ref 06_imprnt.pdf\",\"549\":\"ref 06_imprnt.pdf\",\"550\":\"ref 06_imprnt.pdf\",\"551\":\"ref 06_imprnt.pdf\",\"552\":\"ref 06_imprnt.pdf\",\"553\":\"ref 06_imprnt.pdf\",\"554\":\"ref 06_imprnt.pdf\",\"555\":\"ref 06_imprnt.pdf\",\"556\":\"ref 06_imprnt.pdf\",\"557\":\"ref 06_imprnt.pdf\",\"558\":\"ref 06_imprnt.pdf\",\"559\":\"ref 06_imprnt.pdf\",\"560\":\"ref 06_imprnt.pdf\",\"561\":\"ref 06_imprnt.pdf\",\"562\":\"ref 06_imprnt.pdf\",\"563\":\"ref 06_imprnt.pdf\",\"564\":\"ref 06_imprnt.pdf\",\"565\":\"ref 06_imprnt.pdf\",\"566\":\"ref 06_imprnt.pdf\",\"567\":\"ref 06_imprnt.pdf\",\"568\":\"ref 06_imprnt.pdf\",\"569\":\"ref 06_imprnt.pdf\",\"570\":\"ref 06_imprnt.pdf\",\"571\":\"ref 06_imprnt.pdf\",\"572\":\"ref 06_imprnt.pdf\",\"573\":\"ref 06_imprnt.pdf\",\"574\":\"ref 06_imprnt.pdf\",\"575\":\"ref 06_imprnt.pdf\",\"576\":\"ref 06_imprnt.pdf\",\"577\":\"ref 06_imprnt.pdf\",\"578\":\"ref 06_imprnt.pdf\",\"579\":\"ref 06_imprnt.pdf\",\"580\":\"ref 06_imprnt.pdf\",\"581\":\"ref 06_imprnt.pdf\",\"582\":\"ref 06_imprnt.pdf\",\"583\":\"ref 06_imprnt.pdf\",\"584\":\"ref 06_imprnt.pdf\",\"585\":\"ref 06_imprnt.pdf\",\"586\":\"ref 06_imprnt.pdf\",\"587\":\"ref 06_imprnt.pdf\",\"588\":\"ref 06_imprnt.pdf\",\"589\":\"ref 06_imprnt.pdf\",\"590\":\"ref 06_imprnt.pdf\",\"591\":\"ref 06_imprnt.pdf\",\"592\":\"ref 06_imprnt.pdf\",\"593\":\"ref 06_imprnt.pdf\",\"594\":\"ref 06_imprnt.pdf\",\"595\":\"ref 06_imprnt.pdf\",\"596\":\"ref 06_imprnt.pdf\",\"597\":\"ref 06_imprnt.pdf\",\"598\":\"ref 06_imprnt.pdf\",\"599\":\"ref 06_imprnt.pdf\",\"600\":\"ref 06_imprnt.pdf\",\"601\":\"ref 06_imprnt.pdf\",\"602\":\"ref 06_imprnt.pdf\",\"603\":\"ref 06_imprnt.pdf\",\"604\":\"ref 06_imprnt.pdf\",\"605\":\"ref 06_imprnt.pdf\",\"606\":\"ref 06_imprnt.pdf\",\"607\":\"ref 06_imprnt.pdf\",\"608\":\"ref 06_imprnt.pdf\",\"609\":\"ref 06_imprnt.pdf\",\"610\":\"ref 06_imprnt.pdf\",\"611\":\"ref 06_imprnt.pdf\",\"612\":\"ref 06_imprnt.pdf\",\"613\":\"ref 06_imprnt.pdf\",\"614\":\"ref 06_imprnt.pdf\",\"615\":\"ref 06_imprnt.pdf\",\"616\":\"ref 06_imprnt.pdf\",\"617\":\"ref 06_imprnt.pdf\",\"618\":\"ref 06_imprnt.pdf\",\"619\":\"ref 06_imprnt.pdf\",\"620\":\"ref 06_imprnt.pdf\",\"621\":\"ref 06_imprnt.pdf\",\"622\":\"ref 06_imprnt.pdf\",\"623\":\"ref 06_imprnt.pdf\",\"624\":\"ref 06_imprnt.pdf\",\"625\":\"ref 06_imprnt.pdf\",\"626\":\"ref 06_imprnt.pdf\",\"627\":\"ref 06_imprnt.pdf\",\"628\":\"ref 06_imprnt.pdf\",\"629\":\"ref 06_imprnt.pdf\",\"630\":\"ref 06_imprnt.pdf\",\"631\":\"ref 06_imprnt.pdf\",\"632\":\"ref 06_imprnt.pdf\",\"633\":\"ref 06_imprnt.pdf\",\"634\":\"ref 06_imprnt.pdf\",\"635\":\"ref 06_imprnt.pdf\",\"636\":\"ref 06_imprnt.pdf\",\"637\":\"ref 06_imprnt.pdf\",\"638\":\"ref 06_imprnt.pdf\",\"639\":\"ref 06_imprnt.pdf\",\"640\":\"ref 06_imprnt.pdf\",\"641\":\"ref 06_imprnt.pdf\",\"642\":\"ref 06_imprnt.pdf\",\"643\":\"ref 06_imprnt.pdf\",\"644\":\"ref 06_imprnt.pdf\",\"645\":\"ref 06_imprnt.pdf\",\"646\":\"ref 06_imprnt.pdf\",\"647\":\"ref 06_imprnt.pdf\",\"648\":\"ref 06_imprnt.pdf\",\"649\":\"ref 06_imprnt.pdf\",\"650\":\"ref 06_imprnt.pdf\",\"651\":\"ref 06_imprnt.pdf\",\"652\":\"ref 06_imprnt.pdf\",\"653\":\"ref 06_imprnt.pdf\",\"654\":\"ref 06_imprnt.pdf\",\"655\":\"ref 06_imprnt.pdf\",\"656\":\"ref 06_imprnt.pdf\",\"657\":\"ref 06_imprnt.pdf\",\"658\":\"ref 06_imprnt.pdf\",\"659\":\"ref 06_imprnt.pdf\",\"660\":\"ref 06_imprnt.pdf\",\"661\":\"ref 06_imprnt.pdf\",\"662\":\"ref 06_imprnt.pdf\",\"663\":\"ref 06_imprnt.pdf\",\"664\":\"ref 06_imprnt.pdf\",\"665\":\"ref 06_imprnt.pdf\",\"666\":\"ref 06_imprnt.pdf\",\"667\":\"ref 06_imprnt.pdf\",\"668\":\"ref 06_imprnt.pdf\",\"669\":\"ref 06_imprnt.pdf\",\"670\":\"ref 06_imprnt.pdf\",\"671\":\"ref 06_imprnt.pdf\",\"672\":\"ref 06_imprnt.pdf\",\"673\":\"ref 06_imprnt.pdf\",\"674\":\"ref 06_imprnt.pdf\",\"675\":\"ref 06_imprnt.pdf\",\"676\":\"ref 06_imprnt.pdf\",\"677\":\"ref 06_imprnt.pdf\",\"678\":\"ref 06_imprnt.pdf\",\"679\":\"ref 06_imprnt.pdf\",\"680\":\"ref 06_imprnt.pdf\",\"681\":\"ref 06_imprnt.pdf\",\"682\":\"ref 06_imprnt.pdf\",\"683\":\"ref 06_imprnt.pdf\",\"684\":\"ref 06_imprnt.pdf\",\"685\":\"ref 06_imprnt.pdf\",\"686\":\"ref 06_imprnt.pdf\",\"687\":\"ref 06_imprnt.pdf\",\"688\":\"ref 06_imprnt.pdf\",\"689\":\"ref 06_imprnt.pdf\",\"690\":\"ref 06_imprnt.pdf\",\"691\":\"ref 06_imprnt.pdf\",\"692\":\"ref 06_imprnt.pdf\",\"693\":\"ref 06_imprnt.pdf\",\"694\":\"ref 06_imprnt.pdf\",\"695\":\"ref 06_imprnt.pdf\",\"696\":\"ref 06_imprnt.pdf\",\"697\":\"ref 06_imprnt.pdf\",\"698\":\"ref 06_imprnt.pdf\",\"699\":\"ref 06_imprnt.pdf\",\"700\":\"ref 06_imprnt.pdf\",\"701\":\"ref 06_imprnt.pdf\",\"702\":\"ref 06_imprnt.pdf\",\"703\":\"ref 06_imprnt.pdf\",\"704\":\"ref 06_imprnt.pdf\",\"705\":\"ref 06_imprnt.pdf\",\"706\":\"ref 06_imprnt.pdf\",\"707\":\"ref 06_imprnt.pdf\",\"708\":\"ref 06_imprnt.pdf\",\"709\":\"ref 06_imprnt.pdf\",\"710\":\"ref 06_imprnt.pdf\",\"711\":\"ref 06_imprnt.pdf\",\"712\":\"ref 06_imprnt.pdf\",\"713\":\"ref 06_imprnt.pdf\",\"714\":\"ref 06_imprnt.pdf\",\"715\":\"ref 06_imprnt.pdf\",\"716\":\"ref 06_imprnt.pdf\",\"717\":\"ref 06_imprnt.pdf\",\"718\":\"ref 06_imprnt.pdf\",\"719\":\"ref 06_imprnt.pdf\",\"720\":\"ref 06_imprnt.pdf\",\"721\":\"ref 06_imprnt.pdf\",\"722\":\"ref 06_imprnt.pdf\",\"723\":\"ref 06_imprnt.pdf\",\"724\":\"ref 06_imprnt.pdf\",\"725\":\"ref 06_imprnt.pdf\",\"726\":\"ref 06_imprnt.pdf\",\"727\":\"ref 06_imprnt.pdf\",\"728\":\"ref 06_imprnt.pdf\",\"729\":\"ref 06_imprnt.pdf\",\"730\":\"ref 06_imprnt.pdf\",\"731\":\"ref 06_imprnt.pdf\",\"732\":\"ref 06_imprnt.pdf\",\"733\":\"ref 06_imprnt.pdf\",\"734\":\"ref 06_imprnt.pdf\",\"735\":\"ref 06_imprnt.pdf\",\"736\":\"ref 06_imprnt.pdf\",\"737\":\"ref 06_imprnt.pdf\",\"738\":\"ref 06_imprnt.pdf\",\"739\":\"ref 06_imprnt.pdf\",\"740\":\"ref 06_imprnt.pdf\",\"741\":\"ref 06_imprnt.pdf\",\"742\":\"ref 06_imprnt.pdf\",\"743\":\"ref 06_imprnt.pdf\",\"744\":\"ref 06_imprnt.pdf\",\"745\":\"ref 06_imprnt.pdf\",\"746\":\"ref 06_imprnt.pdf\",\"747\":\"ref 06_imprnt.pdf\",\"748\":\"ref 06_imprnt.pdf\",\"749\":\"ref 06_imprnt.pdf\",\"750\":\"ref 06_imprnt.pdf\",\"751\":\"ref 06_imprnt.pdf\",\"752\":\"ref 06_imprnt.pdf\",\"753\":\"ref 06_imprnt.pdf\",\"754\":\"ref 06_imprnt.pdf\",\"755\":\"ref 06_imprnt.pdf\",\"756\":\"ref 06_imprnt.pdf\",\"757\":\"ref 06_imprnt.pdf\",\"758\":\"ref 06_imprnt.pdf\",\"759\":\"ref 06_imprnt.pdf\",\"760\":\"ref 06_imprnt.pdf\",\"761\":\"ref 06_imprnt.pdf\",\"762\":\"ref 06_imprnt.pdf\",\"763\":\"ref 06_imprnt.pdf\",\"764\":\"ref 06_imprnt.pdf\",\"765\":\"ref 06_imprnt.pdf\",\"766\":\"ref 06_imprnt.pdf\",\"767\":\"ref 06_imprnt.pdf\",\"768\":\"ref 06_imprnt.pdf\",\"769\":\"ref 06_imprnt.pdf\",\"770\":\"ref 06_imprnt.pdf\",\"771\":\"ref 06_imprnt.pdf\",\"772\":\"ref 06_imprnt.pdf\",\"773\":\"ref 06_imprnt.pdf\",\"774\":\"ref 06_imprnt.pdf\",\"775\":\"ref 06_imprnt.pdf\",\"776\":\"ref 06_imprnt.pdf\",\"777\":\"ref 06_imprnt.pdf\",\"778\":\"ref 06_imprnt.pdf\",\"779\":\"ref 06_imprnt.pdf\",\"780\":\"ref 06_imprnt.pdf\",\"781\":\"ref 06_imprnt.pdf\",\"782\":\"ref 06_imprnt.pdf\",\"783\":\"ref 06_imprnt.pdf\",\"784\":\"ref 06_imprnt.pdf\",\"785\":\"ref 06_imprnt.pdf\",\"786\":\"ref 06_imprnt.pdf\",\"787\":\"ref 06_imprnt.pdf\",\"788\":\"ref 06_imprnt.pdf\",\"789\":\"ref 06_imprnt.pdf\",\"790\":\"ref 06_imprnt.pdf\",\"791\":\"ref 06_imprnt.pdf\",\"792\":\"ref 06_imprnt.pdf\",\"793\":\"ref 06_imprnt.pdf\",\"794\":\"ref 06_imprnt.pdf\",\"795\":\"ref 06_imprnt.pdf\",\"796\":\"ref 06_imprnt.pdf\",\"797\":\"ref 06_imprnt.pdf\",\"798\":\"ref 06_imprnt.pdf\",\"799\":\"ref 06_imprnt.pdf\",\"800\":\"ref 06_imprnt.pdf\",\"801\":\"ref 06_imprnt.pdf\",\"802\":\"ref 06_imprnt.pdf\",\"803\":\"ref 06_imprnt.pdf\",\"804\":\"ref 06_imprnt.pdf\",\"805\":\"ref 06_imprnt.pdf\",\"806\":\"ref 06_imprnt.pdf\",\"807\":\"ref 06_imprnt.pdf\",\"808\":\"ref 06_imprnt.pdf\",\"809\":\"ref 06_imprnt.pdf\",\"810\":\"ref 06_imprnt.pdf\",\"811\":\"ref 06_imprnt.pdf\",\"812\":\"ref 06_imprnt.pdf\",\"813\":\"ref 06_imprnt.pdf\",\"814\":\"ref 06_imprnt.pdf\",\"815\":\"ref 06_imprnt.pdf\",\"816\":\"ref 06_imprnt.pdf\",\"817\":\"ref 06_imprnt.pdf\",\"818\":\"ref 06_imprnt.pdf\",\"819\":\"ref 06_imprnt.pdf\",\"820\":\"ref 06_imprnt.pdf\"},\"sentence\":{\"0\":0,\"1\":1,\"2\":2,\"3\":3,\"4\":4,\"5\":5,\"6\":6,\"7\":7,\"8\":8,\"9\":9,\"10\":10,\"11\":11,\"12\":12,\"13\":13,\"14\":14,\"15\":15,\"16\":16,\"17\":17,\"18\":18,\"19\":19,\"20\":20,\"21\":21,\"22\":22,\"23\":23,\"24\":24,\"25\":25,\"26\":26,\"27\":27,\"28\":28,\"29\":29,\"30\":30,\"31\":31,\"32\":32,\"33\":33,\"34\":34,\"35\":35,\"36\":36,\"37\":37,\"38\":38,\"39\":39,\"40\":40,\"41\":41,\"42\":42,\"43\":43,\"44\":44,\"45\":45,\"46\":46,\"47\":47,\"48\":48,\"49\":49,\"50\":50,\"51\":51,\"52\":52,\"53\":53,\"54\":54,\"55\":55,\"56\":56,\"57\":57,\"58\":58,\"59\":59,\"60\":60,\"61\":61,\"62\":62,\"63\":63,\"64\":64,\"65\":65,\"66\":66,\"67\":67,\"68\":68,\"69\":69,\"70\":70,\"71\":71,\"72\":72,\"73\":73,\"74\":74,\"75\":75,\"76\":76,\"77\":77,\"78\":78,\"79\":79,\"80\":80,\"81\":81,\"82\":82,\"83\":83,\"84\":84,\"85\":85,\"86\":86,\"87\":87,\"88\":88,\"89\":89,\"90\":90,\"91\":91,\"92\":92,\"93\":93,\"94\":94,\"95\":95,\"96\":96,\"97\":97,\"98\":98,\"99\":99,\"100\":100,\"101\":101,\"102\":102,\"103\":103,\"104\":104,\"105\":105,\"106\":106,\"107\":107,\"108\":108,\"109\":109,\"110\":110,\"111\":111,\"112\":112,\"113\":113,\"114\":114,\"115\":115,\"116\":116,\"117\":117,\"118\":118,\"119\":119,\"120\":120,\"121\":121,\"122\":122,\"123\":123,\"124\":124,\"125\":125,\"126\":126,\"127\":127,\"128\":128,\"129\":129,\"130\":130,\"131\":131,\"132\":132,\"133\":133,\"134\":134,\"135\":135,\"136\":136,\"137\":137,\"138\":138,\"139\":139,\"140\":140,\"141\":141,\"142\":142,\"143\":143,\"144\":144,\"145\":145,\"146\":146,\"147\":147,\"148\":148,\"149\":149,\"150\":150,\"151\":151,\"152\":152,\"153\":153,\"154\":154,\"155\":155,\"156\":156,\"157\":157,\"158\":158,\"159\":159,\"160\":160,\"161\":161,\"162\":162,\"163\":163,\"164\":164,\"165\":165,\"166\":166,\"167\":167,\"168\":168,\"169\":169,\"170\":170,\"171\":171,\"172\":172,\"173\":173,\"174\":174,\"175\":175,\"176\":176,\"177\":177,\"178\":178,\"179\":179,\"180\":180,\"181\":181,\"182\":182,\"183\":183,\"184\":184,\"185\":185,\"186\":186,\"187\":187,\"188\":188,\"189\":189,\"190\":190,\"191\":191,\"192\":192,\"193\":193,\"194\":194,\"195\":195,\"196\":196,\"197\":197,\"198\":198,\"199\":199,\"200\":200,\"201\":201,\"202\":202,\"203\":203,\"204\":204,\"205\":205,\"206\":206,\"207\":207,\"208\":208,\"209\":209,\"210\":210,\"211\":211,\"212\":212,\"213\":213,\"214\":214,\"215\":215,\"216\":216,\"217\":217,\"218\":218,\"219\":219,\"220\":220,\"221\":221,\"222\":222,\"223\":223,\"224\":224,\"225\":225,\"226\":226,\"227\":227,\"228\":228,\"229\":229,\"230\":230,\"231\":231,\"232\":232,\"233\":233,\"234\":234,\"235\":235,\"236\":236,\"237\":237,\"238\":238,\"239\":239,\"240\":240,\"241\":241,\"242\":242,\"243\":243,\"244\":244,\"245\":245,\"246\":246,\"247\":247,\"248\":248,\"249\":249,\"250\":250,\"251\":251,\"252\":252,\"253\":253,\"254\":254,\"255\":255,\"256\":256,\"257\":257,\"258\":258,\"259\":259,\"260\":260,\"261\":261,\"262\":262,\"263\":263,\"264\":264,\"265\":265,\"266\":266,\"267\":267,\"268\":268,\"269\":269,\"270\":270,\"271\":271,\"272\":272,\"273\":273,\"274\":274,\"275\":275,\"276\":276,\"277\":277,\"278\":278,\"279\":279,\"280\":280,\"281\":281,\"282\":282,\"283\":283,\"284\":284,\"285\":285,\"286\":286,\"287\":287,\"288\":288,\"289\":289,\"290\":290,\"291\":291,\"292\":292,\"293\":293,\"294\":294,\"295\":295,\"296\":296,\"297\":297,\"298\":298,\"299\":299,\"300\":300,\"301\":301,\"302\":302,\"303\":303,\"304\":304,\"305\":305,\"306\":306,\"307\":307,\"308\":308,\"309\":309,\"310\":310,\"311\":311,\"312\":312,\"313\":313,\"314\":314,\"315\":315,\"316\":316,\"317\":317,\"318\":318,\"319\":319,\"320\":320,\"321\":321,\"322\":322,\"323\":323,\"324\":324,\"325\":325,\"326\":326,\"327\":327,\"328\":328,\"329\":329,\"330\":330,\"331\":331,\"332\":332,\"333\":333,\"334\":334,\"335\":335,\"336\":336,\"337\":337,\"338\":338,\"339\":339,\"340\":340,\"341\":341,\"342\":342,\"343\":343,\"344\":344,\"345\":345,\"346\":346,\"347\":347,\"348\":348,\"349\":349,\"350\":350,\"351\":351,\"352\":352,\"353\":353,\"354\":354,\"355\":355,\"356\":356,\"357\":357,\"358\":358,\"359\":359,\"360\":360,\"361\":361,\"362\":362,\"363\":363,\"364\":364,\"365\":365,\"366\":366,\"367\":367,\"368\":368,\"369\":369,\"370\":370,\"371\":371,\"372\":372,\"373\":373,\"374\":374,\"375\":375,\"376\":376,\"377\":377,\"378\":378,\"379\":379,\"380\":380,\"381\":381,\"382\":382,\"383\":383,\"384\":384,\"385\":385,\"386\":386,\"387\":387,\"388\":388,\"389\":389,\"390\":390,\"391\":391,\"392\":392,\"393\":393,\"394\":394,\"395\":395,\"396\":396,\"397\":397,\"398\":398,\"399\":399,\"400\":400,\"401\":401,\"402\":402,\"403\":403,\"404\":404,\"405\":405,\"406\":406,\"407\":0,\"408\":1,\"409\":2,\"410\":3,\"411\":4,\"412\":5,\"413\":6,\"414\":7,\"415\":8,\"416\":9,\"417\":10,\"418\":11,\"419\":12,\"420\":13,\"421\":14,\"422\":15,\"423\":16,\"424\":17,\"425\":18,\"426\":19,\"427\":20,\"428\":21,\"429\":22,\"430\":23,\"431\":24,\"432\":25,\"433\":26,\"434\":27,\"435\":28,\"436\":29,\"437\":30,\"438\":31,\"439\":32,\"440\":33,\"441\":34,\"442\":35,\"443\":36,\"444\":37,\"445\":38,\"446\":39,\"447\":40,\"448\":41,\"449\":42,\"450\":43,\"451\":44,\"452\":45,\"453\":46,\"454\":47,\"455\":48,\"456\":49,\"457\":50,\"458\":51,\"459\":52,\"460\":53,\"461\":54,\"462\":55,\"463\":56,\"464\":57,\"465\":58,\"466\":59,\"467\":60,\"468\":61,\"469\":62,\"470\":63,\"471\":64,\"472\":65,\"473\":66,\"474\":67,\"475\":68,\"476\":69,\"477\":70,\"478\":71,\"479\":72,\"480\":73,\"481\":74,\"482\":75,\"483\":76,\"484\":77,\"485\":78,\"486\":79,\"487\":80,\"488\":81,\"489\":82,\"490\":83,\"491\":84,\"492\":85,\"493\":86,\"494\":87,\"495\":88,\"496\":89,\"497\":90,\"498\":91,\"499\":92,\"500\":93,\"501\":94,\"502\":95,\"503\":96,\"504\":97,\"505\":98,\"506\":99,\"507\":100,\"508\":101,\"509\":102,\"510\":103,\"511\":104,\"512\":105,\"513\":106,\"514\":107,\"515\":108,\"516\":109,\"517\":110,\"518\":111,\"519\":112,\"520\":113,\"521\":114,\"522\":115,\"523\":116,\"524\":117,\"525\":118,\"526\":119,\"527\":120,\"528\":121,\"529\":122,\"530\":123,\"531\":124,\"532\":125,\"533\":126,\"534\":127,\"535\":128,\"536\":129,\"537\":130,\"538\":131,\"539\":132,\"540\":133,\"541\":134,\"542\":135,\"543\":136,\"544\":137,\"545\":138,\"546\":139,\"547\":140,\"548\":141,\"549\":142,\"550\":143,\"551\":144,\"552\":145,\"553\":146,\"554\":147,\"555\":148,\"556\":149,\"557\":150,\"558\":151,\"559\":152,\"560\":153,\"561\":154,\"562\":155,\"563\":156,\"564\":157,\"565\":158,\"566\":159,\"567\":160,\"568\":161,\"569\":162,\"570\":163,\"571\":164,\"572\":165,\"573\":166,\"574\":167,\"575\":168,\"576\":169,\"577\":170,\"578\":171,\"579\":172,\"580\":173,\"581\":174,\"582\":175,\"583\":176,\"584\":177,\"585\":178,\"586\":179,\"587\":180,\"588\":181,\"589\":182,\"590\":183,\"591\":184,\"592\":185,\"593\":186,\"594\":187,\"595\":188,\"596\":189,\"597\":190,\"598\":191,\"599\":192,\"600\":193,\"601\":194,\"602\":195,\"603\":196,\"604\":197,\"605\":198,\"606\":199,\"607\":200,\"608\":201,\"609\":202,\"610\":203,\"611\":204,\"612\":205,\"613\":206,\"614\":207,\"615\":208,\"616\":209,\"617\":210,\"618\":211,\"619\":212,\"620\":213,\"621\":214,\"622\":215,\"623\":216,\"624\":217,\"625\":218,\"626\":219,\"627\":220,\"628\":221,\"629\":222,\"630\":223,\"631\":224,\"632\":225,\"633\":226,\"634\":227,\"635\":228,\"636\":229,\"637\":230,\"638\":231,\"639\":232,\"640\":233,\"641\":234,\"642\":235,\"643\":236,\"644\":237,\"645\":238,\"646\":239,\"647\":240,\"648\":241,\"649\":242,\"650\":243,\"651\":244,\"652\":245,\"653\":246,\"654\":247,\"655\":248,\"656\":249,\"657\":250,\"658\":251,\"659\":252,\"660\":253,\"661\":254,\"662\":255,\"663\":256,\"664\":257,\"665\":258,\"666\":259,\"667\":260,\"668\":261,\"669\":262,\"670\":263,\"671\":264,\"672\":265,\"673\":266,\"674\":267,\"675\":268,\"676\":269,\"677\":270,\"678\":271,\"679\":272,\"680\":273,\"681\":274,\"682\":275,\"683\":276,\"684\":277,\"685\":278,\"686\":279,\"687\":280,\"688\":281,\"689\":282,\"690\":283,\"691\":284,\"692\":285,\"693\":286,\"694\":287,\"695\":288,\"696\":289,\"697\":290,\"698\":291,\"699\":292,\"700\":293,\"701\":294,\"702\":295,\"703\":296,\"704\":297,\"705\":298,\"706\":299,\"707\":300,\"708\":301,\"709\":302,\"710\":303,\"711\":304,\"712\":305,\"713\":306,\"714\":307,\"715\":308,\"716\":309,\"717\":310,\"718\":311,\"719\":312,\"720\":313,\"721\":314,\"722\":315,\"723\":316,\"724\":317,\"725\":318,\"726\":319,\"727\":320,\"728\":321,\"729\":322,\"730\":323,\"731\":324,\"732\":325,\"733\":326,\"734\":327,\"735\":328,\"736\":329,\"737\":330,\"738\":331,\"739\":332,\"740\":333,\"741\":334,\"742\":335,\"743\":336,\"744\":337,\"745\":338,\"746\":339,\"747\":340,\"748\":341,\"749\":342,\"750\":343,\"751\":344,\"752\":345,\"753\":346,\"754\":347,\"755\":348,\"756\":349,\"757\":350,\"758\":351,\"759\":352,\"760\":353,\"761\":354,\"762\":355,\"763\":356,\"764\":357,\"765\":358,\"766\":359,\"767\":360,\"768\":361,\"769\":362,\"770\":363,\"771\":364,\"772\":365,\"773\":366,\"774\":367,\"775\":368,\"776\":369,\"777\":370,\"778\":371,\"779\":372,\"780\":373,\"781\":374,\"782\":375,\"783\":376,\"784\":377,\"785\":378,\"786\":379,\"787\":380,\"788\":381,\"789\":382,\"790\":383,\"791\":384,\"792\":385,\"793\":386,\"794\":387,\"795\":388,\"796\":389,\"797\":390,\"798\":391,\"799\":392,\"800\":393,\"801\":394,\"802\":395,\"803\":396,\"804\":397,\"805\":398,\"806\":399,\"807\":400,\"808\":401,\"809\":402,\"810\":403,\"811\":404,\"812\":405,\"813\":406,\"814\":407,\"815\":408,\"816\":409,\"817\":410,\"818\":411,\"819\":412,\"820\":413},\"text\":{\"0\":\"Sequence to Sequence \\u2013 Video to Text Subhashini Venugopalan1 Marcus Rohrbach2,4 Jeff Donahue2 Raymond Mooney1 Trevor Darrell2 Kate Saenko3 1 University of Texas at Austin 2 University of California, Berkeley 3 University of Massachusetts, Lowell 4 International Computer Science Institute, Berkeley Abstract Real-world videos often have complex dynamics; and methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length.\",\"1\":\"To approach this problem, we propose a novel end-to-end sequence-to-sequence model to generate captions for videos.\",\"2\":\"For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated stateof-the-art performance in image caption generation.\",\"3\":\"Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip.\",\"4\":\"Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model.\",\"5\":\"We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).\",\"6\":\"1.\",\"7\":\"Introduction Describing visual content with natural language text has recently received increased interest, especially describing images with a single sentence [8, 5, 16, 18, 20, 23, 29, 40].\",\"8\":\"Video description has so far seen less attention despite its important applications in human-robot interaction, video indexing, and describing movies for the blind.\",\"9\":\"While image description handles a variable length output sequence of words, video description also has to handle a variable length input sequence of frames.\",\"10\":\"Related approaches to video description have resolved variable length input by holistic video representations [29, 28, 11], pooling over frames [39], or sub-sampling on a fixed number of input frames [43].\",\"11\":\"In contrast, in this work we propose a sequence to sequence model which is trained end-to-end and is able to learn arbitrary temporal structure in the input sequence.\",\"12\":\"Our model is sequence to sequence in a sense that it reads in frames CNN - Action pretrained CNN - Object pretrained Flow images Raw Frames A man is cutting a bottle <eos> LSTMs CNN Outputs Our LSTM network is connected to a CNN for RGB frames or a CNN for optical flow images.\",\"13\":\"Figure 1.\",\"14\":\"Our S2VT approach performs video description using a sequence to sequence model.\",\"15\":\"It incorporates a stacked LSTM which first reads the sequence of frames and then generates a sequence of words.\",\"16\":\"The input visual sequence to the model is comprised of RGB and\\/or optical flow CNN outputs.\",\"17\":\"sequentially and outputs words sequentially.\",\"18\":\"The problem of generating descriptions in open domain videos is difficult not just due to the diverse set of objects, scenes, actions, and their attributes, but also because it is hard to determine the salient content and describe the event appropriately in context.\",\"19\":\"To learn what is worth describing, our model learns from video clips and paired sentences that describe the depicted events in natural language.\",\"20\":\"We use Long Short Term Memory (LSTM) networks [12], a type of recurrent neural network (RNN) that has achieved great success on similar sequence-to-sequence tasks such as speech recognition [10] and machine translation [34].\",\"21\":\"Due to the inherent sequential nature of videos and language, LSTMs are well-suited for generating descriptions of events in videos.\",\"22\":\"The main contribution of this work is to propose a novel model, S2VT, which learns to directly map a sequence of 1 arXiv:1505.00487v3 [cs.CV] 19 Oct 2015 \\fframes to a sequence of words.\",\"23\":\"Figure 1 depicts our model.\",\"24\":\"A stacked LSTM first encodes the frames one by one, taking as input the output of a Convolutional Neural Network (CNN) applied to each input frame\\u2019s intensity values.\",\"25\":\"Once all frames are read, the model generates a sentence word by word.\",\"26\":\"The encoding and decoding of the frame and word representations are learned jointly from a parallel corpus.\",\"27\":\"To model the temporal aspects of activities typically shown in videos, we also compute the optical flow [2] between pairs of consecutive frames.\",\"28\":\"The flow images are also passed through a CNN and provided as input to the LSTM.\",\"29\":\"Flow CNN models have been shown to be beneficial for activity recognition [31, 8].\",\"30\":\"To our knowledge, this is the first approach to video description that uses a general sequence to sequence model.\",\"31\":\"This allows our model to (a) handle a variable number of input frames, (b) learn and use the temporal structure of the video and (c) learn a language model to generate natural, grammatical sentences.\",\"32\":\"Our model is learned jointly and end-to-end, incorporating both intensity and optical flow inputs, and does not require an explicit attention model.\",\"33\":\"We demonstrate that S2VT achieves state-of-the-art performance on three diverse datasets, a standard YouTube corpus (MSVD) [3] and the M-VAD [37] and MPII Movie Description [28] datasets.\",\"34\":\"Our implementation (based on the Caffe [15] deep learning framework) is available on github.\",\"35\":\"https:\\/\\/github.com\\/vsubhashini\\/caffe\\/ tree\\/recurrent\\/examples\\/s2vt.\",\"36\":\"2.\",\"37\":\"Related Work Early work on video captioning considered tagging videos with metadata [1] and clustering captions and videos [14, 25, 42] for retrieval tasks.\",\"38\":\"Several previous methods for generating sentence descriptions [11, 19, 36] used a two stage pipeline that first identifies the semantic content (subject, verb, object) and then generates a sentence based on a template.\",\"39\":\"This typically involved training individual classifiers to identify candidate objects, actions and scenes.\",\"40\":\"They then use a probabilistic graphical model to combine the visual confidences with a language model in order to estimate the most likely content (subject, verb, object, scene) in the video, which is then used to generate a sentence.\",\"41\":\"While this simplified the problem by detaching content generation and surface realization, it requires selecting a set of relevant objects and actions to recognize.\",\"42\":\"Moreover, a template-based approach to sentence generation is insufficient to model the richness of language used in human descriptions \\u2013 e.g., which attributes to use and how to combine them effectively to generate a good description.\",\"43\":\"In contrast, our approach avoids the separation of content identification and sentence generation by learning to directly map videos to full human-provided sentences, learning a language model simultaneously conditioned on visual features.\",\"44\":\"Our models take inspiration from the image caption generation models in [8, 40].\",\"45\":\"Their first step is to generate a fixed length vector representation of an image by extracting features from a CNN.\",\"46\":\"The next step learns to decode this vector into a sequence of words composing the description of the image.\",\"47\":\"While any RNN can be used in principle to decode the sequence, the resulting long-term dependencies can lead to inferior performance.\",\"48\":\"To mitigate this issue, LSTM models have been exploited as sequence decoders, as they are more suited to learning long-range dependencies.\",\"49\":\"In addition, since we are using variable-length video as input, we use LSTMs as sequence to sequence transducers, following the language translation models of [34].\",\"50\":\"In [39], LSTMs are used to generate video descriptions by pooling the representations of individual frames.\",\"51\":\"Their technique extracts CNN features for frames in the video and then mean-pools the results to get a single feature vector representing the entire video.\",\"52\":\"They then use an LSTM as a sequence decoder to generate a description based on this vector.\",\"53\":\"A major shortcoming of this approach is that this representation completely ignores the ordering of the video frames and fails to exploit any temporal information.\",\"54\":\"The approach in [8] also generates video descriptions using an LSTM; however, they employ a version of the two-step approach that uses CRFs to obtain semantic tuples of activity, object, tool, and locatation and then use an LSTM to translate this tuple into a sentence.\",\"55\":\"Moreover, the model in [8] is applied to the limited domain of cooking videos while ours is aimed at generating descriptions for videos \\u201cin the wild\\u201d.\",\"56\":\"Contemporaneous with our work, the approach in [43] also addresses the limitations of [39] in two ways.\",\"57\":\"First, they employ a 3-D convnet model that incorporates spatiotemporal motion features.\",\"58\":\"To obtain the features, they assume videos are of fixed volume (width, height, time).\",\"59\":\"They extract dense trajectory features (HoG, HoF, MBH) [41] over non-overlapping cuboids and concatenate these to form the input.\",\"60\":\"The 3-D convnet is pre-trained on video datasets for action recognition.\",\"61\":\"Second, they include an attention mechanism that learns to weight the frame features nonuniformly conditioned on the previous word input(s) rather than uniformly weighting features from all frames as in [39].\",\"62\":\"The 3-D convnet alone provides limited performance improvement, but in conjunction with the attention model it notably improves performance.\",\"63\":\"We propose a simpler approach to using temporal information by using an LSTM to encode the sequence of video frames into a distributed vector representation that is sufficient to generate a sentential description.\",\"64\":\"Therefore, our direct sequence to sequence model does not require an explicit attention mechanism.\",\"65\":\"Another recent project [33] uses LSTMs to predict the future frame sequence from an encoding of the previous frames.\",\"66\":\"Their model is more similar to the language translation model in [34], which uses one LSTM to encode the \\finput text into a fixed representation, and another LSTM to decode it into a different language.\",\"67\":\"In contrast, we employ a single LSTM that learns both encoding and decoding based on the inputs it is provided.\",\"68\":\"This allows the LSTM to share weights between encoding and decoding.\",\"69\":\"Other related work includes [24, 8], which uses LSTMs for activity classification, predicting an activity class for the representation of each image\\/flow frame.\",\"70\":\"In contrast, our model generates captions after encoding the complete sequence of optical flow images.\",\"71\":\"Specifically, our final model is an ensemble of the sequence to sequence models trained on raw images and optical flow images.\",\"72\":\"3.\",\"73\":\"Approach We propose a sequence to sequence model for video description, where the input is the sequence of video frames (x1, .\",\"74\":\".\",\"75\":\".\",\"76\":\", xn), and the output is the sequence of words (y1, .\",\"77\":\".\",\"78\":\".\",\"79\":\", ym).\",\"80\":\"Naturally, both the input and output are of variable, potentially different, lengths.\",\"81\":\"In our case, there are typically many more frames than words.\",\"82\":\"In our model, we estimate the conditional probability of an output sequence (y1, .\",\"83\":\".\",\"84\":\".\",\"85\":\", ym) given an input sequence (x1, .\",\"86\":\".\",\"87\":\".\",\"88\":\", xn) i.e. p(y1, .\",\"89\":\".\",\"90\":\".\",\"91\":\", ym|x1, .\",\"92\":\".\",\"93\":\".\",\"94\":\", xn) (1) This problem is analogous to machine translation between natural languages, where a sequence of words in the input language is translated to a sequence of words in the output language.\",\"95\":\"Recently, [6, 34] have shown how to effectively attack this sequence to sequence problem with an LSTM Recurrent Neural Network (RNN).\",\"96\":\"We extend this paradigm to inputs comprised of sequences of video frames, significantly simplifying prior RNN-based methods for video description.\",\"97\":\"In the following, we describe our model and architecture in detail, as well as our input and output representation for video and sentences.\",\"98\":\"3.1.\",\"99\":\"LSTMs for sequence modeling The main idea to handle variable-length input and output is to first encode the input sequence of frames, one at a time, representing the video using a latent vector representation, and then decode from that representation to a sentence, one word at a time.\",\"100\":\"Let us first recall the Long Short Term Memory RNN (LSTM), originally proposed in [12].\",\"101\":\"Relying on the LSTM unit proposed in [44], for an input xt at time step t, the LSTM computes a hidden\\/control state ht and a memory cell state ct which is an encoding of everything the cell has observed until time t: it = \\u03c3(Wxixt + Whiht\\u22121 + bi) ft = \\u03c3(Wxf xt + Whf ht\\u22121 + bf ) ot = \\u03c3(Wxoxt + Whoht\\u22121 + bo) gt = \\u03c6(Wxgxt + Whght\\u22121 + bg) ct = ft ct\\u22121 + it gt ht = ot \\u03c6(ct) (2) where \\u03c3 is the sigmoidal non-linearity, \\u03c6 is the hyperbolic tangent non-linearity, represents the element-wise product with the gate value, and the weight matrices denoted by Wij and biases bj are the trained parameters.\",\"102\":\"Thus, in the encoding phase, given an input sequence X (x1, .\",\"103\":\".\",\"104\":\".\",\"105\":\", xn), the LSTM computes a sequence of hidden states (h1, .\",\"106\":\".\",\"107\":\".\",\"108\":\", hn).\",\"109\":\"During decoding it defines a distribution over the output sequence Y (y1, .\",\"110\":\".\",\"111\":\".\",\"112\":\", ym) given the input sequence X as p(Y |X) is p(y1, .\",\"113\":\".\",\"114\":\".\",\"115\":\", ym|x1, .\",\"116\":\".\",\"117\":\".\",\"118\":\", xn) = m Y t=1 p(yt|hn+t\\u22121, yt\\u22121) (3) where the distribution of p(yt|hn+t) is given by a softmax over all of the words in the vocabulary (see Equation 5).\",\"119\":\"Note that hn+t is obtained from hn+t\\u22121, yt\\u22121 based on the recursion in Equation 2. 3.2.\",\"120\":\"Sequence to sequence video to text Our approach, S2VT, is depicted in Figure 2.\",\"121\":\"While [6, 34] first encode the input sequence to a fixed length vector using one LSTM and then use another LSTM to map the vector to a sequence of outputs, we rely on a single LSTM for both the encoding and decoding stage.\",\"122\":\"This allows parameter sharing between the encoding and decoding stage.\",\"123\":\"Our model uses a stack of two LSTMs with 1000 hidden units each.\",\"124\":\"Figure 2 shows the LSTM stack unrolled over time.\",\"125\":\"When two LSTMs are stacked together, as in our case, the hidden representation (ht) from the first LSTM layer (colored red) is provided as the input (xt) to the second LSTM (colored green).\",\"126\":\"The top LSTM layer in our architecture is used to model the visual frame sequence, and the next layer is used to model the output word sequence.\",\"127\":\"Training and Inference In the first several time steps, the top LSTM layer (colored red in Figure 2) receives a sequence of frames and encodes them while the second LSTM layer receives the hidden representation (ht) and concatenates it with null padded input words (zeros), which it then encodes.\",\"128\":\"There is no loss during this stage when the LSTMs are encoding.\",\"129\":\"After all the frames in the video clip are exhausted, the second LSTM layer is fed the beginning-ofsentence (<BOS>) tag, which prompts it to start decoding its current hidden representation into a sequence of words.\",\"130\":\"While training in the decoding stage, the model maximizes for the log-likelihood of the predicted output sentence given the hidden representation of the visual frame sequence, and the previous words it has seen.\",\"131\":\"From Equation 3 for a model with parameters \\u03b8 and output sequence Y = (y1, .\",\"132\":\".\",\"133\":\".\",\"134\":\", ym), this is formulated as: \\u03b8\\u2217 = argmax \\u03b8 m X t=1 log p(yt|hn+t\\u22121, yt\\u22121; \\u03b8) (4) This log-likelihood is optimized over the entire training dataset using stochastic gradient descent.\",\"135\":\"The loss is computed only when the LSTM is learning to decode.\",\"136\":\"Since this \\fLSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM LSTM time <pad> <pad> <pad> <BOS> man LSTM LSTM LSTM LSTM LSTM LSTM LSTM is talking <EOS> <pad> <pad> <pad> <pad> <pad> <pad> A Encoding stage Decoding stage Figure 2.\",\"137\":\"We propose a stack of two LSTMs that learn a representation of a sequence of frames in order to decode it into a sentence that describes the event in the video.\",\"138\":\"The top LSTM layer (colored red) models visual feature inputs.\",\"139\":\"The second LSTM layer (colored green) models language given the text input and the hidden representation of the video sequence.\",\"140\":\"We use <BOS> to indicate begin-of-sentence and <EOS> for the end-of-sentence tag.\",\"141\":\"Zeros are used as a <pad> when there is no input at the time step.\",\"142\":\"loss is propagated back in time, the LSTM learns to generate an appropriate hidden state representation (hn) of the input sequence.\",\"143\":\"The output (zt) of the second LSTM layer is used to obtain the emitted word (y).\",\"144\":\"We apply a softmax function to get the probability distribution over the words y0 in the vocabulary V : p(y|zt) = exp(Wyzt) P y0\\u2208V exp(Wy0 zt) (5) We note that, during the decoding phase, the visual frame representation for the first LSTM layer is simply a vector of zeros that acts as padding input.\",\"145\":\"We require an explicit end-of-sentence tag (<EOS>) to terminate each sentence since this enables the model to define a distribution over sequences of varying lengths.\",\"146\":\"At test time, during each decoding step we choose the word yt with the maximum probability after the softmax (from Equation 5) until it emits the <EOS> token.\",\"147\":\"3.3.\",\"148\":\"Video and text representation RGB frames.\",\"149\":\"Similar to previous LSTM-based image captioning efforts [8, 40] and video-to-text approaches [39, 43], we apply a convolutional neural network (CNN) to input images and provide the output of the top layer as input to the LSTM unit.\",\"150\":\"In this work, we report results using the output of the fc7 layer (after applying the ReLU non-linearity) on the Caffe Reference Net (a variant of AlexNet) and also the 16-layer VGG model [32].\",\"151\":\"We use CNNs that are pretrained on the 1.2M image ILSVRC-2012 object classification subset of the ImageNet dataset [30] and made available publicly via the Caffe ModelZoo.1 Each input video frame is scaled to 256x256, and is cropped to a random 227x227 1https:\\/\\/github.com\\/BVLC\\/caffe\\/wiki\\/Model-Zoo region.\",\"152\":\"It is then processed by the CNN.\",\"153\":\"We remove the original last fully-connected classification layer and learn a new linear embedding of the features to a 500 dimensional space.\",\"154\":\"The lower dimension features form the input (xt) to the first LSTM layer.\",\"155\":\"The weights of the embedding are learned jointly with the LSTM layers during training.\",\"156\":\"Optical Flow.\",\"157\":\"In addition to CNN outputs from raw image (RGB) frames, we also incorporate optical flow measures as input sequences to our architecture.\",\"158\":\"Others [24, 8] have shown that incorporating optical flow information to LSTMs improves activity classification.\",\"159\":\"As many of our descriptions are activity centered, we explore this option for video description as well.\",\"160\":\"We follow the approach in [8, 9] and first extract classical variational optical flow features [2].\",\"161\":\"We then create flow images (as seen in Figure 1) in a manner similar to [9], by centering x and y flow values around 128 and multiplying by a scalar such that flow values fall between 0 and 255.\",\"162\":\"We also calculate the flow magnitude and add it as a third channel to the flow image.\",\"163\":\"We then use a CNN [9] initialized with weights trained on the UCF101 video dataset to classify optical flow images into 101 activity classes.\",\"164\":\"The fc6 layer activations of the CNN are embedded in a lower 500 dimensional space which is then given as input to the LSTM.\",\"165\":\"The rest of the LSTM architecture remains unchanged for flow inputs.\",\"166\":\"In our combined model, we use a shallow fusion technique to integrate flow and RGB features.\",\"167\":\"At each time step of the decoding phase, the model proposes a set of candidate words.\",\"168\":\"We then rescore these hypotheses with the weighted sum of the scores by the flow and RGB networks, where we only need to recompute the score of each new word p(yt = y0 ) as: \\u03b1 \\u00b7 prgb(yt = y0 ) + (1 \\u2212 \\u03b1) \\u00b7 pflow(yt = y0 ) the hyper-parameter \\u03b1 is tuned on the validation set.\",\"169\":\"MSVD MPII-MD MVAD #-sentences 80,827 68,375 56,634 #-tokens 567,874 679,157 568,408 vocab 12,594 21,700 18,092 #-videos 1,970 68,337 46,009 avg.\",\"170\":\"length 10.2s 3.9s 6.2s #-sents per video \\u224841 1 1-2 Table 1.\",\"171\":\"Corpus Statistics.\",\"172\":\"The the number of tokens in all datasets are comparable, however MSVD has multiple descriptions for each video while the movie corpora (MPII-MD, MVAD) have a large number of clips with a single description each.\",\"173\":\"Thus, the number of video-sentence pairs in all 3 datasets are comparable.\",\"174\":\"Text input.\",\"175\":\"The target output sequence of words are represented using one-hot vector encoding (1-of-N coding, where N is the size of the vocabulary).\",\"176\":\"Similar to the treatment of frame features, we embed words to a lower 500 dimensional space by applying a linear transformation to the input data and learning its parameters via back propagation.\",\"177\":\"The embedded word vector concatenated with the output (ht) of the first LSTM layer forms the input to the second LSTM layer (marked green in Figure 2).\",\"178\":\"When considering the output of the LSTM we apply a softmax over the complete vocabulary as in Equation 5. 4. Experimental Setup This secction describes the evaluation of our approach.\",\"179\":\"We first describe the datasets used, then the evaluation protocol, and then the details of our models.\",\"180\":\"4.1.\",\"181\":\"Video description datasets We report results on three video description corpora, namely the Microsoft Video Description corpus (MSVD) [3], the MPII Movie Description Corpus (MPII-MD) [28], and the Montreal Video Annotation Dataset (M-VAD) [37].\",\"182\":\"Together they form the largest parallel corpora with open domain video and natural language descriptions.\",\"183\":\"While MSVD is based on web clips with short humanannotated sentences, MPII-MD and M-VAD contain Hollywood movie snippets with descriptions sourced from script data and audio description.\",\"184\":\"Statistics of each corpus are presented in Table 1. 4.1.1 Microsoft Video Description Corpus (MSVD) The Microsoft Video description corpus [3], is a collection of Youtube clips collected on Mechanical Turk by requesting workers to pick short clips depicting a single activity.\",\"185\":\"The videos were then used to elicit single sentence descriptions from annotators.\",\"186\":\"The original corpus has multi-lingual descriptions, in this work we use only the English descriptions.\",\"187\":\"We do minimal pre-processing on the text by converting all text to lower case, tokenizing the sentences and removing punctuation.\",\"188\":\"We use the data splits provided by [39].\",\"189\":\"Additionally, in each video, we sample every tenth frame as done by [39].\",\"190\":\"4.1.2 MPII Movie Description Dataset (MPII-MD) MPII-MD [28] contains around 68,000 video clips extracted from 94 Hollywood movies.\",\"191\":\"Each clip is accompanied with a single sentence description which is sourced from movie scripts and audio description (AD) data.\",\"192\":\"AD or Descriptive Video Service (DVS) is an additional audio track that is added to the movies to describe explicit visual elements in a movie for the visually impaired.\",\"193\":\"Although the movie snippets are manually aligned to the descriptions, the data is very challenging due to the high diversity of visual and textual content, and the fact that most snippets have only a single reference sentence.\",\"194\":\"We use the training\\/validation\\/test split provided by the authors and extract every fifth frame (videos are shorter than MSVD, averaging 94 frames).\",\"195\":\"4.1.3 Montreal Video Annotation Dataset (M-VAD) The M-VAD movie description corpus [37] is another recent collection of about 49,000 short video clips from 92 movies.\",\"196\":\"It is similar to MPII-MD, but contains only AD data with automatic alignment.\",\"197\":\"We use the same setup as for MPIIMD.\",\"198\":\"4.2.\",\"199\":\"Evaluation Metrics Quantitative evaluation of the models are performed using the METEOR [7] metric which was originally proposed to evaluate machine translation results.\",\"200\":\"The METEOR score is computed based on the alignment between a given hypothesis sentence and a set of candidate reference sentences.\",\"201\":\"METEOR compares exact token matches, stemmed tokens, paraphrase matches, as well as semantically similar matches using WordNet synonyms.\",\"202\":\"This semantic aspect of METEOR distinguishes it from others such as BLEU [26], ROUGE-L [21], or CIDEr [38].\",\"203\":\"The authors of CIDEr [38] evaluated these four measures for image description.\",\"204\":\"They showed that METEOR is always better than BLEU and ROUGE and outperforms CIDEr when the number of references are small (CIDEr is comparable to METEOR when the number of references are large).\",\"205\":\"Since MPII-MD and M-VAD have only a single reference, we decided to use METEOR in all our evaluations.\",\"206\":\"We employ METEOR version 1.5 2 using the code3 released with the Microsoft COCO Evaluation Server [4].\",\"207\":\"4.3. Experimental details of our models All our models take as input either the raw RGB frames directly feeding into the CNN, or pre-processed optical flow 2http:\\/\\/www.cs.cmu.edu\\/\\u02dcalavie\\/METEOR 3https:\\/\\/github.com\\/tylin\\/coco-caption \\fimages (described in Section 3.3).\",\"208\":\"In all of our models, we unroll the LSTM to a fixed 80 time steps during training.\",\"209\":\"We found this to be a good trade-off between memory consumption and the ability to provide many frames (videos) to the LSTM.\",\"210\":\"This setting allows us to fit multiple videos in a single mini-batch (up to 8 for AlexNet and up to 3 for flow models).\",\"211\":\"We note that 94% of the YouTube training videos satisfied this limit (with frames sampled at the rate of 1 in 10).\",\"212\":\"For videos with fewer than 80 time steps (of words and frames), we pad the remaining inputs with zeros.\",\"213\":\"For longer videos, we truncate the number of frames to ensure that the sum of the number of frames and words is within this limit.\",\"214\":\"At test time, we do not constrain the length of the video and our model views all sampled frames.\",\"215\":\"We use the pre-trained AlexNet and VGG CNNs.\",\"216\":\"For VGG, we fix all layers below fc7 to reduce memory consumption and allow faster training.\",\"217\":\"We compare our sequence to sequence LSTM architecture with RGB image features extracted from both AlexNet, and the 16-layer VGG network.\",\"218\":\"In order to compare features from the VGG network with previous models, we include the performance of the mean-pooled model proposed in [39] using the output of the fc7 layer from the 16 layer VGG as a baseline (line 3, Table 2).\",\"219\":\"All our sequence to sequence models are referenced in Table 2 under S2VT.\",\"220\":\"Our first variant, RGB (AlexNet) is the end-to-end model that uses AlexNet on RGB frames.\",\"221\":\"Flow (AlexNet) refers to the model that is obtained by training on optical flow images.\",\"222\":\"RGB (VGG) refers to the model with the 16-layer VGG model on RGB image frames.\",\"223\":\"We also experiment with randomly re-ordered input frames (line 10) to verify that S2VT learns temporal-sequence information.\",\"224\":\"Our final model is an ensemble of the RGB (VGG) and Flow (AlexNet) where the prediction at each time step is a weighted average of the prediction from the individual models.\",\"225\":\"4.4.\",\"226\":\"Related approaches We compare our sequence to sequence models against the factor graph model (FGM) in [36], the mean-pooled models in [39] and the Soft-Attention models of [43].\",\"227\":\"FGM proposed in [36] uses a two step approach to first obtain confidences on subject, verb, object and scene (S,V,O,P) elements and combines these with confidences from a language model using a factor graph to infer the most likely (S,V,O,P) tuple in the video.\",\"228\":\"It then generates a sentence based on a template.\",\"229\":\"The Mean Pool model proposed in [39] pools AlexNet fc7 activations across all frames to create a fixed-length vector representation of the video.\",\"230\":\"It then uses an LSTM to then decode the vector into a sequence of words.\",\"231\":\"Further, the model ia pre-trained on the Flickr30k [13] and MSCOCO [22] image-caption datasets and fine-tuned on MSVD for a significant improvement in performance.\",\"232\":\"We compare Model METEOR FGM [36] 23.9 (1) Mean pool - AlexNet [39] 26.9 (2) - VGG 27.7 (3) - AlexNet COCO pre-trained [39] 29.1 (4) - GoogleNet [43] 28.7 (5) Temporal attention - GoogleNet [43] 29.0 (6) - GoogleNet + 3D-CNN [43] 29.6 (7) S2VT (ours) - Flow (AlexNet) 24.3 (8) - RGB (AlexNet) 27.9 (9) - RGB (VGG) random frame order 28.2 (10) - RGB (VGG) 29.2 (11) - RGB (VGG) + Flow (AlexNet) 29.8 (12) Table 2.\",\"233\":\"MSVD dataset (METEOR in %, higher is better).\",\"234\":\"our models against their basic mean-pooled model and their best model obtained from fine-tuning on Flickr30k and COCO datasets.\",\"235\":\"We also compare against the GoogleNet [35] variant of the mean-pooled model reported in [43].\",\"236\":\"The Temporal-Attention model in [43] is a combination of weighted attention over a fixed set of video frames with input features from GoogleNet and a 3D-convnet trained on HoG, HoF and MBH features from an activity classification model.\",\"237\":\"5.\",\"238\":\"Results and Discussion This section discussses the result of our evaluation shown in Tables 2, 4, and 5. 5.1.\",\"239\":\"MSVD dataset Table 2 shows the results on the MSVD dataset.\",\"240\":\"Rows 1 through 7 present related approaches and the rest are variants of our S2VT approach.\",\"241\":\"Our basic S2VT AlexNet model on RGB video frames (line 9 in Table 2) achieves 27.9% METEOR and improves over the basic mean-pooled model in [39] (line 2, 26.9%) as well as the VGG meanpooled model (line 3, 27.7%);suggesting that S2VT is a more powerful approach.\",\"242\":\"When the model is trained with randomly-ordered frames (line 10 in Table 2), the score is considerably lower, clearly demonstrating that the model benefits from exploiting temporal structure.\",\"243\":\"Our S2VT model which uses flow images (line 8) achieves only 24.3% METEOR but improves the performance of our VGG model from 29.2% (line 11) to 29.8% (line 12), when combined.\",\"244\":\"A reason for the low performance of the flow model could be that optical flow features even for the same activity can vary significantly with context e.g. \\u2018panda eating\\u2019 vs \\u2018person eating\\u2019.\",\"245\":\"Also, the \\fEdit-Distance k = 0 k <= 1 k <= 2 k <= 3 MSVD 42.9 81.2 93.6 96.6 MPII-MD 28.8 43.5 56.4 83.0 MVAD 15.6 28.7 37.8 45.0 Table 3.\",\"246\":\"Percentage of generated sentences which match a sentence of the training set with an edit (Levenshtein) distance of less than 4.\",\"247\":\"All values reported in percentage (%).\",\"248\":\"model only receives very weak signals with regard to the kind of activities depicted in YouTube videos.\",\"249\":\"Some commonly used verbs such as \\u201cplay\\u201d are polysemous and can refer to playing a musical instrument (\\u201cplaying a guitar\\u201d) or playing a sport (\\u201cplaying golf\\u201d).\",\"250\":\"However, integrating RGB with Flow improves the quality of descriptions.\",\"251\":\"Our ensemble using both RGB and Flow performs slightly better than the best model proposed in [43], temporal attention with GoogleNet + 3D-CNN (line 7).\",\"252\":\"The modest size of the improvement is likely due to the much stronger 3D-CNN features (as the difference to GoogleNet alone (line 6) suggests).\",\"253\":\"Thus, the closest comparison between the Temporal Attention Model [43] and S2VT is arguably S2VT with VGG (line 12) vs. their GoogleNet-only model (line 6).\",\"254\":\"Figure 3 shows descriptions generated by our model on sample Youtube clips from MSVD.\",\"255\":\"To compare the originality in generation, we compute the Levenshtein distance of the predicted sentences with those in the training set.\",\"256\":\"From Table 3, for the MSVD corpus, 42.9% of the predictions are identical to some training sentence, and another 38.3% can be obtained by inserting, deleting or substituting one word from some sentence in the training corpus.\",\"257\":\"We note that many of the descriptions generated are relevant.\",\"258\":\"5.2.\",\"259\":\"Movie description datasets For the more challenging MPII-MD and M-VAD datasets we use our single best model, namely S2VT trained on RGB frames and VGG.\",\"260\":\"To avoid over-fitting on the movie corpora we employ drop-out which has proved to be beneficial on these datasets [27].\",\"261\":\"We found it was best to use dropout at the inputs and outputs of both LSTM layers.\",\"262\":\"Further, we used ADAM [17] for optimization with a first momentum coefficient of 0.9 and a second momentum coefficient of 0.999.\",\"263\":\"For MPII-MD, reported in Table 4, we improve over the SMT approach from [28] from 5.6% to 7.1% METEOR and over Mean pooling [39] by 0.4%.\",\"264\":\"Our performance is similar to Visual-Labels [27], a contemporaneous LSTM-based approach which uses no temporal encoding, but more diverse visual features, namely object detectors, as well as activity and scene classifiers.\",\"265\":\"On M-VAD we achieve 6.7% METEOR which significantly outperforms the temporal attention model [43] Approach METEOR SMT (best variant) [28] 5.6 Visual-Labels [27] 7.0 Mean pool (VGG) 6.7 S2VT: RGB (VGG), ours 7.1 Table 4.\",\"266\":\"MPII-MD dataset (METEOR in %, higher is better).\",\"267\":\"Approach METEOR Visual-Labels [27] 6.3 Temporal att.\",\"268\":\"(GoogleNet+3D-CNN) [43] 4 4.3 Mean pool (VGG) 6.1 S2VT: RGB (VGG), ours 6.7 Table 5.\",\"269\":\"M-VAD dataset (METEOR in %, higher is better).\",\"270\":\"(4.3%)4 and Mean pooling (6.1%).\",\"271\":\"On this dataset we also outperform Visual-Labels [27] (6.3%).\",\"272\":\"We report results on the LSMDC challenge5 , which combines M-VAD and MPII-MD.\",\"273\":\"S2VT achieves 7.0% METEOR on the public test set using the evaluation server.\",\"274\":\"In Figure 4 we present descriptions generated by our model on some sample clips from the M-VAD dataset.\",\"275\":\"More example video clips, generated sentences, and data are available on the authors\\u2019 webpages6 .\",\"276\":\"6.\",\"277\":\"Conclusion This paper proposed a novel approach to video description.\",\"278\":\"In contrast to related work, we construct descriptions using a sequence to sequence model, where frames are first read sequentially and then words are generated sequentially.\",\"279\":\"This allows us to handle variable-length input and output while simultaneously modeling temporal structure.\",\"280\":\"Our model achieves state-of-the-art performance on the MSVD dataset, and outperforms related work on two large and challenging movie-description datasets.\",\"281\":\"Despite its conceptual simplicity, our model significantly benefits from additional data, suggesting that it has a high model capacity, and is able to learn complex temporal structure in the input and output sequences for challenging moviedescription datasets.\",\"282\":\"Acknowledgments We thank Lisa Anne Hendricks, Matthew Hausknecht, Damian Mrowca for helpful discussions; and Anna Rohrbach for help with both movie corpora; and the 4We report results using the predictions provided by [43] but using the orginal COCO Evaluation scripts.\",\"283\":\"[43] report 5.7% METEOR for their temporal attention + 3D-CNN model using a different tokenization.\",\"284\":\"5LSMDC: sites.google.com\\/site\\/describingmovies 6http:\\/\\/vsubhashini.github.io\\/s2vt.html \\fCorrect descriptions.\",\"285\":\"Relevant but incorrect descriptions.\",\"286\":\"Irrelevant descriptions.\",\"287\":\"(a) (b) (c) Figure 3.\",\"288\":\"Qualitative results on MSVD YouTube dataset from our S2VT model (RGB on VGG net).\",\"289\":\"(a) Correct descriptions involving different objects and actions for several videos.\",\"290\":\"(b) Relevant but incorrect descriptions.\",\"291\":\"(c) Descriptions that are irrelevant to the event in the video.\",\"292\":\"(1) (2) (3) (4) (5) S2VT (Ours): (1) Now, the van pulls out a window and a tall brick facade of tall trees .\",\"293\":\"a figure stands at a curb.\",\"294\":\"(2) Someone drives off the passenger car and drives off.\",\"295\":\"(3) They drive off the street.\",\"296\":\"(4) They drive off a suburban road and parks in a dirt neighborhood.\",\"297\":\"(5) They drive off a suburban road and parks on a street.\",\"298\":\"(6) Someone sits in the doorway and stares at her with a furrowed brow.\",\"299\":\"(6a) (6b) Temporal Attention (GNet+3D-convatt ): (1) At night , SOMEONE and SOMEONE step into the parking lot.\",\"300\":\"(2) Now the van drives away.\",\"301\":\"(3) They drive away.\",\"302\":\"(4) They drive off.\",\"303\":\"(5) They drive off.\",\"304\":\"(6) At the end of the street , SOMEONE sits with his eyes closed.\",\"305\":\"DVS: (1) Now , at night , our view glides over a highway , its lanes glittering from the lights of traffic below.\",\"306\":\"(2) Someone's suv cruises down a quiet road.\",\"307\":\"(3) Then turn into a parking lot .\",\"308\":\"(4) A neon palm tree glows on a sign that reads oasis motel.\",\"309\":\"(5) Someone parks his suv in front of some rooms.\",\"310\":\"(6) He climbs out with his briefcase , sweeping his cautious gaze around the area.\",\"311\":\"Figure 4.\",\"312\":\"M-VAD Movie corpus: Representative frame from 6 contiguous clips from the movie \\u201cBig Mommas: Like Father, Like Son\\u201d.\",\"313\":\"From left: Temporal Attention (GoogleNet+3D-CNN) [43], S2VT (in blue) trained on the M-VAD dataset, and DVS: ground truth.\",\"314\":\"anonymous reviewers for insightful comments and suggestions.\",\"315\":\"We acknowledge support from ONR ATL Grant N00014-11-1-010, DARPA, AFRL, DoD MURI award N000141110688, DEFT program (AFRL grant FA8750-132-0026), NSF awards IIS-1427425, IIS-1451244, and IIS1212798, and Berkeley Vision and Learning Center.\",\"316\":\"Raymond and Kate acknowledge support from Google.\",\"317\":\"Marcus was supported by the FITweltweit-Program of the German Academic Exchange Service (DAAD).\",\"318\":\"References [1] H. Aradhye, G. Toderici, and J. Yagnik.\",\"319\":\"Video2text: Learning to annotate video content.\",\"320\":\"In ICDMW, 2009. 2 [2] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert.\",\"321\":\"High accuracy optical flow estimation based on a theory for warping.\",\"322\":\"In ECCV, pages 25\\u201336, 2004. 2, 4 [3] D. L. Chen and W. B. Dolan.\",\"323\":\"Collecting highly parallel data for paraphrase evaluation.\",\"324\":\"In ACL, 2011. 2, 5 [4] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dol\\flar, and C. L. Zitnick.\",\"325\":\"Microsoft COCO captions: Data collection and evaluation server.\",\"326\":\"arXiv:1504.00325, 2015. 5 [5] X. Chen and C. L. Zitnick.\",\"327\":\"Learning a recurrent visual representation for image caption generation.\",\"328\":\"CVPR, 2015. 1 [6] K. Cho, B. van Merrie\\u0308nboer, D. Bahdanau, and Y. Bengio.\",\"329\":\"On the properties of neural machine translation: Encoderdecoder approaches.\",\"330\":\"arXiv:1409.1259, 2014. 3 [7] M. Denkowski and A. Lavie.\",\"331\":\"Meteor universal: Language specific translation evaluation for any target language.\",\"332\":\"In EACL, 2014. 5 [8] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell.\",\"333\":\"Long-term recurrent convolutional networks for visual recognition and description.\",\"334\":\"In CVPR, 2015. 1, 2, 3, 4 [9] G. Gkioxari and J. Malik.\",\"335\":\"Finding action tubes.\",\"336\":\"2014. 4 [10] A. Graves and N. Jaitly.\",\"337\":\"Towards end-to-end speech recognition with recurrent neural networks.\",\"338\":\"In ICML, 2014. 1 [11] S. Guadarrama, N. Krishnamoorthy, G. Malkarnenkar, S. Venugopalan, R. Mooney, T. Darrell, and K. Saenko.\",\"339\":\"Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shoot recognition.\",\"340\":\"In ICCV, 2013. 1, 2 [12] S. Hochreiter and J. Schmidhuber.\",\"341\":\"Long short-term memory.\",\"342\":\"Neural computation, 9(8), 1997. 1, 3 [13] P. Hodosh, A. Young, M. Lai, and J. Hockenmaier.\",\"343\":\"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.\",\"344\":\"In TACL, 2014. 6 [14] H. Huang, Y. Lu, F. Zhang, and S. Sun.\",\"345\":\"A multi-modal clustering method for web videos.\",\"346\":\"In ISCTCS.\",\"347\":\"2013. 2 [15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell.\",\"348\":\"Caffe: Convolutional architecture for fast feature embedding.\",\"349\":\"ACMMM, 2014. 2 [16] A. Karpathy and L. Fei-Fei.\",\"350\":\"Deep visual-semantic alignments for generating image descriptions.\",\"351\":\"CVPR, 2015. 1 [17] D. Kingma and J. Ba. Adam: A method for stochastic optimization.\",\"352\":\"arXiv preprint arXiv:1412.6980, 2014. 7 [18] R. Kiros, R. Salakhutdinov, and R. S. Zemel.\",\"353\":\"Unifying visual-semantic embeddings with multimodal neural language models.\",\"354\":\"arXiv:1411.2539, 2014. 1 [19] N. Krishnamoorthy, G. Malkarnenkar, R. J. Mooney, K. Saenko, and S. Guadarrama.\",\"355\":\"Generating natural-language video descriptions using text-mined knowledge.\",\"356\":\"In AAAI, July 2013. 2 [20] P. Kuznetsova, V. Ordonez, T. L. Berg, U. C. Hill, and Y. Choi.\",\"357\":\"Treetalk: Composition and compression of trees for image descriptions.\",\"358\":\"In TACL, 2014. 1 [21] C.-Y. Lin.\",\"359\":\"Rouge: A package for automatic evaluation of summaries.\",\"360\":\"In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74\\u201381, 2004. 5 [22] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dolla\\u0301r, and C. L. Zitnick.\",\"361\":\"Microsoft coco: Common objects in context.\",\"362\":\"In ECCV, 2014. 6 [23] J. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille.\",\"363\":\"Deep captioning with multimodal recurrent neural networks (mrnn).\",\"364\":\"arXiv:1412.6632, 2014. 1 [24] J. Y. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici.\",\"365\":\"Beyond short snippets: Deep networks for video classification.\",\"366\":\"CVPR, 2015. 3, 4 [25] P.\",\"367\":\"Over, G. Awad, M. Michel, J. Fiscus, G. Sanders, B. Shaw, A. F. Smeaton, and G. Que\\u0301enot.\",\"368\":\"TRECVID 2012 \\u2013 an overview of the goals, tasks, data, evaluation mechanisms and metrics.\",\"369\":\"In Proceedings of TRECVID 2012, 2012. 2 [26] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.\",\"370\":\"Bleu: a method for automatic evaluation of machine translation.\",\"371\":\"In ACL, 2002. 5 [27] A. Rohrbach, M. Rohrbach, and B. Schiele.\",\"372\":\"The long-short story of movie description.\",\"373\":\"GCPR, 2015. 7 [28] A. Rohrbach, M. Rohrbach, N. Tandon, and B. Schiele.\",\"374\":\"A dataset for movie description.\",\"375\":\"In CVPR, 2015. 1, 2, 5, 7 [29] M. Rohrbach, W. Qiu, I. Titov, S. Thater, M. Pinkal, and B. Schiele.\",\"376\":\"Translating video content to natural language descriptions.\",\"377\":\"In ICCV, 2013. 1 [30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei.\",\"378\":\"ILSVRC, 2014. 4 [31] K. Simonyan and A. Zisserman.\",\"379\":\"Two-stream convolutional networks for action recognition in videos.\",\"380\":\"In NIPS, 2014. 2 [32] K. Simonyan and A. Zisserman.\",\"381\":\"Very deep convolutional networks for large-scale image recognition.\",\"382\":\"CoRR, abs\\/1409.1556, 2014. 4 [33] N. Srivastava, E. Mansimov, and R. Salakhutdinov.\",\"383\":\"Unsupervised learning of video representations using LSTMs.\",\"384\":\"ICML, 2015. 2 [34] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks.\",\"385\":\"In NIPS, 2014. 1, 2, 3 [35] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.\",\"386\":\"Going deeper with convolutions.\",\"387\":\"CVPR, 2015. 6 [36] J. Thomason, S. Venugopalan, S. Guadarrama, K. Saenko, and R. J. Mooney.\",\"388\":\"Integrating language and vision to generate natural language descriptions of videos in the wild.\",\"389\":\"In COLING, 2014. 2, 6 [37] A. Torabi, C. Pal, H. Larochelle, and A. Courville.\",\"390\":\"Using descriptive video services to create a large data source for video annotation research.\",\"391\":\"arXiv:1503.01070v1, 2015. 2, 5 [38] R. Vedantam, C. L. Zitnick, and D. Parikh.\",\"392\":\"CIDEr: Consensus-based image description evaluation.\",\"393\":\"CVPR, 2015. 5 [39] S. Venugopalan, H. Xu, J. Donahue, M. Rohrbach, R. Mooney, and K. Saenko.\",\"394\":\"Translating videos to natural language using deep recurrent neural networks.\",\"395\":\"In NAACL, 2015. 1, 2, 4, 5, 6, 7 [40] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan.\",\"396\":\"Show and tell: A neural image caption generator.\",\"397\":\"CVPR, 2015. 1, 2, 4 [41] H. Wang and C. Schmid.\",\"398\":\"Action recognition with improved trajectories.\",\"399\":\"In ICCV, pages 3551\\u20133558.\",\"400\":\"IEEE, 2013. 2 [42] S. Wei, Y. Zhao, Z. Zhu, and N. Liu.\",\"401\":\"Multimodal fusion for video search reranking.\",\"402\":\"TKDE, 2010. 2 [43] L. Yao, A. Torabi, K. Cho, N. Ballas, C. Pal, H. Larochelle, and A. Courville.\",\"403\":\"Describing videos by exploiting temporal structure.\",\"404\":\"arXiv:1502.08029v4, 2015. 1, 2, 4, 6, 7, 8 [44] W. Zaremba and I. Sutskever.\",\"405\":\"Learning to execute.\",\"406\":\"arXiv:1410.4615, 2014. 3\",\"407\":\"DevNet: A Deep Event Network for Multimedia Event Detection and Evidence Recounting Chuang Gan1\\u2217 Naiyan Wang2\\u2217 Yi Yang3 Dit-Yan Yeung2 Alexander G. Hauptmann4 1 Institute for Interdisciplinary Information Sciences, Tsinghua University, China 2 Hong Kong University of Science and Technology 3 Centre for Quantum Computation and Intelligent Systems, University of Technology, Sydney, Australia 4 School of Computer Science, Carnegie Mellon University, USA {ganchuang1990, winsty, yee.i.yang }@gmail.com, dyyeung@cse.ust.hk, alex@cs.cmu.edu Abstract In this paper, we focus on complex event detection in internet videos while also providing the key evidences of the detection results.\",\"408\":\"Convolutional Neural Networks (CNNs) have achieved promising performance in image classi\\ufb01cation and action recognition tasks.\",\"409\":\"However, it remains an open problem how to use CNNs for video event detection and recounting, mainly due to the complexity and diversity of video events.\",\"410\":\"In this work, we propose a \\ufb02exible deep CNN infrastructure, namely Deep Event Network (DevNet), that simultaneously detects pre-de\\ufb01ned events and provides key spatial-temporal evidences.\",\"411\":\"Taking key frames of videos as input, we \\ufb01rst detect the event of interest at the video level by aggregating the CNN features of the key frames.\",\"412\":\"The pieces of evidences which recount the detection results, are also automatically localized, both temporally and spatially.\",\"413\":\"The challenge is that we only have video level labels, while the key evidences usually take place at the frame levels.\",\"414\":\"Based on the intrinsic property of CNNs, we \\ufb01rst generate a spatial-temporal saliency map by back passing through DevNet, which then can be used to \\ufb01nd the key frames which are most indicative to the event, as well as to localize the speci\\ufb01c spatial position, usually an object, in the frame of the highly indicative area.\",\"415\":\"Experiments on the large scale TRECVID 2014 MEDTest dataset demonstrate the promising performance of our method, both for event detection and evidence recounting.\",\"416\":\"1.\",\"417\":\"Introduction Detecting complex events in videos is a challenging task which has received signi\\ufb01cant research attention in the \\u2217The \\ufb01rst two authors contribute equally to this work.\",\"418\":\"Temporal key evidence Temporal key evidence Temporal Key Evidence Spatial Key Evidence Complex Event Video Event Label Attempting a bike trick Figure 1.\",\"419\":\"Given a video for testing, DevNet not only provides an event label but also spatial-temporal key evidences.\",\"420\":\"computer vision community.\",\"421\":\"Compared to atomic concept recognition, which mainly focuses on recognizing particular objects and scene in still images or simple motions in short video clips of 5-10 seconds, multimedia event detection deals with more complex videos that consist of various interactions of human actions and objects in different scenes often lasting for several minutes to even an hour.\",\"422\":\"Thus, an event is a semantic abstraction of video sequences of higher level than a concept and often consists of multiple concepts.\",\"423\":\"For example, a \\u201cTown hall meeting\\u201d event can be described by multiple objects (e.g., persons, podium), a scene (e.g., in a conference room), actions (e.g., talking, meeting) and acoustic concepts (e.g., speech, clapping).\",\"424\":\"Besides the concern of detecting semantic events, in many situations just assigning a video an event label is not enough, as discussed in [35, 34, 25, 45], because a long unconstrained video may contain a lot of irrelevant information and even the same event label may contain large intra-class variations.\",\"425\":\"Besides providing a single event label, many users also 4321 \\fwant to know why the video is recognized as this event, namely the key evidences that lead to the detection decision.\",\"426\":\"This process is called Multimedia Event Recounting (MER).\",\"427\":\"If event detection answers the question \\u201cIs this the desired event?\\u201d, event recounting answers the next question, \\u201cWhy does this video contain the desired event?\\u201d.\",\"428\":\"The key step of event recounting is localizing the key evidences, referred to here as evidence recounting, which is the focus of this paper.\",\"429\":\"The recounting result is category-speci\\ufb01c.\",\"430\":\"It is unlike traditional video summarization tasks [24, 27, 12], which mainly seek to reduce the redundancy of the videos.\",\"431\":\"As a result, the localized key evidences could effectively summarize the videos, and then allow the users to browse the videos and grasp the important parts quickly.\",\"432\":\"Although many algorithms have been proposed for the event detection and recounting problems recently, the challenges have not been fully addressed.\",\"433\":\"The most successful methods for event detection are still aggregating shallow hand-crafted visual features, e.g., SIFT [26], MOSIFT [4], trajectory [37], improved dense trajectory [38], followed by feature pooling and a conventional classi\\ufb01er, such as Support Vector Machine (SVM) [2] or Kernel Ridge Regression (KR) [36].\",\"434\":\"However, such shallow features and event detection pipeline cannot capture the high complexity and variability of unconstrained videos.\",\"435\":\"For recounting, most of the previous works focus on generating temporal-level key evidences (informative key frames or shots).\",\"436\":\"However, this is still far from satisfactory because even within one frame, the scene and objects may be cluttered and non-informative.\",\"437\":\"As the example in Figure 1 shows, the spatial localized bike wheel may suggest that this video is an \\u201cAttempting a bike trick\\u201d event, while the background window does not.\",\"438\":\"Therefore, we decide to take it a step further, not only to localize the temporal evidences but also the spatial evidences.\",\"439\":\"However, simultaneously assigning the retrieved video an event label and providing spatial-temporal key evidences is a non-trivial task due to the following reasons.\",\"440\":\"First, different video sequences of the same event may have dramatic variations.\",\"441\":\"Taking the \\u201cWinning a race without vehicle\\u201d event as an example, it may take place in a stadium, in a swimming pool or even in an urban park, where the visual features could be very different.\",\"442\":\"Therefore, we can hardly utilize the rigid templates or rules to localize the key evidences.\",\"443\":\"Second, the cost of collection and annotation of spatial-temporal key evidences is generally extremely high.\",\"444\":\"It is prohibitive to extend the traditional fully-supervised object localization approaches for images, which employ the ground-truth bounding box information for training, to the video event recounting task directly.\",\"445\":\"In contrast to hand-crafted features, learning features with Convolutional Neural Networks (CNNs) [17], has shown great potentials in various computer vision tasks giving state-of-the-art performance in image recognition [17, 11, 3, 13, 40, 39] and promising results in action recognition [16, 32].\",\"446\":\"The successes of CNNs also shed light on the multimedia event detection and recounting problems.\",\"447\":\"However, whether and how the CNN architecture could be exploited for the video event detection and recounting problems has never been studied before.\",\"448\":\"This motivates us to apply CNNs to detecting and recounting event videos.\",\"449\":\"In this paper, we propose a Deep Event Network (DevNet) that can simultaneously detect high-level events and localize spatial-temporal key evidences.\",\"450\":\"To reduce the in\\ufb02uence of limited training data, we \\ufb01rst pre-train the DevNet using the largest image dataset to date, ImageNet [7], and then transfer the image-level features and train a new video-level event detector by \\ufb01ne-tuning the network.\",\"451\":\"Next, we exploit the intrinsic property of CNNs to generate a spatial-temporal saliency map without resorting to additional training steps.\",\"452\":\"We only need to rank the saliency scores on the key frame level to localize the informative temporal evidences.\",\"453\":\"For the top ranked key frames, we apply the graph-cut algorithm [1] to the segmentation of discriminative regions as the spatial key evidences.\",\"454\":\"Note that the localization process only utilizes the video-level event label without requiring the annotations of key frames and bounding boxes.\",\"455\":\"Our work makes the following contributions: \\u2022 To the best of our knowledge, we are the \\ufb01rst to conduct high-level video event detection and spatialtemporal key evidence localization based on CNNs.\",\"456\":\"\\u2022 This is the \\ufb01rst paper that attempts to not only localize temporal key evidences (informative key frames and shots), but also provide discriminative spatial regions for evidence recounting.\",\"457\":\"\\u2022 We show that our framework signi\\ufb01cantly outperforms state-of-the-art hand-crafted shallow features on event detection tasks and achieves satisfactory results for localizing spatial-temporal key evidences, which con\\ufb01rm the importance of representation learning for the event detection and evidence recounting tasks.\",\"458\":\"The rest of this paper is organized as follows.\",\"459\":\"In Section 2, we review related work in multimedia event detection, multimedia event recounting and CNNs.\",\"460\":\"Section 3 presents the DevNet and in particular details on how it can be applied to multimedia event detection and recounting.\",\"461\":\"The experimental settings and evaluation results are presented in Section 4.\",\"462\":\"Section 5 concludes the paper.\",\"463\":\"2.\",\"464\":\"Related Work Our framework relates to three research directions: event detection, event recounting, and CNNs, which will be brie\\ufb02y reviewed in this section.\",\"465\":\"4322 \\f2.1.\",\"466\":\"Event Detection Complex event detection has attracted a lot of research interest in the past decade.\",\"467\":\"A recent review can be found in [15].\",\"468\":\"A video event detection system usually consists of the following procedure: feature extraction, quantization\\/pooling, and classi\\ufb01er training.\",\"469\":\"Many event detection approaches rely on shallow low-level features such as SIFT [26] for static key frames, and STIP [20] and MOSIFT [4] for videos.\",\"470\":\"Recently, state-of-the-art shallow video representation makes use of dense point trajectories [37, 38].\",\"471\":\"Its feature vectors are obtained by tracking densely sampled points and describing the volume around tracklets by histograms of optical \\ufb02ow (HOF) [21], histograms of oriented gradients (HOG) [5], and motion boundary histograms (MBH) [6].\",\"472\":\"To aggregate video-level features, it then applies Fisher vector coding [29] on the shallow low-level features.\",\"473\":\"Moreover, there are also several attempts to conduct few-shots [44, 28] and even zeroshot [42, 10] event detection.\",\"474\":\"2.2.\",\"475\":\"Event Recounting Multimedia event recounting aims to \\ufb01nd the eventspeci\\ufb01c discriminative parts of video.\",\"476\":\"Most existing approaches focus on the temporal domain.\",\"477\":\"In [34, 25, 45, 30], they apply object and action detectors or low-level visual features to localize temporal key evidences.\",\"478\":\"They train a video-level classi\\ufb01er and then use it to rank the key frames or shots.\",\"479\":\"These approaches are based on the assumption that the video-level classi\\ufb01ers that can distinguish positive and negative exemplars can also be used to distinguish the informative shots.\",\"480\":\"However, these approaches equally treat the shots or key frames within the video.\",\"481\":\"Consequently, the classi\\ufb01er may be confused by the ubiquitous but noninformative shots from videos.\",\"482\":\"To overcome these limitations, [18] and [19] formulated the problem as a multipleinstance learning problem, aiming at learning an instancelevel event detection and recounting model by selecting the informative shots or key fames during the training process.\",\"483\":\"However, these approaches could only localize temporal key evidences.\",\"484\":\"2.3.\",\"485\":\"Convolutional Neural Networks Deep learning tries to model high-level abstraction of data by using model architectures composed of multiple nonlinear transformations.\",\"486\":\"Speci\\ufb01cally, CNNs [23] correspond to a biologically-inspired class of deep learning models that have demonstrated extraordinary abilities for some high-level vision tasks, such as image classi\\ufb01cation [17], object detection [11], and scene labeling [9].\",\"487\":\"Moreover, the features learned by large networks trained on the ImageNet dataset [7] show great generalization ability that yields state-of-the-art performance beyond standard image classi\\ufb01cation tasks, e.g., on several action recognition datasets [16, 32].\",\"488\":\"Besides, the problem of understanding and visualizing deep CNNs [47, 22, 8] has also attracted a lot of research attention.\",\"489\":\"Very recently, [31, 47] proposed to localize the objects in images in a weakly supervised manner without relying on bounding box annotations.\",\"490\":\"Compared to still image data and shot action videos, there is relatively little work on applying CNNs to multimedia event detection and recounting tasks.\",\"491\":\"This motivates us to exploit the powerful features learned by CNNs to solve these problems.\",\"492\":\"3.\",\"493\":\"DevNet Framework In the proposed DevNet, the CNN architecture is similar to the network described in [17] except that it is much deeper.\",\"494\":\"The CNN contains nine convolutional layers and three fully-connected layers.\",\"495\":\"Between these two parts, a spatial pyramid pooling layer [13] is adopted.\",\"496\":\"Consequently, without suf\\ufb01cient training data, it is very dif\\ufb01cult to obtain an effective DevNet model for event detection.\",\"497\":\"Thus, we \\ufb01rst use the large ImageNet dataset [7] to pre-train the CNN for parameter initialization.\",\"498\":\"The goal of this pre-training stage is to learn generic image-level features.\",\"499\":\"However, directly using the parameters obtained from training on ImageNet for video event detection is not a proper choice, due to the domain difference between multimedia event detection and image classi\\ufb01cation.\",\"500\":\"Thus we apply dataset-speci\\ufb01c \\ufb01ne-tuning [3, 11, 41, 32] to adjust the parameters.\",\"501\":\"After \\ufb01ne-tuning the parameters of the DevNet, we apply a single backward pass to identify the pixels in the same video with strong responses as spatial-temporal key evidences for event recounting.\",\"502\":\"The DevNet framework is depicted in Figure 2.\",\"503\":\"3.1.\",\"504\":\"DevNet Pre-training Our experiments start with a deep CNN trained on the ILSVRC-2014 dataset [16] which includes 1.2M training images categorized into 1000 classes.\",\"505\":\"The structure of our CNN is shown in Figure 2.\",\"506\":\"It is implemented using the Caffe [14] toolbox.\",\"507\":\"Given a training image, we \\ufb01rst resize its shorter edge to 256 pixels.\",\"508\":\"Then, we randomly extract \\ufb01xed-size 224 \\u00d7 224 patches from the resized images and train our network with these extracted patches.\",\"509\":\"Each extracted patch is pre-processed by image mean subtraction, random illumination and contrast augmentation [33].\",\"510\":\"As described in [17], the output of the last fully-connected layer is fed into a 1000-way softmax layer with the multinomial logistic regression used to de\\ufb01ne the loss function, which is equivalent to de\\ufb01ning a probability distribution over the 1000 classes.\",\"511\":\"For all layers, we use Recti\\ufb01ed Linear Units (ReLU) [17] as the nonlinear activation function.\",\"512\":\"We train the network by using stochastic gradient descent with a momentum of 0.9 and weight decay of 0.0005.\",\"513\":\"To 4323 \\fPre-training on ImageNet parameter transferring ... ...\",\"514\":\"Max pooling Fine-tuning on MED videos parameter transferring ... .\",\"515\":\".\",\"516\":\"... convolution layer fully connected layer classifier Figure 2.\",\"517\":\"An illustration of the infrastructure of DevNet.\",\"518\":\"We \\ufb01rst pre-trained the DevNet using the ImageNet, and then \\ufb01ne-tuning on the MED video dataset.\",\"519\":\"overcome over-\\ufb01tting, the \\ufb01rst two fully-connected layers are followed by a dropout layer with a dropout rate of 0.5.\",\"520\":\"The learning rate is initialized to 0.01 for all layers and is reduced to one-tenth of the current rate after every 20 epochs (80 epochs in all).\",\"521\":\"Our weight layer con\\ufb01guration is: conv64-conv192-conv384-conv384-conv384-conv384conv384-conv384-conv384-full4096-full4096-full1000.\",\"522\":\"On ILSVRC2014 [7] validation set, the network achieves the top-1\\/top-5 classi\\ufb01cation error of 29.7%\\/10.5%.\",\"523\":\"3.2.\",\"524\":\"DevNet Fine-tuning Inspired by the recent success of domain-speci\\ufb01c \\ufb01netuning [11, 3, 41], we also utilize the training video data in the MED dataset as target data to further adjust the parameters.\",\"525\":\"This is to adapt the model pre-trained on the ImageNet dataset to the video event detection task.\",\"526\":\"The steps of videolevel \\ufb01ne-tuning are described as follows.\",\"527\":\"First, we remove the softmax classi\\ufb01er and the last fully connected layer of the pre-trained network since it is speci\\ufb01c to the ImageNet classi\\ufb01cation task.\",\"528\":\"Next, to aggregate image-level features into the video-level representation, cross-image max-pooling is applied to fuse the outputs of the second fully-connected layer from all the key frames within the same video.\",\"529\":\"Let st = (s1t, ..., snt)T \\u2208 Rn be the feature vector of key frame t and f = (f1, ..., fn)T \\u2208 Rn be the video-level feature vector.\",\"530\":\"Then the ith dimension of the video-level feature vector f can be represented as: fi = max t sit.\",\"531\":\"(1) Considering that the evaluation of event detection tasks is based on ranking, there is no inter-event competition and so we replace the softmax loss with a more appropriate c-way independent logistic regression, which produces a detection score for each event class.\",\"532\":\"Here c denotes the number of event classes.\",\"533\":\"To handle the imbalance of positive and negative exemplars, we randomly sample them in a 1:1 ratio during the \\ufb01ne-tuning process.\",\"534\":\"Implementation details.\",\"535\":\"Similar to the approach described in [46], we sample several key frames from each video and then follow the same preprocessing and data augmentation steps as in the pre-training stage.\",\"536\":\"Furthermore, as inspired by the experience of [11, 3], we use different learning rates for different layers.\",\"537\":\"The learning rates of the convolutional layers, the \\ufb01rst two fully-connected layers and the last fully-connected layer are initialized to 0.001, 0.001 and 0.01, respectively.\",\"538\":\"We \\ufb01ne-tune for 30 epochs in total and decrease the learning rate to one-tenth of the current rate of each layer after 10 epochs.\",\"539\":\"After \\ufb01ne-tuning, we use the video-level representation after cross-image max-pooling (i.e., f in Eq. (1), the last fully connected layer) as the features for the event detection task.\",\"540\":\"3.3.\",\"541\":\"Gradient-based Spatial-temporal Saliency Map In this section, we extend a previous method [31] which generates class saliency maps for images to the video domain.\",\"542\":\"The main idea of our event recounting approach is that, given a learned detection DevNet and a class of interest, we trace back to the original input image by a backward pass with which we can \\ufb01nd how each pixel affects the \\ufb01nal 4324 \\fdetection score for the speci\\ufb01ed event class.\",\"543\":\"Let us start with a motivational example.\",\"544\":\"For a video V , we represent it as X \\u2208 Rp\\u00d7q\\u00d7n , where p and q denote the height and width of each frame and n is the number of frames.\",\"545\":\"We consider a simple case in which the detection score of event class c is linear with respect to the video, i.e. Sc(V ) = wT c x + bc, (2) where x \\u2208 Rpqn\\u00d71 is a vectorized form of the video V , and wc \\u2208 Rpqn\\u00d71 and bc are the weight vector and bias of the model.\",\"546\":\"In this case, it is easy to see that the magnitude of the elements of wc speci\\ufb01es the importance of the corresponding pixels of V for class c. In the case of a deep CNN, however, the class score Sc(V ) is a highly nonlinear function of V , so the assumption and analysis in the previous paragraph cannot be applied directly.\",\"547\":\"However, we can approximate Sc(V ) by a \\ufb01rst-order Taylor expansion expanding at V0, where Sc(V ) \\u2248 wT c x + b, (3) with the derivative of Sc(V ) with respect to V at point V0 as: wc = \\u2202Sc \\u2202V \\u0002 \\u0002 \\u0002 \\u0002 V0 .\",\"548\":\"(4) The magnitude of the derivative of Eq. (4) indicates which pixels within the video need to be changed the least to affect the class score the most.\",\"549\":\"We can expect that such pixels are the spatial-temporal key evidences to detect this event.\",\"550\":\"Given a video that belongs to event class c with k key frames of size p \\u00d7 q, the spatial and temporal key evidences are computed as follows.\",\"551\":\"First, the derivative wc in Eq. (4) is found by back-propagation.\",\"552\":\"After that, the saliency map is obtained by rearranging the elements of vector wc.\",\"553\":\"In the case of a gray scale image, the number of elements in wc is equal to the number of pixels in each frame multiplied by the number of key frames.\",\"554\":\"So the saliency score of each pixel in each key frame can be computed as M(i, j, k) = |wc(h(i, j, k))|, where h(i, j, k) is the index of the element of wc corresponding to the image pixel in the ith row and jth column of the kth key frame.\",\"555\":\"In the case of a multi-channel (e.g. RGB) image, we take the maximum magnitude of wc across all color channels of each pixel as the saliency value.\",\"556\":\"Thus for each event class, we can derive a single class-speci\\ufb01c saliency score for each pixel in the video.\",\"557\":\"It is important to note that our spatial-temporal saliency maps M \\u2208 Rp\\u00d7q\\u00d7n are extracted using the DevNet trained on the video-level label and hence no additional annotation (such as informative key frames and bounding boxes) is required.\",\"558\":\"The computation of saliency maps is extremely fast since it only requires a single backward pass without additional training.\",\"559\":\"After obtaining the spatial-temporal saliency map, we average the saliency scores of all the pixels within a key frame to obtain a key-frame level saliency score, and then we rank the key-frame level saliency scores to obtain the informative key frame.\",\"560\":\"For the top ranked key frames, we use the saliency scores as guidance and apply the graph-cut algorithm [1] to segment the spatial salient regions.\",\"561\":\"4.\",\"562\":\"Experiments We present the dataset, experimental settings, evaluation criteria and experimental results in this section.\",\"563\":\"4.1.\",\"564\":\"Evaluation Dataset We perform our experiments on the challenging NIST TRECVID 2014 Multimedia Event Detection dataset \\u2217 .\",\"565\":\"To the best of our knowledge, it is the largest publicly available video corpora in the literature for event detection and recounting.\",\"566\":\"This dataset contains unconstrained web videos with large variation in length, quality and resolution.\",\"567\":\"In addition, it also comes with ground-truth video-level annotations for 20 event categories.\",\"568\":\"Following the 100EX evaluation procedure outlined by the NIST TRECVID detection task, we used three different partitions for evaluation: Background, which contains about 5000 background videos not belonging to any of the target events, and 100EX, which contains 100 positive videos for each event, are used as the training set.\",\"569\":\"MEDTest, which contains 23,954 videos, is used as the test set.\",\"570\":\"4.2.\",\"571\":\"Event Detection Protocol The event detection task is to rank the videos in the database according to the speci\\ufb01c query.\",\"572\":\"We may also regard it as a video retrieval task.\",\"573\":\"Our event detection approach consists of the following consecutive steps: 1. Extracting key frames.\",\"574\":\"As processing all MED video frames will be computationally expensive, we only extract features from the key frames.\",\"575\":\"Thus we start with detecting the shot boundaries by calculating the color histograms for all the frames.\",\"576\":\"For each frame, we then subtract the previous color histogram from the current one.\",\"577\":\"If the absolute value of the difference is larger than a certain threshold, this key frame is marked as a shot boundary [46].\",\"578\":\"After detecting the shot, we use the frame in the middle to represent that shot.\",\"579\":\"By using this algorithm, we extracted about 1.2 million key frames from the TRECVID MED 2014 dataset.\",\"580\":\"2. Extracting features.\",\"581\":\"We use the features of the last fully-connected layer after cross-frame max-pooling for video representation.\",\"582\":\"We then normalize the features to make the l2 norm of the feature vector equal \\u2217http:\\/\\/nist.gov\\/itl\\/iad\\/mig\\/med14.cfm 4325 \\fto 1.\",\"583\":\"More details are presented in Section 3.2.\",\"584\":\"3.\",\"585\":\"Training event classi\\ufb01er.\",\"586\":\"Due to limited training data in the video level, directly using the classi\\ufb01er in DevNet will result in inferior performance.\",\"587\":\"Support vector machines (SVMs) [2] and kernel ridge regression (KR) [36] with \\u03c72 kernel are used.\",\"588\":\"We obtain the regularization parameters by 5-fold cross validation.\",\"589\":\"4.\",\"590\":\"Testing event classi\\ufb01er.\",\"591\":\"We apply the trained event classi\\ufb01er on MEDTest and rank the videos by their detection results.\",\"592\":\"4.3.\",\"593\":\"Event Detection Results We use two evaluation metrics for ranked lists which are used by the NIST: Minimal Normalized Detection Cost (MinNDC) and Average Precision (AP) for each event.\",\"594\":\"The de\\ufb01nition of MinNDC is: MinNDC = CMD \\u00d7 PMD \\u00d7 PT + CF A \\u00d7 PF A \\u00d7 (1 \\u2212 PT ) min(CMD \\u00d7 PT , CMD \\u00d7 (1 \\u2212 PT )) .\",\"595\":\"(5) Here PMD is the miss detection probability and PF A is the false positive rate.\",\"596\":\"CMD = 80 is the cost for miss detection, CF A = 1 is the cost for false alarm, and PT = 0.001 is a constant which speci\\ufb01es the prior rate of event instances.\",\"597\":\"Average Precision (AP) is a common metric for evaluation of ranking list.\",\"598\":\"We also use the mean Average Precision (mAP) to evaluate the results by averaging all the events.\",\"599\":\"A lower MinNDC or a higher AP and mAP value indicates better detection performance.\",\"600\":\"We compare our method with state-of-the-art handcrafted features, improved dense trajectory with Fisher vector (IDTFV) for the event detection.\",\"601\":\"We adopt the software of improved trajectories provided by Heng et al. [38] to extract raw trajectory features for each video in the MED14 dataset with default parameters, that is, frames of length 15 for each trajectory on a dense grid with 5-pixel spacing.\",\"602\":\"We use PCA to reduce the dimensionality of the raw trajectory features from 426 to 213.\",\"603\":\"Then we aggregate the features for each video using a Fisher vector [29] with 256 Gaussians, resulting in a 109,056-dimensional vector.\",\"604\":\"We also follow the suggestions to apply power normalization and l2 normalization to the feature vectors.\",\"605\":\"Table 1 reports experimental results making comparison with state-of-the-art shallow features with a single modality.\",\"606\":\"From the results, we can see that the proposed CNNbased DevNet has 5.86% improvements in terms of mean Average Precision (mAP) compared with the state-of-theart IDTFV shallow features by averaging over all events, which validates the effectiveness of the learned representation by DevNet approach.\",\"607\":\"4.4.\",\"608\":\"Evidence Recounting Protocol The goal of multimedia event evidence recounting is to give spatial-temporal key evidences for the videos detected 0 20 40 60 80 100 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 Recounting Percent (%) Evidence Quality DevNet Baseline Figure 3.\",\"609\":\"Comparison in terms of evidence quality against recounting percentage.\",\"610\":\"Table 2.\",\"611\":\"Event recounting results comparing with the baseline approach.\",\"612\":\"T means temporal key evidences and S means spatial key evidences.\",\"613\":\"ID Baseline(T) DevNet(T) Baseline(S) DevNet(S) E021 3 7 1 9 E022 4 6 1 9 E023 1 9 0 10 E024 3 7 3 7 E025 5 5 3 7 E026 3 7 1 9 E027 6 4 0 10 E028 4 6 0 10 E029 5 5 5 5 E030 4 6 0 10 E031 5 5 0 10 E032 2 8 3 7 E033 3 7 0 10 E034 3 7 2 8 E035 4 6 3 7 E036 5 5 4 6 E037 2 8 2 8 E038 4 6 0 10 E039 3 7 4 6 E040 3 7 3 7 Average 3.6 6.4 1.75 8.25 as positive.\",\"614\":\"Our event recounting approach consists of the following steps: 1. Extracting key frames.\",\"615\":\"It is the same as that for the event detection task.\",\"616\":\"2.\",\"617\":\"Spatial-temporal saliency map.\",\"618\":\"Given the event label we are interested in, we perform a backward pass based on the DevNet model to assign to each pixel in the testing video a saliency score.\",\"619\":\"The higher score a pixel gets, the more likely it contributes to the key evidence.\",\"620\":\"More details can be found in Section 3.3.\",\"621\":\"3.\",\"622\":\"Selecting informative key frames.\",\"623\":\"For each key frame, we compute the average of the saliency scores of all pixels and use it as the key-frame level saliency score.\",\"624\":\"A higher score indicates that the key frame is more discriminative.\",\"625\":\"We use the key frames with the N highest scores as temporal key evidences.\",\"626\":\"4.\",\"627\":\"Segmenting discriminative regions.\",\"628\":\"We use the spa4326 \\fTable 1.\",\"629\":\"Event detection results comparing with improved dense trajectory Fisher vector (IDTFV).\",\"630\":\"LOWER MinNDC \\/ HIGHER AP indicates BETTER performance.\",\"631\":\"The best results are highlighted in bold .\",\"632\":\"Event Description ID Evaluation Metric IDTFV (SVM) IDTFV (KR) DevNet (SVM) DevNet (KR) Attempting a bike trick E021 AP MinNDC 0.1131 0.1684 0.0986 0.2984 0.2887 0.2413 0.2741 0.2293 Cleaning an appliance E022 AP MinNDC 0.2406 0.553 0.2190 0.5783 0.2449 0.4270 0.1998 0.4251 Dog show E023 AP MinNDC 0.6554 0.2695 0.6609 0.1705 0.7251 0.0423 0.7504 0.0537 Giving directions to a location E024 AP MinNDC 0.0898 0.8294 0.0542 0.8164 0.1059 0.6532 0.1022 0.6510 Marriage proposal E025 AP MinNDC 0.1187 0.8290 0.1349 0.8244 0.0782 0.8626 0.0481 0.8725 Renovating a home E026 AP MinNDC 0.1400 0.7053 0.1683 0.7026 0.1880 0.5647 0.1911 0.5313 Rock climbing E027 AP MinNDC 0.2258 0.3691 0.2034 0.3931 0.2272 0.3706 0.2230 0.3529 Town hall meeting E028 AP MinNDC 0.4024 0.4180 0.3474 0.4023 0.4286 0.3805 0.3831 0.3554 Winning a race without a vehicle E029 AP MinNDC 0.2162 0.3024 0.2346 0.2946 0.2486 0.2637 0.2463 0.2658 Working on a metal crafts project E030 AP MinNDC 0.2145 0.6900 0.1985 0.5881 0.1606 0.5248 0.2063 0.5165 Beekeeping E031 AP MinNDC 0.6313 0.2693 0.6790 0.1935 0.8238 0.0817 0.8041 0.0634 Wedding shower E032 AP MinNDC 0.2140 0.4847 0.1588 0.5335 0.2716 0.3563 0.3149 0.4465 Non-motorized vehicle repair E033 AP MinNDC 0.3489 0.4497 0.3645 0.3708 0.5787 0.2306 0.6354 0.2316 Fixing musical instrument E034 AP MinNDC 0.2091 0.3458 0.2727 0.3494 0.4453 0.2365 0.4633 0.2475 Horse riding competition E035 AP MinNDC 0.3526 0.3164 0.3851 0.2768 0.3943 0.2606 0.4409 0.2820 Felling a tree E036 AP MinNDC 0.1947 0.4552 0.2611 0.3399 0.2271 0.4193 0.1979 0.4331 Parking a vehicle E037 AP MinNDC 0.2633 0.5537 0.2848 0.5337 0.3337 0.1455 0.3735 0.1314 Playing fetch E038 AP MinNDC 0.0676 0.5198 0.0731 0.4972 0.1093 0.5411 0.1115 0.5584 Tailgating E039 AP MinNDC 0.4648 0.3354 0.4529 0.3464 0.4035 0.3235 0.3929 0.3339 Tuning musical instrument E040 AP MinNDC 0.2283 0.4834 0.2347 0.4768 0.2937 0.4486 0.2982 0.4186 Average AP MinNDC 0.2696 0.4674 0.2743 0.4493 0.3288 0.3687 0.3329 0.3699 tial saliency maps of the selected key frames for initialization and apply graph-cut [1] to segment the discriminative regions as spatial key evidences.\",\"633\":\"For the temporal key evidence localization task, we compare our results with a state-of-the-art approach [35], which won the \\ufb01rst place in the NIST TRECVID 2013 MER task.\",\"634\":\"To the best of our knowledge, we are the \\ufb01rst to deal with spatial key evidence localization and hence there exist no algorithms for comparison.\",\"635\":\"Thus, we compare with the unsupervised salient object detection approach [48] in the selected key frames to generate spatial key evidences.\",\"636\":\"4.5.\",\"637\":\"Evidence Recounting Result Evaluation of video recounting results is dif\\ufb01cult because no ground-truth information is available.\",\"638\":\"Thus we conducted an experiment based on human evaluation.\",\"639\":\"Two criteria were used: evidence quality, which measures how well the localized key evidences can convince the judge that a speci\\ufb01c event occurs in the video; and recounting percent, which measures how compact the video snippets are compared to the whole video.\",\"640\":\"A few volunteers were asked to serve as evaluators.\",\"641\":\"Before evaluation, each evaluator was shown the event category descriptions in text as well as 10 positive examples in the training set.\",\"642\":\"For each event, we used all the positive videos from MEDTest for evaluation.\",\"643\":\"During the evaluation process, the evaluators were \\ufb01rst shown 1, 5, 10, 25, 50, 75 and 100 percents of the test videos separately.\",\"644\":\"They voted on whether the key frames shown could convince them that it is a positive exemplar.\",\"645\":\"We show the comparison results by plotting the evidence 4327 \\fDog show Dog show Dog show Wedding shower Wedding shower Wedding shower Playing fetch Playing fetch Playing fetch Horse riding Horse riding Horse riding Parking a vehicle Parking a vehicle Parking a vehicle Renovating a home Renovating a home Renovating a home Figure 4.\",\"646\":\"Event recounting results generated by DevNet.\",\"647\":\"From left to right are top one temporal key evidence, spatial saliency map, and spatial key evidence.\",\"648\":\"quality (percentage of test videos convinced as positive exemplars) against the recounting percentage in Figure 3.\",\"649\":\"Then the evaluators were presented the temporal key evidences (key frames) generated by [35] and DevNet with \\ufb01xed recounting percentage (5%) and the spatial key evidences generated by [48] and DevNet.\",\"650\":\"They voted on which key evidences are more informative.\",\"651\":\"The voting results are shown in Table 2.\",\"652\":\"From Figure 3, we can see that DevNet can reduce the recounting percentage by 15% to 25% to get the same evidence quality as the baseline method.\",\"653\":\"This validates that our approach provides reasonably good evidences for users to rapidly and accurately grasp the basic ideas of the video events.\",\"654\":\"Table 2 summarizes the evaluators\\u2019 preferences between our approach and the approach compared for each event.\",\"655\":\"It can be seen that DevNet is better for most of the events.\",\"656\":\"Some visual results are also shown in Figure 4.\",\"657\":\"5.\",\"658\":\"Conclusion In this paper, we presented a novel DevNet framework to address the video event detection and evidence recounting problems.\",\"659\":\"Based on the proposed DevNet, the CNN pretrained on large-scale image datasets, e.g, ImageNet, can be successfully transferred to the video domain.\",\"660\":\"In addition, we apply a single back pass on DevNet (no additional annotations are required) to localize the spatial-temporal key evidences for the event recounting.\",\"661\":\"We evaluate our experiment results on the challenging TRECVID MED 2014 dataset, and achieve a signi\\ufb01cant improvement than the state-of-the-art hand-crafted shallow features on the event detection task and satisfying event recounting results.\",\"662\":\"We believe the event detection results could be further improved by the better model initialization and effective feature encoding [43].\",\"663\":\"In future work, we will add the motion information into the DevNet and also extend this method to generate tag descriptions for the spatial-temporal key evidences.\",\"664\":\"6.\",\"665\":\"Acknowledgement This work was supported in part by the National Basic Research Program of China Grant 2011CBA00300, 2011CBA00301, the National Natural Science Foundation of China Grant 61033001, 61361136003, partially supported by research grant FSGRF14EG36, and partially supported by Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20068.\",\"666\":\"We thanks for the generous donation of the GPUs by NVIDIA.\",\"667\":\"The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.\",\"668\":\"Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the of\\ufb01cial policies or endorsements, either expressed or implied, of IARPA, DoI\\/NBC, or the U.S. Government.\",\"669\":\"4328 \\fReferences [1] Y. Y. Boykov and M.-P. Jolly.\",\"670\":\"Interactive graph cuts for optimal boundary & region segmentation of objects in nd images.\",\"671\":\"In ICCV, pages 105\\u2013112, 2001.\",\"672\":\"4322, 4325, 4327 [2] C.-C. Chang and C.-J. Lin.\",\"673\":\"LIBSVM: A library for support vector machines.\",\"674\":\"ACM Transactions on Intelligent Systems and Technology, 2:27:1\\u201327:27, 2011.\",\"675\":\"Software available at http:\\/\\/www.csie.ntu.edu.tw\\/ \\u02dccjlin\\/libsvm.\",\"676\":\"4322, 4326 [3] K. Chat\\ufb01eld, K. Simonyan, A. Vedaldi, and A. Zisserman.\",\"677\":\"Return of the devil in the details: Delving deep into convolutional nets.\",\"678\":\"In BMVC, 2014.\",\"679\":\"4322, 4323, 4324 [4] M.-y. Chen and A. Hauptmann.\",\"680\":\"Mosift: Recognizing human actions in surveillance videos.\",\"681\":\"2009.\",\"682\":\"4322, 4323 [5] N. Dalal and B. Triggs.\",\"683\":\"Histograms of oriented gradients for human detection.\",\"684\":\"In CVPR, pages 886\\u2013893, 2005.\",\"685\":\"4323 [6] N. Dalal, B. Triggs, and C. Schmid.\",\"686\":\"Human detection using oriented histograms of \\ufb02ow and appearance.\",\"687\":\"In ECCV, pages 428\\u2013441.\",\"688\":\"2006.\",\"689\":\"4323 [7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei.\",\"690\":\"Imagenet: A large-scale hierarchical image database.\",\"691\":\"In CVPR, pages 248\\u2013255, 2009.\",\"692\":\"4322, 4323, 4324 [8] D. Erhan, Y. Bengio, A. Courville, and P. Vincent.\",\"693\":\"Visualizing higher-layer features of a deep network.\",\"694\":\"Dept.\",\"695\":\"IRO, Universite\\u0301 de Montre\\u0301al, Tech.\",\"696\":\"Rep, 2009.\",\"697\":\"4323 [9] C. Farabet, C. Couprie, L. Najman, and Y. LeCun.\",\"698\":\"Learning hierarchical features for scene labeling.\",\"699\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1915\\u2013 1929, 2013.\",\"700\":\"4323 [10] C. Gan, M. Lin, Y. Yang, Y. Zhuang, and A. G. Hauptmann.\",\"701\":\"Exploring semantic inter-class relationships (SIR) for zeroshot action recognition.\",\"702\":\"In AAAI, 2015.\",\"703\":\"4323 [11] R. Girshick, J. Donahue, T. Darrell, and J. Malik.\",\"704\":\"Rich feature hierarchies for accurate object detection and semantic segmentation.\",\"705\":\"CVPR, pages 580\\u2013587, 2014.\",\"706\":\"4322, 4323, 4324 [12] M. Gygli, H. Grabner, H. Riemenschneider, and L. Van Gool.\",\"707\":\"Creating summaries from user videos.\",\"708\":\"In ECCV, 2014.\",\"709\":\"4322 [13] K. He, X. Zhang, S. Ren, and J. Sun.\",\"710\":\"Spatial pyramid pooling in deep convolutional networks for visual recognition.\",\"711\":\"In ECCV, pages 346\\u2013361.\",\"712\":\"2014.\",\"713\":\"4322, 4323 [14] Y. Jia.\",\"714\":\"Caffe: An open source convolutional architecture for fast feature embedding.\",\"715\":\"http:\\/\\/caffe.\",\"716\":\"berkeleyvision.\",\"717\":\"org, 2013.\",\"718\":\"4323 [15] Y.-G. Jiang, S. Bhattacharya, S.-F. Chang, and M. Shah.\",\"719\":\"High-level event recognition in unconstrained videos.\",\"720\":\"International Journal of Multimedia Information Retrieval, 2(2):73\\u2013101, 2013.\",\"721\":\"4323 [16] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei.\",\"722\":\"Large-scale video classi\\ufb01cation with convolutional neural networks.\",\"723\":\"In CVPR, pages 1725\\u20131732, 2014.\",\"724\":\"4322, 4323 [17] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\",\"725\":\"Imagenet classi\\ufb01cation with deep convolutional neural networks.\",\"726\":\"In NIPS, pages 1097\\u20131105, 2012.\",\"727\":\"4322, 4323 [18] K.-T. Lai, D. Liu, M.-S. Chen, and S.-F. Chang.\",\"728\":\"Recognizing complex events in videos by learning key static-dynamic evidences.\",\"729\":\"In ECCV, pages 675\\u2013688, 2014.\",\"730\":\"4323 [19] K.-T. Lai, D. Liu, M.-S. Chen, and S.-F. Chang.\",\"731\":\"Video event detection by inferring temporal instance labels.\",\"732\":\"In CVPR, pages 2251\\u20132258, 2014.\",\"733\":\"4323 [20] I. Laptev.\",\"734\":\"On space-time interest points.\",\"735\":\"International Journal of Computer Vision, 64(2-3):107\\u2013123, 2005.\",\"736\":\"4323 [21] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld.\",\"737\":\"Learning realistic human actions from movies.\",\"738\":\"In CVPR, pages 1\\u20138, 2008.\",\"739\":\"4323 [22] Q. V. Le. Building high-level features using large scale unsupervised learning.\",\"740\":\"In ICASSP, pages 8595\\u20138598, 2013.\",\"741\":\"4323 [23] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.\",\"742\":\"Gradientbased learning applied to document recognition.\",\"743\":\"Proceedings of the IEEE, 86(11):2278\\u20132324, 1998.\",\"744\":\"4323 [24] Y. J. Lee, J. Ghosh, and K. Grauman.\",\"745\":\"Discovering important people and objects for egocentric video summarization.\",\"746\":\"In CVPR, pages 3\\u20132, 2012.\",\"747\":\"4322 [25] J. Liu, Q. Yu, O. Javed, S. Ali, A. Tamrakar, A. Divakaran, H. Cheng, and H. Sawhney.\",\"748\":\"Video event recognition using concept attributes.\",\"749\":\"In WACV, pages 339\\u2013346, 2013.\",\"750\":\"4321, 4323 [26] D. G. Lowe.\",\"751\":\"Distinctive image features from scaleinvariant keypoints.\",\"752\":\"International journal of computer vision, 60(2):91\\u2013110, 2004.\",\"753\":\"4322, 4323 [27] Z. Lu and K. Grauman.\",\"754\":\"Story-driven summarization for egocentric video.\",\"755\":\"In CVPR, pages 2714\\u20132721, 2013.\",\"756\":\"4322 [28] Z. Ma, Y. Yang, N. Sebe, and A. G. Hauptmann.\",\"757\":\"Knowledge adaptation with partially shared features for event detection using few exemplars.\",\"758\":\"pages 1789\\u20131802, 2014.\",\"759\":\"4323 [29] D. Oneata, J. Verbeek, C. Schmid, et al.\",\"760\":\"Action and event recognition with \\ufb01sher vectors on a compact feature set.\",\"761\":\"In ICCV, 2013.\",\"762\":\"4323, 4326 [30] D. Potapov, M. Douze, Z. Harchaoui, and C. Schmid.\",\"763\":\"Category-speci\\ufb01c video summarization.\",\"764\":\"In ECCV, pages 540\\u2013555.\",\"765\":\"2014.\",\"766\":\"4323 [31] K. Simonyan, A. Vedaldi, and A. Zisserman.\",\"767\":\"Deep inside convolutional networks: Visualising image classi\\ufb01cation models and saliency maps.\",\"768\":\"arXiv preprint arXiv:1312.6034, 2013.\",\"769\":\"4323, 4324 [32] K. Simonyan and A. Zisserman.\",\"770\":\"Two-stream convolutional networks for action recognition in videos.\",\"771\":\"In NIPS, 2014.\",\"772\":\"4322, 4323 [33] K. Simonyan and A. Zisserman.\",\"773\":\"Very deep convolutional networks for large-scale image recognition.\",\"774\":\"ICLR, 2015.\",\"775\":\"4323 [34] C. Sun, B. Burns, R. Nevatia, C. Snoek, B. Bolles, G. Myers, W. Wang, and E. Yeh.\",\"776\":\"ISOMER: Informative segment observations for multimedia event recounting.\",\"777\":\"In ICMR, page 241, 2014.\",\"778\":\"4321, 4323 [35] C. Sun and R. Nevatia.\",\"779\":\"DISCOVER: Discovering important segments for classi\\ufb01cation of video events and recounting.\",\"780\":\"In CVPR, pages 2569\\u2013257, 2014.\",\"781\":\"4321, 4327, 4328 [36] V. Vovk.\",\"782\":\"Kernel ridge regression.\",\"783\":\"In Empirical Inference, pages 105\\u2013116.\",\"784\":\"2013.\",\"785\":\"4322, 4326 4329 \\f[37] H. Wang, A. Klaser, C. Schmid, and C.-L. Liu.\",\"786\":\"Action recognition by dense trajectories.\",\"787\":\"In CVPR, pages 3169\\u20133176, 2011.\",\"788\":\"4322, 4323 [38] H. Wang and C. Schmid.\",\"789\":\"Action recognition with improved trajectories.\",\"790\":\"In ICCV, 2013.\",\"791\":\"4322, 4323, 4326 [39] X. Wang, D. F. Fouhey, and A. Gupta.\",\"792\":\"Designing deep networks for surface normal estimation.\",\"793\":\"CVPR, 2015.\",\"794\":\"4322 [40] X. Wang, L. Zhang, L. Lin, Z. Liang, and W. Zuo.\",\"795\":\"Deep joint task learning for generic object extraction.\",\"796\":\"In NIPS, pages 523\\u2013531, 2014.\",\"797\":\"4322 [41] Y. Wei, W. Xia, J. Huang, B. Ni, J. Dong, Y. Zhao, and S. Yan.\",\"798\":\"CNN: Single-label to multi-label.\",\"799\":\"arXiv preprint arXiv:1406.5726, 2014.\",\"800\":\"4323, 4324 [42] S. Wu, S. Bondugula, F. Luisier, X. Zhuang, and P. Natarajan.\",\"801\":\"Zero-shot event detection using multi-modal fusion of weakly supervised concepts.\",\"802\":\"In CVPR, pages 2665\\u20132672, 2014.\",\"803\":\"4323 [43] Z. Xu, Y. Yang, and A. G. Hauptmann.\",\"804\":\"A discriminative CNN video representation for event detection.\",\"805\":\"CVPR, 2015.\",\"806\":\"4328 [44] Y. Yang, Z. Ma, Z. Xu, S. Yan, and A. G. Hauptmann.\",\"807\":\"How related exemplars help complex event detection in web videos?\",\"808\":\"In ICCV, pages 2104\\u20132111, 2013.\",\"809\":\"4323 [45] Q. Yu, J. Liu, H. Cheng, A. Divakaran, and H. Sawhney.\",\"810\":\"Multimedia event recounting with concept based representation.\",\"811\":\"In ACM Multimedia, pages 1073\\u20131076, 2012.\",\"812\":\"4321, 4323 [46] S.-I. Yu, L. Jiang, Z. Mao, X. Chang, X. Du, C. Gan, Z. Lan, Z. Xu, X. Li, Y. Cai, et al. Informedia@ TRECVID 2014 MED and MER.\",\"813\":\"4324, 4325 [47] M. D. Zeiler and R. Fergus.\",\"814\":\"Visualizing and understanding convolutional networks.\",\"815\":\"In ECCV, pages 818\\u2013833.\",\"816\":\"2014.\",\"817\":\"4323 [48] W. Zhu, S. Liang, Y. Wei, and J. Sun.\",\"818\":\"Saliency optimization from robust background detection.\",\"819\":\"In CVPR, pages 2814\\u2013 2821, 2014.\",\"820\":\"4327, 4328 4330\"}}", "query": "Video Captioning is a task of automatic captioning a video by understanding the action and event in the video which can help in the retrieval of the video efficiently through text.", "history": "{\"filename\":{\"37\":\"ref 19-Imprtnt.pdf\",\"1\":\"ref 19-Imprtnt.pdf\",\"376\":\"ref 19-Imprtnt.pdf\",\"277\":\"ref 19-Imprtnt.pdf\",\"181\":\"ref 19-Imprtnt.pdf\",\"355\":\"ref 19-Imprtnt.pdf\",\"18\":\"ref 19-Imprtnt.pdf\",\"149\":\"ref 19-Imprtnt.pdf\",\"319\":\"ref 19-Imprtnt.pdf\",\"14\":\"ref 19-Imprtnt.pdf\",\"30\":\"ref 19-Imprtnt.pdf\",\"185\":\"ref 19-Imprtnt.pdf\",\"468\":\"ref 06_imprnt.pdf\",\"40\":\"ref 19-Imprtnt.pdf\",\"19\":\"ref 19-Imprtnt.pdf\",\"707\":\"ref 06_imprnt.pdf\",\"54\":\"ref 19-Imprtnt.pdf\",\"390\":\"ref 19-Imprtnt.pdf\",\"9\":\"ref 19-Imprtnt.pdf\",\"403\":\"ref 19-Imprtnt.pdf\"},\"sentence\":{\"37\":37,\"1\":1,\"376\":376,\"277\":277,\"181\":181,\"355\":355,\"18\":18,\"149\":149,\"319\":319,\"14\":14,\"30\":30,\"185\":185,\"468\":61,\"40\":40,\"19\":19,\"707\":300,\"54\":54,\"390\":390,\"9\":9,\"403\":403},\"text\":{\"37\":\"Related Work Early work on video captioning considered tagging videos with metadata [1] and clustering captions and videos [14, 25, 42] for retrieval tasks.\",\"1\":\"To approach this problem, we propose a novel end-to-end sequence-to-sequence model to generate captions for videos.\",\"376\":\"Translating video content to natural language descriptions.\",\"277\":\"Conclusion This paper proposed a novel approach to video description.\",\"181\":\"Video description datasets We report results on three video description corpora, namely the Microsoft Video Description corpus (MSVD) [3], the MPII Movie Description Corpus (MPII-MD) [28], and the Montreal Video Annotation Dataset (M-VAD) [37].\",\"355\":\"Generating natural-language video descriptions using text-mined knowledge.\",\"18\":\"The problem of generating descriptions in open domain videos is difficult not just due to the diverse set of objects, scenes, actions, and their attributes, but also because it is hard to determine the salient content and describe the event appropriately in context.\",\"149\":\"Similar to previous LSTM-based image captioning efforts [8, 40] and video-to-text approaches [39, 43], we apply a convolutional neural network (CNN) to input images and provide the output of the top layer as input to the LSTM unit.\",\"319\":\"Video2text: Learning to annotate video content.\",\"14\":\"Our S2VT approach performs video description using a sequence to sequence model.\",\"30\":\"To our knowledge, this is the first approach to video description that uses a general sequence to sequence model.\",\"185\":\"The videos were then used to elicit single sentence descriptions from annotators.\",\"468\":\"A video event detection system usually consists of the following procedure: feature extraction, quantization\\/pooling, and classi\\ufb01er training.\",\"40\":\"They then use a probabilistic graphical model to combine the visual confidences with a language model in order to estimate the most likely content (subject, verb, object, scene) in the video, which is then used to generate a sentence.\",\"19\":\"To learn what is worth describing, our model learns from video clips and paired sentences that describe the depicted events in natural language.\",\"707\":\"Creating summaries from user videos.\",\"54\":\"The approach in [8] also generates video descriptions using an LSTM; however, they employ a version of the two-step approach that uses CRFs to obtain semantic tuples of activity, object, tool, and locatation and then use an LSTM to translate this tuple into a sentence.\",\"390\":\"Using descriptive video services to create a large data source for video annotation research.\",\"9\":\"While image description handles a variable length output sequence of words, video description also has to handle a variable length input sequence of frames.\",\"403\":\"Describing videos by exploiting temporal structure.\"},\"relevance\":{\"37\":1,\"1\":0,\"376\":0,\"277\":0,\"181\":0,\"355\":1,\"18\":1,\"149\":0,\"319\":1,\"14\":1,\"30\":1,\"185\":0,\"468\":0,\"40\":0,\"19\":1,\"707\":true,\"54\":true,\"390\":true,\"9\":false,\"403\":true}}"}